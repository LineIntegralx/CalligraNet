{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPwTW7igXXVAfNN422Cqfrx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LineIntegralx/CalligraNet/blob/main/Training_Scripts/CNN_Swin_CTC_Training_v2_Finalized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGcAcoDPMr3V",
        "outputId": "9fd39094-8b45-45ab-8df1-9679cb792fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import timm\n"
      ],
      "metadata": {
        "id": "Sgfpu5M1NYDv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "ROOT = Path(\"/content/drive/MyDrive/EECE693_Project\")\n",
        "\n",
        "DATA_ROOTS = {\n",
        "    \"D0_preprocessed\": ROOT / \"Preprocessed_HICMA\",\n",
        "    \"D1_augmented\":    ROOT / \"Augmented_HICMA\",\n",
        "    \"D2_synth\":        ROOT / \"HICMA_Plus_Synthetic\",\n",
        "}\n",
        "\n",
        "CKPT_DIR = ROOT / \"SwinCTC_Checkpoints\"\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Checkpoint dir:\", CKPT_DIR)\n",
        "\n",
        "BATCH_SIZE    = 8\n",
        "NUM_EPOCHS    = 40\n",
        "FREEZE_EPOCHS = 5          # epochs with Swin frozen\n",
        "LR_MAIN       = 3e-4       # CNN stem + CTC head\n",
        "LR_SWIN       = 1e-4       # Swin fine-tuning\n",
        "WEIGHT_DECAY  = 1e-2\n",
        "PATIENCE      = 7          # early stopping on val CER\n",
        "\n",
        "TARGET_HEIGHT = 256\n",
        "PAD_DIVISOR   = 32\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bV4g-ipNZkj",
        "outputId": "ced6c39e-1ad8-40ca-a54f-c86af97aa1a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Checkpoint dir: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split_dfs(base_dir: Path):\n",
        "    dfs = {}\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        df = pd.read_csv(base_dir / f\"{split}_labels.csv\")\n",
        "        df[\"label\"] = df[\"label\"].astype(str)\n",
        "        df[\"img_name\"] = df[\"img_name\"].astype(str).str.strip()\n",
        "        dfs[split] = df\n",
        "    return dfs\n",
        "\n",
        "for name, base in DATA_ROOTS.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    dfs = load_split_dfs(base)\n",
        "    for split, df in dfs.items():\n",
        "        print(f\"{split}: {len(df)} rows, classes={df['class'].value_counts().to_dict()}\")\n",
        "        print(f\"  avg label len = {df['label'].str.len().mean():.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69HoSwK-NbIi",
        "outputId": "101eb314-1c28-4491-b400-dbdc9e93f98a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== D0_preprocessed ===\n",
            "train: 4020 rows, classes={'Naskh': 2988, 'Thuluth': 808, 'Diwani': 190, 'Kufic': 21, 'Muhaquaq': 13}\n",
            "  avg label len = 42.5\n",
            "val: 502 rows, classes={'Naskh': 373, 'Thuluth': 101, 'Diwani': 23, 'Kufic': 3, 'Muhaquaq': 2}\n",
            "  avg label len = 41.6\n",
            "test: 503 rows, classes={'Naskh': 374, 'Thuluth': 101, 'Diwani': 24, 'Muhaquaq': 2, 'Kufic': 2}\n",
            "  avg label len = 44.5\n",
            "\n",
            "=== D1_augmented ===\n",
            "train: 20000 rows, classes={'Naskh': 4000, 'Thuluth': 4000, 'Diwani': 4000, 'Muhaquaq': 4000, 'Kufic': 4000}\n",
            "  avg label len = 34.3\n",
            "val: 502 rows, classes={'Naskh': 373, 'Thuluth': 101, 'Diwani': 23, 'Kufic': 3, 'Muhaquaq': 2}\n",
            "  avg label len = 41.6\n",
            "test: 503 rows, classes={'Naskh': 374, 'Thuluth': 101, 'Diwani': 24, 'Muhaquaq': 2, 'Kufic': 2}\n",
            "  avg label len = 44.5\n",
            "\n",
            "=== D2_synth ===\n",
            "train: 27316 rows, classes={'Kufic': 6923, 'Diwani': 6296, 'Thuluth': 6097, 'Naskh': 4000, 'Muhaquaq': 4000}\n",
            "  avg label len = 37.6\n",
            "val: 1416 rows, classes={'Naskh': 373, 'Kufic': 368, 'Thuluth': 363, 'Diwani': 310, 'Muhaquaq': 2}\n",
            "  avg label len = 44.7\n",
            "test: 1418 rows, classes={'Naskh': 374, 'Kufic': 368, 'Thuluth': 363, 'Diwani': 311, 'Muhaquaq': 2}\n",
            "  avg label len = 46.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build char vocab from richest dataset (D2_synth)\n",
        "def build_vocab_from_dataset(base_dir: Path):\n",
        "    labels = []\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        df = pd.read_csv(base_dir / f\"{split}_labels.csv\")\n",
        "        labels.extend(df[\"label\"].astype(str).tolist())\n",
        "    all_text = \"\".join(labels)\n",
        "    char_counter = Counter(all_text)\n",
        "    chars = sorted(list(char_counter.keys()))\n",
        "    print(\"\\nNum unique chars:\", len(chars))\n",
        "    return chars\n",
        "\n",
        "chars = build_vocab_from_dataset(DATA_ROOTS[\"D2_synth\"])\n",
        "\n",
        "BLANK_IDX = 0\n",
        "stoi = {ch: i + 1 for i, ch in enumerate(chars)}   # chars start at 1\n",
        "itos = {i + 1: ch for i, ch in enumerate(chars)}\n",
        "vocab_size = len(chars) + 1  # + blank\n",
        "print(\"vocab_size (including blank):\", vocab_size)\n",
        "\n",
        "class TextEncoder:\n",
        "    def __init__(self, stoi, itos, blank_idx=0):\n",
        "        self.stoi = stoi\n",
        "        self.itos = itos\n",
        "        self.blank_idx = blank_idx\n",
        "\n",
        "    def encode(self, text: str):\n",
        "        return [self.stoi[c] for c in text if c in self.stoi]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return \"\".join(self.itos[i] for i in ids if i in self.itos)\n",
        "\n",
        "text_encoder = TextEncoder(stoi, itos, BLANK_IDX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn8seXPRNci5",
        "outputId": "1cfe0b6e-1437-4b5a-a328-1e443de31cdc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Num unique chars: 70\n",
            "vocab_size (including blank): 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_HEIGHT = 256\n",
        "PAD_DIVISOR   = 32\n",
        "MAX_WIDTH_RESIZED = 1600  # you can later try 1024 or 2048\n",
        "\n",
        "class ResizePadTo256:\n",
        "    def __init__(self,\n",
        "                 target_height=TARGET_HEIGHT,\n",
        "                 pad_divisor=PAD_DIVISOR,\n",
        "                 max_width_resized=MAX_WIDTH_RESIZED):\n",
        "        self.target_height = target_height\n",
        "        self.pad_divisor = pad_divisor\n",
        "        self.max_width_resized = max_width_resized\n",
        "\n",
        "    def __call__(self, pil_img: Image.Image):\n",
        "        # 1) to grayscale numpy\n",
        "        img = pil_img.convert(\"L\")\n",
        "        img = np.array(img)  # HÃ—W\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        # 2) resize to fixed height\n",
        "        scale = self.target_height / float(h)\n",
        "        new_w = int(round(w * scale))\n",
        "        img_resized = cv2.resize(\n",
        "            img, (new_w, self.target_height), interpolation=cv2.INTER_AREA\n",
        "        )\n",
        "\n",
        "        # 3) if still too wide, compress width\n",
        "        if new_w > self.max_width_resized:\n",
        "            new_w = self.max_width_resized\n",
        "            img_resized = cv2.resize(\n",
        "                img_resized, (new_w, self.target_height), interpolation=cv2.INTER_AREA\n",
        "            )\n",
        "\n",
        "        # 4) pad width to multiple-of-divisor\n",
        "        if self.pad_divisor is not None:\n",
        "            padded_w = int(np.ceil(new_w / self.pad_divisor) * self.pad_divisor)\n",
        "        else:\n",
        "            padded_w = new_w\n",
        "\n",
        "        canvas = np.full((self.target_height, padded_w), 255, dtype=np.uint8)\n",
        "        x0 = (padded_w - new_w) // 2\n",
        "        canvas[:, x0:x0+new_w] = img_resized\n",
        "\n",
        "        # 5) to tensor [1, H, W] in [-1, 1]\n",
        "        tensor = torch.from_numpy(canvas).float() / 255.0\n",
        "        tensor = (tensor - 0.5) / 0.5\n",
        "        return tensor.unsqueeze(0)\n",
        "\n",
        "resize_pad = ResizePadTo256()\n",
        "train_transform = resize_pad\n",
        "eval_transform  = resize_pad\n"
      ],
      "metadata": {
        "id": "zZ3cji7UNeOC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HICMADataset(Dataset):\n",
        "    def __init__(self, base_dir: Path, split: str, transform, text_encoder: TextEncoder):\n",
        "        self.base_dir = base_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.text_encoder = text_encoder\n",
        "\n",
        "        df = pd.read_csv(self.base_dir / f\"{split}_labels.csv\")\n",
        "        df[\"img_name\"] = df[\"img_name\"].astype(str).str.strip()\n",
        "        df[\"label\"] = df[\"label\"].astype(str)\n",
        "        self.df = df[[\"img_name\", \"class\", \"label\"]].copy()\n",
        "\n",
        "        self.img_dir = self.base_dir / split / \"images\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_name = row[\"img_name\"]\n",
        "        cls      = row[\"class\"]\n",
        "        text     = row[\"label\"]\n",
        "\n",
        "        img_path = self.img_dir / img_name\n",
        "        image = Image.open(img_path).convert(\"L\")\n",
        "        image = self.transform(image)               # [1, H, W]\n",
        "\n",
        "        target = torch.tensor(self.text_encoder.encode(text), dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"target\": target,\n",
        "            \"text\": text,\n",
        "            \"class\": cls,\n",
        "            \"img_name\": img_name,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "xFkmrNj3Nfys"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_collate(batch):\n",
        "    images  = [b[\"image\"] for b in batch]    # each: [1, H_i, W_i]\n",
        "    targets = [b[\"target\"] for b in batch]\n",
        "\n",
        "    batch_size = len(images)\n",
        "    C = images[0].shape[0]\n",
        "    heights = [img.shape[1] for img in images]\n",
        "    widths  = [img.shape[2] for img in images]\n",
        "\n",
        "    max_h = max(heights)\n",
        "    max_w = max(widths)\n",
        "\n",
        "    def round_up(x, m=4):\n",
        "        return ((x + m - 1) // m) * m\n",
        "\n",
        "    max_h = round_up(max_h, 4)\n",
        "    max_w = round_up(max_w, 4)\n",
        "\n",
        "    pad_val = 1.0  # white in normalized space (since we use [-1,1])\n",
        "    padded = torch.full((batch_size, C, max_h, max_w),\n",
        "                        pad_val, dtype=images[0].dtype)\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        h, w = img.shape[1], img.shape[2]\n",
        "        padded[i, :, :h, :w] = img  # top-left placement\n",
        "\n",
        "    target_lengths = torch.tensor([t.size(0) for t in targets], dtype=torch.long)\n",
        "    targets_concat = torch.cat(targets, dim=0)\n",
        "\n",
        "    meta = {\n",
        "        \"texts\": [b[\"text\"] for b in batch],\n",
        "        \"img_names\": [b[\"img_name\"] for b in batch],\n",
        "        \"classes\": [b[\"class\"] for b in batch],\n",
        "        \"widths\": widths,\n",
        "        \"heights\": heights,\n",
        "    }\n",
        "\n",
        "    return padded, targets_concat, target_lengths, meta\n",
        "\n",
        "# quick sanity check on one dataset\n",
        "ds = HICMADataset(DATA_ROOTS[\"D0_preprocessed\"], \"train\", train_transform, text_encoder)\n",
        "print(\"Sample image shape:\", ds[0][\"image\"].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84DjyFugNkBD",
        "outputId": "a4d03da0-307c-43d7-edbc-ffec2cf90947"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image shape: torch.Size([1, 256, 1600])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNSwinCTC(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN stem: reduce width moderately, keep height\n",
        "        self.cnn_stem = nn.Sequential(\n",
        "            # 1 x H x W -> 32 x H x (W/2)\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=(1, 2), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # 32 x H x W/2 -> 64 x H x (W/4)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=(1, 2), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # 64 x H x W/4 -> 3 x H x (W/4)\n",
        "            nn.Conv2d(64, 3, kernel_size=3, stride=(1, 1), padding=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        # Swin-T backbone, NCHW features\n",
        "        self.swin = timm.create_model(\n",
        "            \"swin_tiny_patch4_window7_224\",\n",
        "            pretrained=True,\n",
        "            features_only=True,\n",
        "            out_indices=[-1],\n",
        "            in_chans=3,\n",
        "            img_size=256,\n",
        "            strict_img_size=False,\n",
        "        )\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.proj = None  # will be created lazily once we know C\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [B, 1, H, W]\n",
        "        returns:\n",
        "          log_probs: [T, B, C]\n",
        "          T_len: int (same for all in batch)\n",
        "        \"\"\"\n",
        "        # CNN stem\n",
        "        x = self.cnn_stem(x)              # [B, 3, Hc, Wc]\n",
        "\n",
        "        # Swin features: [B, C, Hs, Ws]\n",
        "        feat = self.swin(x)[0]\n",
        "        B, C, Hs, Ws = feat.shape\n",
        "\n",
        "        # Lazily create projection with correct input dim C\n",
        "        if self.proj is None:\n",
        "            self.proj = nn.Linear(C, self.hidden_dim).to(feat.device)\n",
        "\n",
        "        # Flatten spatial dims: T = Hs * Ws\n",
        "        feat = feat.permute(0, 2, 3, 1).contiguous()  # [B, Hs, Ws, C]\n",
        "        feat = feat.view(B, Hs * Ws, C)               # [B, T, C]\n",
        "\n",
        "        feat = self.proj(feat)                        # [B, T, hidden]\n",
        "        feat = self.dropout(feat)\n",
        "        logits = self.classifier(feat)                # [B, T, vocab_size]\n",
        "\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        T_len = logits.size(1)\n",
        "        return log_probs.permute(1, 0, 2), T_len      # [T, B, C], T\n"
      ],
      "metadata": {
        "id": "vuX2snbBNlwj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def levenshtein(a, b):\n",
        "    dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
        "    for i in range(len(a) + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(len(b) + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, len(a) + 1):\n",
        "        for j in range(1, len(b) + 1):\n",
        "            cost = 0 if a[i - 1] == b[j - 1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j] + 1,\n",
        "                dp[i][j-1] + 1,\n",
        "                dp[i-1][j-1] + cost,\n",
        "            )\n",
        "    return dp[-1][-1]\n",
        "\n",
        "def compute_cer(preds, gts):\n",
        "    total_dist, total_len = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        total_dist += levenshtein(p, g)\n",
        "        total_len  += len(g)\n",
        "    return total_dist / max(total_len, 1)\n",
        "\n",
        "def compute_wer(preds, gts):\n",
        "    total_dist, total_len = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        p_words = p.split()\n",
        "        g_words = g.split()\n",
        "        total_dist += levenshtein(p_words, g_words)\n",
        "        total_len  += len(g_words)\n",
        "    return total_dist / max(total_len, 1)\n",
        "\n",
        "def greedy_decode(log_probs, text_encoder: TextEncoder):\n",
        "    \"\"\"\n",
        "    log_probs: [T, B, C]\n",
        "    return: list of predicted strings\n",
        "    \"\"\"\n",
        "    max_ids = log_probs.argmax(dim=-1).transpose(0, 1)  # [B, T]\n",
        "    pred_strs = []\n",
        "    for seq in max_ids:\n",
        "        prev = BLANK_IDX\n",
        "        ids = []\n",
        "        for i in seq.tolist():\n",
        "            if i != prev and i != BLANK_IDX:\n",
        "                ids.append(i)\n",
        "            prev = i\n",
        "        pred_strs.append(text_encoder.decode(ids))\n",
        "    return pred_strs\n"
      ],
      "metadata": {
        "id": "sCqXohCWNoiH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, text_encoder: TextEncoder):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds, all_gts = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, target_lengths, meta in loader:\n",
        "            images  = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            log_probs, T_len = model(images)  # [T, B, C]\n",
        "            input_lengths = torch.full(\n",
        "                (images.size(0),), T_len, dtype=torch.long, device=device\n",
        "            )\n",
        "\n",
        "            loss = F.ctc_loss(\n",
        "                log_probs, targets,\n",
        "                input_lengths, target_lengths,\n",
        "                blank=BLANK_IDX, zero_infinity=True\n",
        "            )\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            pred_strs = greedy_decode(log_probs.cpu(), text_encoder)\n",
        "            all_preds.extend(pred_strs)\n",
        "            all_gts.extend(meta[\"texts\"])\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    cer = compute_cer(all_preds, all_gts)\n",
        "    wer = compute_wer(all_preds, all_gts)\n",
        "    return avg_loss, cer, wer\n"
      ],
      "metadata": {
        "id": "OywFQUuUNqoW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloaders_for_experiment(base_dir: Path, text_encoder: TextEncoder):\n",
        "    \"\"\"Create train/val/test datasets + dataloaders for one experiment.\"\"\"\n",
        "    train_ds = HICMADataset(base_dir, \"train\", train_transform, text_encoder)\n",
        "    val_ds   = HICMADataset(base_dir, \"val\",   eval_transform,  text_encoder)\n",
        "    # Fixed test set from *original* HICMA (D0)\n",
        "    test_ds  = HICMADataset(DATA_ROOTS[\"D0_preprocessed\"], \"test\", eval_transform, text_encoder)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=0, pin_memory=True, collate_fn=ctc_collate\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=0, pin_memory=True, collate_fn=ctc_collate\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=0, pin_memory=True, collate_fn=ctc_collate\n",
        "    )\n",
        "    return train_ds, val_ds, test_ds, train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "1qMV_DDvNuVt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check on shapes and T_len vs target lengths\n",
        "base_dir = DATA_ROOTS[\"D0_preprocessed\"]\n",
        "\n",
        "train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = \\\n",
        "    make_dataloaders_for_experiment(base_dir, text_encoder)\n",
        "\n",
        "model = CNNSwinCTC(vocab_size=vocab_size).to(device)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "images, targets, target_lengths, meta = batch\n",
        "images = images.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    log_probs, T_len = model(images)\n",
        "\n",
        "print(\"images.shape:\", images.shape)               # [B, 1, H, W]\n",
        "print(\"log_probs.shape:\", log_probs.shape)         # [T, B, C]\n",
        "print(\"T_len:\", T_len)\n",
        "print(\"target_lengths (first 8):\", target_lengths[:8].tolist())\n",
        "print(\"max target length:\", target_lengths.max().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQK-yOtKN1VK",
        "outputId": "af0e7757-458d-42bd-8c85-8da5f6de4981"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images.shape: torch.Size([8, 1, 256, 1600])\n",
            "log_probs.shape: torch.Size([9984, 8, 71])\n",
            "T_len: 9984\n",
            "target_lengths (first 8): [35, 42, 20, 27, 34, 52, 41, 50]\n",
            "max target length: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_experiment(exp_name: str, base_dir: Path):\n",
        "    print(f\"\\n\\n########## {exp_name} on {base_dir.name} ##########\")\n",
        "\n",
        "    print(\"  -> Building dataloaders...\")\n",
        "    train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = \\\n",
        "        make_dataloaders_for_experiment(base_dir, text_encoder)\n",
        "    print(\"  -> Dataloaders ready. Building model...\")\n",
        "\n",
        "    model = CNNSwinCTC(vocab_size=vocab_size).to(device)\n",
        "    print(\"  -> Model ready. Starting training...\")\n",
        "\n",
        "    # Stage 1: freeze Swin\n",
        "    for p in model.swin.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    main_params = [p for n, p in model.named_parameters() if not n.startswith(\"swin.\")]\n",
        "    swin_params = [p for n, p in model.named_parameters() if n.startswith(\"swin.\")]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [\n",
        "            {\"params\": main_params, \"lr\": LR_MAIN},\n",
        "            {\"params\": swin_params, \"lr\": LR_SWIN},\n",
        "        ],\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    best_val_cer = float(\"inf\")\n",
        "    best_val_wer = None\n",
        "    best_epoch   = -1\n",
        "    best_state   = None\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        # Unfreeze Swin after FREEZE_EPOCHS\n",
        "        if epoch == FREEZE_EPOCHS + 1:\n",
        "            print(\">> Unfreezing Swin backbone for fine-tuning.\")\n",
        "            for p in model.swin.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch:02d}:\")\n",
        "        for batch_idx, (images, targets, target_lengths, meta) in enumerate(train_loader):\n",
        "            images  = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            log_probs, T_len = model(images)\n",
        "            input_lengths = torch.full(\n",
        "                (images.size(0),), T_len, dtype=torch.long, device=device\n",
        "            )\n",
        "\n",
        "            loss = F.ctc_loss(\n",
        "                log_probs, targets,\n",
        "                input_lengths, target_lengths,\n",
        "                blank=BLANK_IDX, zero_infinity=True\n",
        "            )\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"    batch {batch_idx}/{len(train_loader)}  loss={loss.item():.4f}\")\n",
        "\n",
        "        train_loss = running_loss / len(train_ds)\n",
        "        val_loss, val_cer, val_wer = evaluate(model, val_loader, text_encoder)\n",
        "        print(f\"  train_loss={train_loss:.4f}  \"\n",
        "              f\"val_loss={val_loss:.4f}  \"\n",
        "              f\"val_CER={val_cer:.4f}  \"\n",
        "              f\"val_WER={val_wer:.4f}\")\n",
        "\n",
        "        # Track best model by val CER (still keeps checkpoints!)\n",
        "        if val_cer < best_val_cer:\n",
        "            best_val_cer = val_cer\n",
        "            best_val_wer = val_wer\n",
        "            best_epoch   = epoch\n",
        "            best_state   = model.state_dict()\n",
        "\n",
        "            ckpt_path = CKPT_DIR / f\"{exp_name}_best.pt\"\n",
        "            torch.save({\n",
        "                \"model_state\": best_state,\n",
        "                \"epoch\": best_epoch,\n",
        "                \"val_cer\": best_val_cer,\n",
        "                \"val_wer\": best_val_wer,\n",
        "                \"vocab_size\": vocab_size,\n",
        "                \"chars\": chars,\n",
        "            }, ckpt_path)\n",
        "            print(f\"  >> New best model saved to: {ckpt_path}\")\n",
        "\n",
        "    # Load best model and evaluate on test set\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    test_loss, test_cer, test_wer = evaluate(model, test_loader, text_encoder)\n",
        "    print(f\"[{exp_name}] BEST epoch={best_epoch}  \"\n",
        "          f\"val_CER={best_val_cer:.4f}  val_WER={best_val_wer:.4f}  \"\n",
        "          f\"TEST_CER={test_cer:.4f}  TEST_WER={test_wer:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"exp\": exp_name,\n",
        "        \"dataset\": base_dir.name,\n",
        "        \"train_size\": len(train_ds),\n",
        "        \"val_size\": len(val_ds),\n",
        "        \"test_size\": len(test_ds),\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"best_val_cer\": best_val_cer,\n",
        "        \"best_val_wer\": best_val_wer,\n",
        "        \"test_cer\": test_cer,\n",
        "        \"test_wer\": test_wer,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "orPsNgIzN4-u"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "exp_order = [\n",
        "    (\"D0_preprocessed\", DATA_ROOTS[\"D0_preprocessed\"]),\n",
        "    (\"D1_augmented\",    DATA_ROOTS[\"D1_augmented\"]),\n",
        "    (\"D2_synth\",        DATA_ROOTS[\"D2_synth\"]),\n",
        "]\n",
        "\n",
        "for exp_name, base in exp_order:\n",
        "    res = train_experiment(exp_name, base)\n",
        "    results.append(res)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oT-J4g7ON9fR",
        "outputId": "5fdae7e2-bf8c-4f08-f5f7-93ab5bd56adc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "########## D0_preprocessed on Preprocessed_HICMA ##########\n",
            "  -> Building dataloaders...\n",
            "  -> Dataloaders ready. Building model...\n",
            "  -> Model ready. Starting training...\n",
            "\n",
            "Epoch 01:\n",
            "    batch 0/503  loss=1857.7651\n",
            "    batch 50/503  loss=1262.1812\n",
            "    batch 100/503  loss=493.9257\n",
            "    batch 150/503  loss=508.2015\n",
            "    batch 200/503  loss=127.7903\n",
            "    batch 250/503  loss=95.5062\n",
            "    batch 300/503  loss=74.3637\n",
            "    batch 350/503  loss=15.5091\n",
            "    batch 400/503  loss=15.0952\n",
            "    batch 450/503  loss=12.7883\n",
            "    batch 500/503  loss=9.1789\n",
            "  train_loss=274.5385  val_loss=6.3209  val_CER=0.7543  val_WER=1.0131\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 02:\n",
            "    batch 0/503  loss=11.0013\n",
            "    batch 50/503  loss=7.0710\n",
            "    batch 100/503  loss=6.2885\n",
            "    batch 150/503  loss=26.0705\n",
            "    batch 200/503  loss=6.2459\n",
            "    batch 250/503  loss=4.7795\n",
            "    batch 300/503  loss=6.9814\n",
            "    batch 350/503  loss=20.5592\n",
            "    batch 400/503  loss=8.3603\n",
            "    batch 450/503  loss=6.1376\n",
            "    batch 500/503  loss=6.7661\n",
            "  train_loss=7.6683  val_loss=4.3097  val_CER=0.7987  val_WER=0.9990\n",
            "\n",
            "Epoch 03:\n",
            "    batch 0/503  loss=4.1910\n",
            "    batch 50/503  loss=5.8906\n",
            "    batch 100/503  loss=4.1029\n",
            "    batch 150/503  loss=6.5168\n",
            "    batch 200/503  loss=21.0255\n",
            "    batch 250/503  loss=6.2009\n",
            "    batch 300/503  loss=4.2132\n",
            "    batch 350/503  loss=5.6818\n",
            "    batch 400/503  loss=4.2596\n",
            "    batch 450/503  loss=4.2438\n",
            "    batch 500/503  loss=4.0037\n",
            "  train_loss=5.9177  val_loss=4.4496  val_CER=0.8095  val_WER=0.9995\n",
            "\n",
            "Epoch 04:\n",
            "    batch 0/503  loss=4.5158\n",
            "    batch 50/503  loss=3.8751\n",
            "    batch 100/503  loss=3.8947\n",
            "    batch 150/503  loss=3.8353\n",
            "    batch 200/503  loss=4.0037\n",
            "    batch 250/503  loss=3.7555\n",
            "    batch 300/503  loss=7.6536\n",
            "    batch 350/503  loss=3.7176\n",
            "    batch 400/503  loss=5.3199\n",
            "    batch 450/503  loss=4.7241\n",
            "    batch 500/503  loss=3.7795\n",
            "  train_loss=5.3440  val_loss=4.1955  val_CER=0.7807  val_WER=1.0007\n",
            "\n",
            "Epoch 05:\n",
            "    batch 0/503  loss=4.8373\n",
            "    batch 50/503  loss=4.5705\n",
            "    batch 100/503  loss=3.8582\n",
            "    batch 150/503  loss=3.6767\n",
            "    batch 200/503  loss=4.1311\n",
            "    batch 250/503  loss=3.8200\n",
            "    batch 300/503  loss=5.1226\n",
            "    batch 350/503  loss=4.2757\n",
            "    batch 400/503  loss=3.7166\n",
            "    batch 450/503  loss=3.6238\n",
            "    batch 500/503  loss=19.7661\n",
            "  train_loss=5.0608  val_loss=3.9241  val_CER=0.8033  val_WER=0.9967\n",
            ">> Unfreezing Swin backbone for fine-tuning.\n",
            "\n",
            "Epoch 06:\n",
            "    batch 0/503  loss=3.9107\n",
            "    batch 50/503  loss=3.3224\n",
            "    batch 100/503  loss=3.2062\n",
            "    batch 150/503  loss=3.2751\n",
            "    batch 200/503  loss=3.1068\n",
            "    batch 250/503  loss=3.9858\n",
            "    batch 300/503  loss=3.0234\n",
            "    batch 350/503  loss=3.1509\n",
            "    batch 400/503  loss=3.2942\n",
            "    batch 450/503  loss=3.2728\n",
            "    batch 500/503  loss=3.0109\n",
            "  train_loss=3.2690  val_loss=3.3808  val_CER=0.8341  val_WER=0.9998\n",
            "\n",
            "Epoch 07:\n",
            "    batch 0/503  loss=3.0623\n",
            "    batch 50/503  loss=3.2315\n",
            "    batch 100/503  loss=2.9525\n",
            "    batch 150/503  loss=3.1014\n",
            "    batch 200/503  loss=3.0853\n",
            "    batch 250/503  loss=2.9809\n",
            "    batch 300/503  loss=3.0812\n",
            "    batch 350/503  loss=2.9791\n",
            "    batch 400/503  loss=3.0694\n",
            "    batch 450/503  loss=2.9288\n",
            "    batch 500/503  loss=2.9549\n",
            "  train_loss=3.0420  val_loss=3.2056  val_CER=0.7833  val_WER=1.0029\n",
            "\n",
            "Epoch 08:\n",
            "    batch 0/503  loss=3.2908\n",
            "    batch 50/503  loss=2.9926\n",
            "    batch 100/503  loss=3.0103\n",
            "    batch 150/503  loss=2.9552\n",
            "    batch 200/503  loss=2.7892\n",
            "    batch 250/503  loss=2.9126\n",
            "    batch 300/503  loss=2.7733\n",
            "    batch 350/503  loss=2.9531\n",
            "    batch 400/503  loss=2.8154\n",
            "    batch 450/503  loss=3.1131\n",
            "    batch 500/503  loss=2.9062\n",
            "  train_loss=2.9333  val_loss=3.1355  val_CER=0.7621  val_WER=1.0264\n",
            "\n",
            "Epoch 09:\n",
            "    batch 0/503  loss=2.9377\n",
            "    batch 50/503  loss=2.7536\n",
            "    batch 100/503  loss=2.8048\n",
            "    batch 150/503  loss=2.9429\n",
            "    batch 200/503  loss=2.9689\n",
            "    batch 250/503  loss=2.8453\n",
            "    batch 300/503  loss=2.7242\n",
            "    batch 350/503  loss=2.8817\n",
            "    batch 400/503  loss=2.9490\n",
            "    batch 450/503  loss=2.8255\n",
            "    batch 500/503  loss=2.9487\n",
            "  train_loss=2.8847  val_loss=2.9554  val_CER=0.7572  val_WER=1.0594\n",
            "\n",
            "Epoch 10:\n",
            "    batch 0/503  loss=2.8537\n",
            "    batch 50/503  loss=2.6493\n",
            "    batch 100/503  loss=2.8662\n",
            "    batch 150/503  loss=2.8383\n",
            "    batch 200/503  loss=2.8182\n",
            "    batch 250/503  loss=2.8569\n",
            "    batch 300/503  loss=2.8484\n",
            "    batch 350/503  loss=2.7720\n",
            "    batch 400/503  loss=2.7582\n",
            "    batch 450/503  loss=2.8010\n",
            "    batch 500/503  loss=2.8060\n",
            "  train_loss=2.8518  val_loss=2.9984  val_CER=0.7682  val_WER=1.0076\n",
            "\n",
            "Epoch 11:\n",
            "    batch 0/503  loss=2.9214\n",
            "    batch 50/503  loss=2.6776\n",
            "    batch 100/503  loss=2.9965\n",
            "    batch 150/503  loss=2.9050\n",
            "    batch 200/503  loss=2.8159\n",
            "    batch 250/503  loss=2.7918\n",
            "    batch 300/503  loss=2.8687\n",
            "    batch 350/503  loss=2.7810\n",
            "    batch 400/503  loss=2.9286\n",
            "    batch 450/503  loss=2.7123\n",
            "    batch 500/503  loss=2.8160\n",
            "  train_loss=2.8296  val_loss=2.8887  val_CER=0.7665  val_WER=1.0518\n",
            "\n",
            "Epoch 12:\n",
            "    batch 0/503  loss=2.7911\n",
            "    batch 50/503  loss=2.7124\n",
            "    batch 100/503  loss=2.8829\n",
            "    batch 150/503  loss=2.9936\n",
            "    batch 200/503  loss=2.8519\n",
            "    batch 250/503  loss=2.7968\n",
            "    batch 300/503  loss=2.7909\n",
            "    batch 350/503  loss=2.6712\n",
            "    batch 400/503  loss=2.8075\n",
            "    batch 450/503  loss=3.0159\n",
            "    batch 500/503  loss=2.7629\n",
            "  train_loss=2.8052  val_loss=2.8821  val_CER=0.7626  val_WER=1.0133\n",
            "\n",
            "Epoch 13:\n",
            "    batch 0/503  loss=2.7705\n",
            "    batch 50/503  loss=2.8686\n",
            "    batch 100/503  loss=2.8808\n",
            "    batch 150/503  loss=2.7581\n",
            "    batch 200/503  loss=2.6681\n",
            "    batch 250/503  loss=2.9884\n",
            "    batch 300/503  loss=2.7504\n",
            "    batch 350/503  loss=2.6742\n",
            "    batch 400/503  loss=2.8541\n",
            "    batch 450/503  loss=2.9721\n",
            "    batch 500/503  loss=2.5729\n",
            "  train_loss=2.7781  val_loss=2.8354  val_CER=0.7389  val_WER=1.0442\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 14:\n",
            "    batch 0/503  loss=2.7179\n",
            "    batch 50/503  loss=2.8063\n",
            "    batch 100/503  loss=2.8359\n",
            "    batch 150/503  loss=2.9360\n",
            "    batch 200/503  loss=2.7767\n",
            "    batch 250/503  loss=2.6202\n",
            "    batch 300/503  loss=2.7451\n",
            "    batch 350/503  loss=2.7043\n",
            "    batch 400/503  loss=2.8070\n",
            "    batch 450/503  loss=2.7169\n",
            "    batch 500/503  loss=2.8494\n",
            "  train_loss=2.7468  val_loss=2.8285  val_CER=0.7401  val_WER=1.0390\n",
            "\n",
            "Epoch 15:\n",
            "    batch 0/503  loss=2.8026\n",
            "    batch 50/503  loss=2.7509\n",
            "    batch 100/503  loss=2.6496\n",
            "    batch 150/503  loss=2.7926\n",
            "    batch 200/503  loss=2.5092\n",
            "    batch 250/503  loss=2.6946\n",
            "    batch 300/503  loss=2.7008\n",
            "    batch 350/503  loss=2.7474\n",
            "    batch 400/503  loss=2.8318\n",
            "    batch 450/503  loss=2.7260\n",
            "    batch 500/503  loss=2.7403\n",
            "  train_loss=2.7164  val_loss=2.8494  val_CER=0.7359  val_WER=1.0354\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 16:\n",
            "    batch 0/503  loss=2.7284\n",
            "    batch 50/503  loss=2.8324\n",
            "    batch 100/503  loss=2.7489\n",
            "    batch 150/503  loss=2.7719\n",
            "    batch 200/503  loss=2.7248\n",
            "    batch 250/503  loss=2.8260\n",
            "    batch 300/503  loss=2.7081\n",
            "    batch 350/503  loss=2.8959\n",
            "    batch 400/503  loss=2.7928\n",
            "    batch 450/503  loss=2.8284\n",
            "    batch 500/503  loss=2.5727\n",
            "  train_loss=2.6786  val_loss=2.7929  val_CER=0.7187  val_WER=1.0302\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 17:\n",
            "    batch 0/503  loss=2.7486\n",
            "    batch 50/503  loss=2.7224\n",
            "    batch 100/503  loss=2.5394\n",
            "    batch 150/503  loss=2.8510\n",
            "    batch 200/503  loss=2.5626\n",
            "    batch 250/503  loss=2.5431\n",
            "    batch 300/503  loss=2.8139\n",
            "    batch 350/503  loss=2.5246\n",
            "    batch 400/503  loss=2.8062\n",
            "    batch 450/503  loss=2.7339\n",
            "    batch 500/503  loss=2.7444\n",
            "  train_loss=2.6328  val_loss=2.7699  val_CER=0.7236  val_WER=1.0214\n",
            "\n",
            "Epoch 18:\n",
            "    batch 0/503  loss=2.2508\n",
            "    batch 50/503  loss=2.1538\n",
            "    batch 100/503  loss=2.7375\n",
            "    batch 150/503  loss=2.5200\n",
            "    batch 200/503  loss=2.2031\n",
            "    batch 250/503  loss=2.7818\n",
            "    batch 300/503  loss=2.3717\n",
            "    batch 350/503  loss=2.3412\n",
            "    batch 400/503  loss=2.7111\n",
            "    batch 450/503  loss=2.6063\n",
            "    batch 500/503  loss=2.6139\n",
            "  train_loss=2.5876  val_loss=2.7680  val_CER=0.7288  val_WER=1.0021\n",
            "\n",
            "Epoch 19:\n",
            "    batch 0/503  loss=2.5358\n",
            "    batch 50/503  loss=2.4981\n",
            "    batch 100/503  loss=2.7083\n",
            "    batch 150/503  loss=2.2027\n",
            "    batch 200/503  loss=2.1418\n",
            "    batch 250/503  loss=2.5647\n",
            "    batch 300/503  loss=2.5401\n",
            "    batch 350/503  loss=2.6941\n",
            "    batch 400/503  loss=2.6922\n",
            "    batch 450/503  loss=2.3984\n",
            "    batch 500/503  loss=2.7442\n",
            "  train_loss=2.5361  val_loss=2.7945  val_CER=0.7180  val_WER=1.0162\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 20:\n",
            "    batch 0/503  loss=2.5664\n",
            "    batch 50/503  loss=2.7305\n",
            "    batch 100/503  loss=2.5205\n",
            "    batch 150/503  loss=1.7652\n",
            "    batch 200/503  loss=2.3655\n",
            "    batch 250/503  loss=2.8474\n",
            "    batch 300/503  loss=2.2599\n",
            "    batch 350/503  loss=2.5532\n",
            "    batch 400/503  loss=2.3126\n",
            "    batch 450/503  loss=2.2690\n",
            "    batch 500/503  loss=2.4759\n",
            "  train_loss=2.4683  val_loss=2.8183  val_CER=0.7146  val_WER=1.0083\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 21:\n",
            "    batch 0/503  loss=2.5274\n",
            "    batch 50/503  loss=2.6382\n",
            "    batch 100/503  loss=2.4178\n",
            "    batch 150/503  loss=2.6258\n",
            "    batch 200/503  loss=2.4669\n",
            "    batch 250/503  loss=2.5977\n",
            "    batch 300/503  loss=2.5989\n",
            "    batch 350/503  loss=2.5786\n",
            "    batch 400/503  loss=2.5512\n",
            "    batch 450/503  loss=1.7627\n",
            "    batch 500/503  loss=2.2196\n",
            "  train_loss=2.3997  val_loss=2.8736  val_CER=0.7127  val_WER=1.0150\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 22:\n",
            "    batch 0/503  loss=2.2995\n",
            "    batch 50/503  loss=2.4046\n",
            "    batch 100/503  loss=2.5648\n",
            "    batch 150/503  loss=2.3212\n",
            "    batch 200/503  loss=2.3733\n",
            "    batch 250/503  loss=2.2971\n",
            "    batch 300/503  loss=2.3511\n",
            "    batch 350/503  loss=2.5350\n",
            "    batch 400/503  loss=2.3701\n",
            "    batch 450/503  loss=2.4605\n",
            "    batch 500/503  loss=2.2630\n",
            "  train_loss=2.3192  val_loss=2.9154  val_CER=0.7089  val_WER=1.0152\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 23:\n",
            "    batch 0/503  loss=2.2081\n",
            "    batch 50/503  loss=2.4148\n",
            "    batch 100/503  loss=2.0749\n",
            "    batch 150/503  loss=2.1969\n",
            "    batch 200/503  loss=2.2614\n",
            "    batch 250/503  loss=1.9723\n",
            "    batch 300/503  loss=2.0913\n",
            "    batch 350/503  loss=2.3136\n",
            "    batch 400/503  loss=2.0391\n",
            "    batch 450/503  loss=2.6873\n",
            "    batch 500/503  loss=2.3456\n",
            "  train_loss=2.2389  val_loss=3.0449  val_CER=0.7033  val_WER=1.0226\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 24:\n",
            "    batch 0/503  loss=2.3699\n",
            "    batch 50/503  loss=2.1733\n",
            "    batch 100/503  loss=2.2421\n",
            "    batch 150/503  loss=2.0837\n",
            "    batch 200/503  loss=1.9476\n",
            "    batch 250/503  loss=2.2957\n",
            "    batch 300/503  loss=2.0486\n",
            "    batch 350/503  loss=2.2636\n",
            "    batch 400/503  loss=2.5601\n",
            "    batch 450/503  loss=2.1559\n",
            "    batch 500/503  loss=2.2889\n",
            "  train_loss=2.1548  val_loss=3.0112  val_CER=0.7046  val_WER=1.0145\n",
            "\n",
            "Epoch 25:\n",
            "    batch 0/503  loss=1.7736\n",
            "    batch 50/503  loss=1.8347\n",
            "    batch 100/503  loss=2.4901\n",
            "    batch 150/503  loss=1.5465\n",
            "    batch 200/503  loss=2.1850\n",
            "    batch 250/503  loss=1.9573\n",
            "    batch 300/503  loss=2.1594\n",
            "    batch 350/503  loss=2.2311\n",
            "    batch 400/503  loss=2.0973\n",
            "    batch 450/503  loss=2.2512\n",
            "    batch 500/503  loss=2.5340\n",
            "  train_loss=2.0646  val_loss=3.1567  val_CER=0.7019  val_WER=1.0162\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 26:\n",
            "    batch 0/503  loss=1.5563\n",
            "    batch 50/503  loss=1.8732\n",
            "    batch 100/503  loss=2.1652\n",
            "    batch 150/503  loss=2.3371\n",
            "    batch 200/503  loss=1.9954\n",
            "    batch 250/503  loss=1.5556\n",
            "    batch 300/503  loss=1.8132\n",
            "    batch 350/503  loss=2.2199\n",
            "    batch 400/503  loss=2.3118\n",
            "    batch 450/503  loss=2.2529\n",
            "    batch 500/503  loss=1.7293\n",
            "  train_loss=1.9806  val_loss=3.2389  val_CER=0.6966  val_WER=1.0164\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 27:\n",
            "    batch 0/503  loss=1.8334\n",
            "    batch 50/503  loss=1.4569\n",
            "    batch 100/503  loss=2.0877\n",
            "    batch 150/503  loss=1.7340\n",
            "    batch 200/503  loss=2.1263\n",
            "    batch 250/503  loss=1.8566\n",
            "    batch 300/503  loss=1.6490\n",
            "    batch 350/503  loss=1.6491\n",
            "    batch 400/503  loss=1.8856\n",
            "    batch 450/503  loss=2.4375\n",
            "    batch 500/503  loss=1.8198\n",
            "  train_loss=1.8755  val_loss=3.3246  val_CER=0.6886  val_WER=1.0316\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 28:\n",
            "    batch 0/503  loss=1.7917\n",
            "    batch 50/503  loss=2.0137\n",
            "    batch 100/503  loss=1.2728\n",
            "    batch 150/503  loss=2.0341\n",
            "    batch 200/503  loss=1.5090\n",
            "    batch 250/503  loss=1.6950\n",
            "    batch 300/503  loss=1.6306\n",
            "    batch 350/503  loss=1.7181\n",
            "    batch 400/503  loss=1.6278\n",
            "    batch 450/503  loss=1.9756\n",
            "    batch 500/503  loss=1.4253\n",
            "  train_loss=1.7899  val_loss=3.4626  val_CER=0.6865  val_WER=1.0387\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 29:\n",
            "    batch 0/503  loss=1.6738\n",
            "    batch 50/503  loss=1.9415\n",
            "    batch 100/503  loss=2.1241\n",
            "    batch 150/503  loss=1.3201\n",
            "    batch 200/503  loss=1.3079\n",
            "    batch 250/503  loss=1.9055\n",
            "    batch 300/503  loss=1.9755\n",
            "    batch 350/503  loss=2.0617\n",
            "    batch 400/503  loss=1.4157\n",
            "    batch 450/503  loss=1.9178\n",
            "    batch 500/503  loss=1.7065\n",
            "  train_loss=1.7013  val_loss=3.5475  val_CER=0.6904  val_WER=1.0280\n",
            "\n",
            "Epoch 30:\n",
            "    batch 0/503  loss=1.6614\n",
            "    batch 50/503  loss=1.6854\n",
            "    batch 100/503  loss=2.0294\n",
            "    batch 150/503  loss=1.5080\n",
            "    batch 200/503  loss=1.7062\n",
            "    batch 250/503  loss=1.7962\n",
            "    batch 300/503  loss=2.0598\n",
            "    batch 350/503  loss=1.4431\n",
            "    batch 400/503  loss=1.4869\n",
            "    batch 450/503  loss=1.8341\n",
            "    batch 500/503  loss=1.5506\n",
            "  train_loss=1.5972  val_loss=3.7527  val_CER=0.6872  val_WER=1.0423\n",
            "\n",
            "Epoch 31:\n",
            "    batch 0/503  loss=1.5530\n",
            "    batch 50/503  loss=1.6083\n",
            "    batch 100/503  loss=1.1035\n",
            "    batch 150/503  loss=1.2599\n",
            "    batch 200/503  loss=1.2316\n",
            "    batch 250/503  loss=1.3183\n",
            "    batch 300/503  loss=1.4377\n",
            "    batch 350/503  loss=1.4535\n",
            "    batch 400/503  loss=1.5981\n",
            "    batch 450/503  loss=1.4294\n",
            "    batch 500/503  loss=1.5776\n",
            "  train_loss=1.5127  val_loss=3.7844  val_CER=0.6901  val_WER=1.0333\n",
            "\n",
            "Epoch 32:\n",
            "    batch 0/503  loss=1.8229\n",
            "    batch 50/503  loss=1.5706\n",
            "    batch 100/503  loss=1.3757\n",
            "    batch 150/503  loss=1.7085\n",
            "    batch 200/503  loss=1.4325\n",
            "    batch 250/503  loss=1.5738\n",
            "    batch 300/503  loss=1.6741\n",
            "    batch 350/503  loss=1.5680\n",
            "    batch 400/503  loss=1.3227\n",
            "    batch 450/503  loss=1.3781\n",
            "    batch 500/503  loss=1.6269\n",
            "  train_loss=1.4349  val_loss=3.9620  val_CER=0.6905  val_WER=1.0371\n",
            "\n",
            "Epoch 33:\n",
            "    batch 0/503  loss=1.2493\n",
            "    batch 50/503  loss=1.8729\n",
            "    batch 100/503  loss=1.0905\n",
            "    batch 150/503  loss=1.9221\n",
            "    batch 200/503  loss=1.8573\n",
            "    batch 250/503  loss=1.2111\n",
            "    batch 300/503  loss=1.3865\n",
            "    batch 350/503  loss=1.5145\n",
            "    batch 400/503  loss=1.3869\n",
            "    batch 450/503  loss=0.9506\n",
            "    batch 500/503  loss=1.3616\n",
            "  train_loss=1.3324  val_loss=4.0821  val_CER=0.6900  val_WER=1.0330\n",
            "\n",
            "Epoch 34:\n",
            "    batch 0/503  loss=1.4307\n",
            "    batch 50/503  loss=1.3782\n",
            "    batch 100/503  loss=1.3179\n",
            "    batch 150/503  loss=1.6896\n",
            "    batch 200/503  loss=1.1891\n",
            "    batch 250/503  loss=0.9779\n",
            "    batch 300/503  loss=1.1831\n",
            "    batch 350/503  loss=1.5566\n",
            "    batch 400/503  loss=1.9588\n",
            "    batch 450/503  loss=1.3541\n",
            "    batch 500/503  loss=1.3387\n",
            "  train_loss=1.2566  val_loss=4.2697  val_CER=0.6892  val_WER=1.0359\n",
            "\n",
            "Epoch 35:\n",
            "    batch 0/503  loss=0.3942\n",
            "    batch 50/503  loss=1.3025\n",
            "    batch 100/503  loss=1.1842\n",
            "    batch 150/503  loss=1.1488\n",
            "    batch 200/503  loss=0.9986\n",
            "    batch 250/503  loss=1.6428\n",
            "    batch 300/503  loss=0.8794\n",
            "    batch 350/503  loss=1.1739\n",
            "    batch 400/503  loss=1.3622\n",
            "    batch 450/503  loss=0.9887\n",
            "    batch 500/503  loss=1.2353\n",
            "  train_loss=1.1674  val_loss=4.3938  val_CER=0.6900  val_WER=1.0442\n",
            "\n",
            "Epoch 36:\n",
            "    batch 0/503  loss=0.8615\n",
            "    batch 50/503  loss=0.9915\n",
            "    batch 100/503  loss=1.4436\n",
            "    batch 150/503  loss=0.8462\n",
            "    batch 200/503  loss=1.3762\n",
            "    batch 250/503  loss=0.9477\n",
            "    batch 300/503  loss=1.1880\n",
            "    batch 350/503  loss=1.0132\n",
            "    batch 400/503  loss=0.9480\n",
            "    batch 450/503  loss=0.8754\n",
            "    batch 500/503  loss=1.2435\n",
            "  train_loss=1.1007  val_loss=4.5085  val_CER=0.6916  val_WER=1.0549\n",
            "\n",
            "Epoch 37:\n",
            "    batch 0/503  loss=0.8381\n",
            "    batch 50/503  loss=1.4558\n",
            "    batch 100/503  loss=1.1453\n",
            "    batch 150/503  loss=1.0256\n",
            "    batch 200/503  loss=1.2618\n",
            "    batch 250/503  loss=1.4416\n",
            "    batch 300/503  loss=0.6522\n",
            "    batch 350/503  loss=1.1598\n",
            "    batch 400/503  loss=0.8492\n",
            "    batch 450/503  loss=1.0380\n",
            "    batch 500/503  loss=1.3086\n",
            "  train_loss=1.0233  val_loss=4.6248  val_CER=0.6847  val_WER=1.0447\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "\n",
            "Epoch 38:\n",
            "    batch 0/503  loss=0.5791\n",
            "    batch 50/503  loss=0.5328\n",
            "    batch 100/503  loss=1.1008\n",
            "    batch 150/503  loss=0.7774\n",
            "    batch 200/503  loss=1.0962\n",
            "    batch 250/503  loss=0.8681\n",
            "    batch 300/503  loss=0.7501\n",
            "    batch 350/503  loss=0.6780\n",
            "    batch 400/503  loss=0.7902\n",
            "    batch 450/503  loss=0.7911\n",
            "    batch 500/503  loss=0.9423\n",
            "  train_loss=0.9647  val_loss=4.9026  val_CER=0.6896  val_WER=1.0418\n",
            "\n",
            "Epoch 39:\n",
            "    batch 0/503  loss=0.8337\n",
            "    batch 50/503  loss=1.2715\n",
            "    batch 100/503  loss=0.7415\n",
            "    batch 150/503  loss=1.2365\n",
            "    batch 200/503  loss=0.7536\n",
            "    batch 250/503  loss=1.2878\n",
            "    batch 300/503  loss=0.7495\n",
            "    batch 350/503  loss=0.9173\n",
            "    batch 400/503  loss=0.9566\n",
            "    batch 450/503  loss=1.1222\n",
            "    batch 500/503  loss=0.5808\n",
            "  train_loss=0.8942  val_loss=5.0478  val_CER=0.6879  val_WER=1.0361\n",
            "\n",
            "Epoch 40:\n",
            "    batch 0/503  loss=1.0565\n",
            "    batch 50/503  loss=1.1255\n",
            "    batch 100/503  loss=0.8048\n",
            "    batch 150/503  loss=0.6801\n",
            "    batch 200/503  loss=0.8947\n",
            "    batch 250/503  loss=0.5869\n",
            "    batch 300/503  loss=0.6532\n",
            "    batch 350/503  loss=0.8373\n",
            "    batch 400/503  loss=0.4844\n",
            "    batch 450/503  loss=1.4034\n",
            "    batch 500/503  loss=0.8517\n",
            "  train_loss=0.8385  val_loss=5.1035  val_CER=0.6890  val_WER=1.0366\n",
            "[D0_preprocessed] BEST epoch=37  val_CER=0.6847  val_WER=1.0447  TEST_CER=0.6916  TEST_WER=1.0387\n",
            "\n",
            "\n",
            "########## D1_augmented on Augmented_HICMA ##########\n",
            "  -> Building dataloaders...\n",
            "  -> Dataloaders ready. Building model...\n",
            "  -> Model ready. Starting training...\n",
            "\n",
            "Epoch 01:\n",
            "    batch 0/2500  loss=2981.0774\n",
            "    batch 50/2500  loss=1980.5436\n",
            "    batch 100/2500  loss=2659.9165\n",
            "    batch 150/2500  loss=1098.1777\n",
            "    batch 200/2500  loss=684.4348\n",
            "    batch 250/2500  loss=211.9096\n",
            "    batch 300/2500  loss=156.7182\n",
            "    batch 350/2500  loss=116.7478\n",
            "    batch 400/2500  loss=358.6357\n",
            "    batch 450/2500  loss=31.5051\n",
            "    batch 500/2500  loss=72.8976\n",
            "    batch 550/2500  loss=1393.5055\n",
            "    batch 600/2500  loss=87.6436\n",
            "    batch 650/2500  loss=51.7539\n",
            "    batch 700/2500  loss=35.8641\n",
            "    batch 750/2500  loss=24.5777\n",
            "    batch 800/2500  loss=424.2877\n",
            "    batch 850/2500  loss=55.1975\n",
            "    batch 900/2500  loss=36.6575\n",
            "    batch 950/2500  loss=34.3452\n",
            "    batch 1000/2500  loss=52.5596\n",
            "    batch 1050/2500  loss=2601.2178\n",
            "    batch 1100/2500  loss=39.2313\n",
            "    batch 1150/2500  loss=33.0123\n",
            "    batch 1200/2500  loss=25.2473\n",
            "    batch 1250/2500  loss=35.7867\n",
            "    batch 1300/2500  loss=45.7167\n",
            "    batch 1350/2500  loss=40.3302\n",
            "    batch 1400/2500  loss=47.6570\n",
            "    batch 1450/2500  loss=53.2400\n",
            "    batch 1500/2500  loss=19.4898\n",
            "    batch 1550/2500  loss=28.4057\n",
            "    batch 1600/2500  loss=21.3828\n",
            "    batch 1650/2500  loss=13.0027\n",
            "    batch 1700/2500  loss=25.3496\n",
            "    batch 1750/2500  loss=16.8403\n",
            "    batch 1800/2500  loss=412.9395\n",
            "    batch 1850/2500  loss=35.9976\n",
            "    batch 1900/2500  loss=35.2783\n",
            "    batch 1950/2500  loss=22.1325\n",
            "    batch 2000/2500  loss=25.7046\n",
            "    batch 2050/2500  loss=14.9363\n",
            "    batch 2100/2500  loss=493.7545\n",
            "    batch 2150/2500  loss=1364.8716\n",
            "    batch 2200/2500  loss=20.6176\n",
            "    batch 2250/2500  loss=14.5350\n",
            "    batch 2300/2500  loss=22.0332\n",
            "    batch 2350/2500  loss=383.0461\n",
            "    batch 2400/2500  loss=25.8711\n",
            "    batch 2450/2500  loss=19.6790\n",
            "  train_loss=252.7845  val_loss=87.7953  val_CER=0.9692  val_WER=1.0000\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 02:\n",
            "    batch 0/2500  loss=28.8044\n",
            "    batch 50/2500  loss=33.8790\n",
            "    batch 100/2500  loss=14.1951\n",
            "    batch 150/2500  loss=13.5126\n",
            "    batch 200/2500  loss=480.3707\n",
            "    batch 250/2500  loss=21.9629\n",
            "    batch 300/2500  loss=470.0581\n",
            "    batch 350/2500  loss=22.2656\n",
            "    batch 400/2500  loss=978.7789\n",
            "    batch 450/2500  loss=25.1222\n",
            "    batch 500/2500  loss=47.2921\n",
            "    batch 550/2500  loss=28.7938\n",
            "    batch 600/2500  loss=13.6261\n",
            "    batch 650/2500  loss=33.7167\n",
            "    batch 700/2500  loss=42.6746\n",
            "    batch 750/2500  loss=13.4717\n",
            "    batch 800/2500  loss=46.2711\n",
            "    batch 850/2500  loss=20.4584\n",
            "    batch 900/2500  loss=21.5230\n",
            "    batch 950/2500  loss=395.9528\n",
            "    batch 1000/2500  loss=1002.1229\n",
            "    batch 1050/2500  loss=478.8162\n",
            "    batch 1100/2500  loss=21.2165\n",
            "    batch 1150/2500  loss=17.0288\n",
            "    batch 1200/2500  loss=16.3806\n",
            "    batch 1250/2500  loss=22.3261\n",
            "    batch 1300/2500  loss=27.6993\n",
            "    batch 1350/2500  loss=22.3680\n",
            "    batch 1400/2500  loss=35.1735\n",
            "    batch 1450/2500  loss=12.5089\n",
            "    batch 1500/2500  loss=12.1932\n",
            "    batch 1550/2500  loss=18.8220\n",
            "    batch 1600/2500  loss=1570.0800\n",
            "    batch 1650/2500  loss=20.3223\n",
            "    batch 1700/2500  loss=24.6019\n",
            "    batch 1750/2500  loss=29.3047\n",
            "    batch 1800/2500  loss=14.0983\n",
            "    batch 1850/2500  loss=484.5068\n",
            "    batch 1900/2500  loss=25.2931\n",
            "    batch 1950/2500  loss=29.0273\n",
            "    batch 2000/2500  loss=949.8342\n",
            "    batch 2050/2500  loss=28.2439\n",
            "    batch 2100/2500  loss=32.4863\n",
            "    batch 2150/2500  loss=22.8424\n",
            "    batch 2200/2500  loss=20.4563\n",
            "    batch 2250/2500  loss=27.8020\n",
            "    batch 2300/2500  loss=20.3145\n",
            "    batch 2350/2500  loss=12.1840\n",
            "    batch 2400/2500  loss=2435.0608\n",
            "    batch 2450/2500  loss=24.9654\n",
            "  train_loss=119.8321  val_loss=91.5202  val_CER=0.9687  val_WER=1.0000\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 03:\n",
            "    batch 0/2500  loss=18.8435\n",
            "    batch 50/2500  loss=22.5489\n",
            "    batch 100/2500  loss=16.6155\n",
            "    batch 150/2500  loss=20.6978\n",
            "    batch 200/2500  loss=20.6644\n",
            "    batch 250/2500  loss=18.0268\n",
            "    batch 300/2500  loss=32.5155\n",
            "    batch 350/2500  loss=16.9790\n",
            "    batch 400/2500  loss=34.0120\n",
            "    batch 450/2500  loss=20.4590\n",
            "    batch 500/2500  loss=13.4944\n",
            "    batch 550/2500  loss=19.5317\n",
            "    batch 600/2500  loss=17.4711\n",
            "    batch 650/2500  loss=14.2142\n",
            "    batch 700/2500  loss=419.1808\n",
            "    batch 750/2500  loss=13.2977\n",
            "    batch 800/2500  loss=9.2813\n",
            "    batch 850/2500  loss=22.3087\n",
            "    batch 900/2500  loss=31.1156\n",
            "    batch 950/2500  loss=32.3219\n",
            "    batch 1000/2500  loss=405.5362\n",
            "    batch 1050/2500  loss=20.1142\n",
            "    batch 1100/2500  loss=12.2208\n",
            "    batch 1150/2500  loss=12.2508\n",
            "    batch 1200/2500  loss=20.6033\n",
            "    batch 1250/2500  loss=25.8020\n",
            "    batch 1300/2500  loss=220.1935\n",
            "    batch 1350/2500  loss=13.5307\n",
            "    batch 1400/2500  loss=20.1286\n",
            "    batch 1450/2500  loss=29.6056\n",
            "    batch 1500/2500  loss=16.1728\n",
            "    batch 1550/2500  loss=16.1643\n",
            "    batch 1600/2500  loss=25.1675\n",
            "    batch 1650/2500  loss=14.3507\n",
            "    batch 1700/2500  loss=16.4492\n",
            "    batch 1750/2500  loss=467.1793\n",
            "    batch 1800/2500  loss=26.1185\n",
            "    batch 1850/2500  loss=22.9945\n",
            "    batch 1900/2500  loss=18.7731\n",
            "    batch 1950/2500  loss=24.9645\n",
            "    batch 2000/2500  loss=16.0493\n",
            "    batch 2050/2500  loss=11.4812\n",
            "    batch 2100/2500  loss=20.2282\n",
            "    batch 2150/2500  loss=13.7585\n",
            "    batch 2200/2500  loss=18.1144\n",
            "    batch 2250/2500  loss=20.3137\n",
            "    batch 2300/2500  loss=483.4684\n",
            "    batch 2350/2500  loss=12.1451\n",
            "    batch 2400/2500  loss=21.5983\n",
            "    batch 2450/2500  loss=23.2070\n",
            "  train_loss=117.5124  val_loss=91.9336  val_CER=0.9563  val_WER=1.0005\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 04:\n",
            "    batch 0/2500  loss=27.7149\n",
            "    batch 50/2500  loss=27.1119\n",
            "    batch 100/2500  loss=18.9054\n",
            "    batch 150/2500  loss=16.8490\n",
            "    batch 200/2500  loss=26.7139\n",
            "    batch 250/2500  loss=20.0246\n",
            "    batch 300/2500  loss=15.4577\n",
            "    batch 350/2500  loss=18.6772\n",
            "    batch 400/2500  loss=18.1152\n",
            "    batch 450/2500  loss=32.4243\n",
            "    batch 500/2500  loss=12.1325\n",
            "    batch 550/2500  loss=35.8215\n",
            "    batch 600/2500  loss=15.9660\n",
            "    batch 650/2500  loss=15.7085\n",
            "    batch 700/2500  loss=11.2900\n",
            "    batch 750/2500  loss=21.0404\n",
            "    batch 800/2500  loss=15.3716\n",
            "    batch 850/2500  loss=16.2280\n",
            "    batch 900/2500  loss=20.8191\n",
            "    batch 950/2500  loss=12.3783\n",
            "    batch 1000/2500  loss=13.5251\n",
            "    batch 1050/2500  loss=22.0790\n",
            "    batch 1100/2500  loss=17.0491\n",
            "    batch 1150/2500  loss=15.5522\n",
            "    batch 1200/2500  loss=24.6588\n",
            "    batch 1250/2500  loss=17.0117\n",
            "    batch 1300/2500  loss=473.3700\n",
            "    batch 1350/2500  loss=18.9681\n",
            "    batch 1400/2500  loss=22.8366\n",
            "    batch 1450/2500  loss=20.6644\n",
            "    batch 1500/2500  loss=23.3142\n",
            "    batch 1550/2500  loss=420.4018\n",
            "    batch 1600/2500  loss=29.0328\n",
            "    batch 1650/2500  loss=478.2674\n",
            "    batch 1700/2500  loss=286.1305\n",
            "    batch 1750/2500  loss=383.5444\n",
            "    batch 1800/2500  loss=19.5215\n",
            "    batch 1850/2500  loss=417.6490\n",
            "    batch 1900/2500  loss=20.8245\n",
            "    batch 1950/2500  loss=22.2203\n",
            "    batch 2000/2500  loss=18.6841\n",
            "    batch 2050/2500  loss=14.1025\n",
            "    batch 2100/2500  loss=13.8944\n",
            "    batch 2150/2500  loss=16.8403\n",
            "    batch 2200/2500  loss=16.5790\n",
            "    batch 2250/2500  loss=10.4272\n",
            "    batch 2300/2500  loss=19.4083\n",
            "    batch 2350/2500  loss=26.4004\n",
            "    batch 2400/2500  loss=15.0979\n",
            "    batch 2450/2500  loss=994.5045\n",
            "  train_loss=116.1079  val_loss=91.8492  val_CER=0.9763  val_WER=1.0000\n",
            "\n",
            "Epoch 05:\n",
            "    batch 0/2500  loss=19.0329\n",
            "    batch 50/2500  loss=29.0010\n",
            "    batch 100/2500  loss=12.3042\n",
            "    batch 150/2500  loss=22.2178\n",
            "    batch 200/2500  loss=16.7578\n",
            "    batch 250/2500  loss=17.6368\n",
            "    batch 300/2500  loss=17.8102\n",
            "    batch 350/2500  loss=12.0113\n",
            "    batch 400/2500  loss=24.6202\n",
            "    batch 450/2500  loss=17.5035\n",
            "    batch 500/2500  loss=22.7234\n",
            "    batch 550/2500  loss=10.8906\n",
            "    batch 600/2500  loss=20.5137\n",
            "    batch 650/2500  loss=21.2684\n",
            "    batch 700/2500  loss=21.3839\n",
            "    batch 750/2500  loss=19.2153\n",
            "    batch 800/2500  loss=25.8844\n",
            "    batch 850/2500  loss=450.1379\n",
            "    batch 900/2500  loss=15.7775\n",
            "    batch 950/2500  loss=15.4238\n",
            "    batch 1000/2500  loss=17.2617\n",
            "    batch 1050/2500  loss=22.1740\n",
            "    batch 1100/2500  loss=15.3678\n",
            "    batch 1150/2500  loss=16.8310\n",
            "    batch 1200/2500  loss=11.7011\n",
            "    batch 1250/2500  loss=14.9984\n",
            "    batch 1300/2500  loss=19.4879\n",
            "    batch 1350/2500  loss=14.7961\n",
            "    batch 1400/2500  loss=12.9758\n",
            "    batch 1450/2500  loss=20.0753\n",
            "    batch 1500/2500  loss=12.1183\n",
            "    batch 1550/2500  loss=17.1087\n",
            "    batch 1600/2500  loss=12.5906\n",
            "    batch 1650/2500  loss=12.7873\n",
            "    batch 1700/2500  loss=16.2589\n",
            "    batch 1750/2500  loss=14.5653\n",
            "    batch 1800/2500  loss=17.2107\n",
            "    batch 1850/2500  loss=992.8684\n",
            "    batch 1900/2500  loss=10.6807\n",
            "    batch 1950/2500  loss=19.3067\n",
            "    batch 2000/2500  loss=11.5827\n",
            "    batch 2050/2500  loss=24.9001\n",
            "    batch 2100/2500  loss=24.2904\n",
            "    batch 2150/2500  loss=501.0435\n",
            "    batch 2200/2500  loss=20.3478\n",
            "    batch 2250/2500  loss=20.2204\n",
            "    batch 2300/2500  loss=10.1819\n",
            "    batch 2350/2500  loss=14.4761\n",
            "    batch 2400/2500  loss=14.1316\n",
            "    batch 2450/2500  loss=18.6361\n",
            "  train_loss=115.1022  val_loss=89.9238  val_CER=0.9766  val_WER=1.0000\n",
            ">> Unfreezing Swin backbone for fine-tuning.\n",
            "\n",
            "Epoch 06:\n",
            "    batch 0/2500  loss=954.1748\n",
            "    batch 50/2500  loss=6.9860\n",
            "    batch 100/2500  loss=4.6883\n",
            "    batch 150/2500  loss=4.3808\n",
            "    batch 200/2500  loss=4.2366\n",
            "    batch 250/2500  loss=3.4263\n",
            "    batch 300/2500  loss=4.2692\n",
            "    batch 350/2500  loss=4.0441\n",
            "    batch 400/2500  loss=3.5909\n",
            "    batch 450/2500  loss=3.1745\n",
            "    batch 500/2500  loss=3.4178\n",
            "    batch 550/2500  loss=3.2937\n",
            "    batch 600/2500  loss=3739.7888\n",
            "    batch 650/2500  loss=2.5590\n",
            "    batch 700/2500  loss=1492.0829\n",
            "    batch 750/2500  loss=3.6731\n",
            "    batch 800/2500  loss=3.1320\n",
            "    batch 850/2500  loss=3.2597\n",
            "    batch 900/2500  loss=3.3610\n",
            "    batch 950/2500  loss=3.9550\n",
            "    batch 1000/2500  loss=2.9404\n",
            "    batch 1050/2500  loss=2.9364\n",
            "    batch 1100/2500  loss=3.7958\n",
            "    batch 1150/2500  loss=3.3883\n",
            "    batch 1200/2500  loss=2.8963\n",
            "    batch 1250/2500  loss=3.3781\n",
            "    batch 1300/2500  loss=2.8071\n",
            "    batch 1350/2500  loss=3.2368\n",
            "    batch 1400/2500  loss=2.9617\n",
            "    batch 1450/2500  loss=3.0474\n",
            "    batch 1500/2500  loss=2.6317\n",
            "    batch 1550/2500  loss=947.8668\n",
            "    batch 1600/2500  loss=3.0339\n",
            "    batch 1650/2500  loss=3.2825\n",
            "    batch 1700/2500  loss=2.9098\n",
            "    batch 1750/2500  loss=3.0305\n",
            "    batch 1800/2500  loss=3.0937\n",
            "    batch 1850/2500  loss=2.0576\n",
            "    batch 1900/2500  loss=2.6787\n",
            "    batch 1950/2500  loss=2.6513\n",
            "    batch 2000/2500  loss=2.4728\n",
            "    batch 2050/2500  loss=3.1250\n",
            "    batch 2100/2500  loss=3.6607\n",
            "    batch 2150/2500  loss=1.9336\n",
            "    batch 2200/2500  loss=2.2997\n",
            "    batch 2250/2500  loss=2.2019\n",
            "    batch 2300/2500  loss=2.8027\n",
            "    batch 2350/2500  loss=1.6325\n",
            "    batch 2400/2500  loss=3.0421\n",
            "    batch 2450/2500  loss=2.0374\n",
            "  train_loss=166.5734  val_loss=146.6386  val_CER=0.7969  val_WER=0.9955\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 07:\n",
            "    batch 0/2500  loss=2.2420\n",
            "    batch 50/2500  loss=2.7889\n",
            "    batch 100/2500  loss=2.3088\n",
            "    batch 150/2500  loss=2.7533\n",
            "    batch 200/2500  loss=3.0293\n",
            "    batch 250/2500  loss=822.2231\n",
            "    batch 300/2500  loss=2.8238\n",
            "    batch 350/2500  loss=2.6513\n",
            "    batch 400/2500  loss=2.3404\n",
            "    batch 450/2500  loss=3.6492\n",
            "    batch 500/2500  loss=2.7183\n",
            "    batch 550/2500  loss=2.3957\n",
            "    batch 600/2500  loss=2.6068\n",
            "    batch 650/2500  loss=2.1941\n",
            "    batch 700/2500  loss=3.1917\n",
            "    batch 750/2500  loss=1.0419\n",
            "    batch 800/2500  loss=3.6255\n",
            "    batch 850/2500  loss=1.8923\n",
            "    batch 900/2500  loss=2.4804\n",
            "    batch 950/2500  loss=1.8689\n",
            "    batch 1000/2500  loss=2.1716\n",
            "    batch 1050/2500  loss=2.3719\n",
            "    batch 1100/2500  loss=2.6724\n",
            "    batch 1150/2500  loss=2.2665\n",
            "    batch 1200/2500  loss=2.4245\n",
            "    batch 1250/2500  loss=1.9197\n",
            "    batch 1300/2500  loss=1.7224\n",
            "    batch 1350/2500  loss=2.2246\n",
            "    batch 1400/2500  loss=563.9648\n",
            "    batch 1450/2500  loss=2.5549\n",
            "    batch 1500/2500  loss=2.2113\n",
            "    batch 1550/2500  loss=1.5751\n",
            "    batch 1600/2500  loss=2.0180\n",
            "    batch 1650/2500  loss=2.0541\n",
            "    batch 1700/2500  loss=1.5937\n",
            "    batch 1750/2500  loss=540.7117\n",
            "    batch 1800/2500  loss=1.5206\n",
            "    batch 1850/2500  loss=1.5666\n",
            "    batch 1900/2500  loss=1.9166\n",
            "    batch 1950/2500  loss=1.5462\n",
            "    batch 2000/2500  loss=2.6885\n",
            "    batch 2050/2500  loss=564.2924\n",
            "    batch 2100/2500  loss=1.2364\n",
            "    batch 2150/2500  loss=1.5656\n",
            "    batch 2200/2500  loss=1.8887\n",
            "    batch 2250/2500  loss=0.8278\n",
            "    batch 2300/2500  loss=1.2165\n",
            "    batch 2350/2500  loss=1.8510\n",
            "    batch 2400/2500  loss=2.3858\n",
            "    batch 2450/2500  loss=2.0279\n",
            "  train_loss=137.1226  val_loss=134.3945  val_CER=0.7754  val_WER=0.9898\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 08:\n",
            "    batch 0/2500  loss=1.7403\n",
            "    batch 50/2500  loss=3.0929\n",
            "    batch 100/2500  loss=6.1053\n",
            "    batch 150/2500  loss=3.6089\n",
            "    batch 200/2500  loss=3.1677\n",
            "    batch 250/2500  loss=3.1852\n",
            "    batch 300/2500  loss=3.3162\n",
            "    batch 350/2500  loss=2.6821\n",
            "    batch 400/2500  loss=2.7980\n",
            "    batch 450/2500  loss=3.5287\n",
            "    batch 500/2500  loss=3.0415\n",
            "    batch 550/2500  loss=3.0299\n",
            "    batch 600/2500  loss=2.7303\n",
            "    batch 650/2500  loss=2.5882\n",
            "    batch 700/2500  loss=3.0472\n",
            "    batch 750/2500  loss=2.4006\n",
            "    batch 800/2500  loss=2.4992\n",
            "    batch 850/2500  loss=2.9017\n",
            "    batch 900/2500  loss=2.8654\n",
            "    batch 950/2500  loss=2.3603\n",
            "    batch 1000/2500  loss=2.2170\n",
            "    batch 1050/2500  loss=2.1817\n",
            "    batch 1100/2500  loss=1.4745\n",
            "    batch 1150/2500  loss=2.4427\n",
            "    batch 1200/2500  loss=2.4725\n",
            "    batch 1250/2500  loss=1.8696\n",
            "    batch 1300/2500  loss=2.5443\n",
            "    batch 1350/2500  loss=2.2379\n",
            "    batch 1400/2500  loss=1.7887\n",
            "    batch 1450/2500  loss=2.2300\n",
            "    batch 1500/2500  loss=1.9812\n",
            "    batch 1550/2500  loss=0.8105\n",
            "    batch 1600/2500  loss=2.1170\n",
            "    batch 1650/2500  loss=2.1737\n",
            "    batch 1700/2500  loss=2.1154\n",
            "    batch 1750/2500  loss=1.8234\n",
            "    batch 1800/2500  loss=2.6807\n",
            "    batch 1850/2500  loss=2.4296\n",
            "    batch 1900/2500  loss=0.5802\n",
            "    batch 1950/2500  loss=2.4936\n",
            "    batch 2000/2500  loss=2.8818\n",
            "    batch 2050/2500  loss=1.7376\n",
            "    batch 2100/2500  loss=1.3942\n",
            "    batch 2150/2500  loss=1.4860\n",
            "    batch 2200/2500  loss=1.5594\n",
            "    batch 2250/2500  loss=1.6222\n",
            "    batch 2300/2500  loss=1.8023\n",
            "    batch 2350/2500  loss=1.5578\n",
            "    batch 2400/2500  loss=1.6992\n",
            "    batch 2450/2500  loss=1.6981\n",
            "  train_loss=20.6774  val_loss=2.8862  val_CER=0.7230  val_WER=1.0273\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 09:\n",
            "    batch 0/2500  loss=1.9742\n",
            "    batch 50/2500  loss=1.9370\n",
            "    batch 100/2500  loss=1.6852\n",
            "    batch 150/2500  loss=1.5924\n",
            "    batch 200/2500  loss=1.3519\n",
            "    batch 250/2500  loss=1.6567\n",
            "    batch 300/2500  loss=0.8194\n",
            "    batch 350/2500  loss=1.8625\n",
            "    batch 400/2500  loss=1.8454\n",
            "    batch 450/2500  loss=1.3622\n",
            "    batch 500/2500  loss=2.5071\n",
            "    batch 550/2500  loss=2.2447\n",
            "    batch 600/2500  loss=1.1607\n",
            "    batch 650/2500  loss=1.7812\n",
            "    batch 700/2500  loss=2.0408\n",
            "    batch 750/2500  loss=2.1289\n",
            "    batch 800/2500  loss=1.8708\n",
            "    batch 850/2500  loss=1689.3098\n",
            "    batch 900/2500  loss=2.4508\n",
            "    batch 950/2500  loss=1.6512\n",
            "    batch 1000/2500  loss=1.7407\n",
            "    batch 1050/2500  loss=653.8753\n",
            "    batch 1100/2500  loss=2.2577\n",
            "    batch 1150/2500  loss=1.4247\n",
            "    batch 1200/2500  loss=1.4615\n",
            "    batch 1250/2500  loss=1.8295\n",
            "    batch 1300/2500  loss=1.8857\n",
            "    batch 1350/2500  loss=1.4878\n",
            "    batch 1400/2500  loss=1.7474\n",
            "    batch 1450/2500  loss=1.0873\n",
            "    batch 1500/2500  loss=2.6037\n",
            "    batch 1550/2500  loss=2.0470\n",
            "    batch 1600/2500  loss=2.0106\n",
            "    batch 1650/2500  loss=2.3911\n",
            "    batch 1700/2500  loss=2.1566\n",
            "    batch 1750/2500  loss=2.0565\n",
            "    batch 1800/2500  loss=0.4269\n",
            "    batch 1850/2500  loss=1.1251\n",
            "    batch 1900/2500  loss=0.7640\n",
            "    batch 1950/2500  loss=1.3962\n",
            "    batch 2000/2500  loss=0.6026\n",
            "    batch 2050/2500  loss=0.7502\n",
            "    batch 2100/2500  loss=1.5439\n",
            "    batch 2150/2500  loss=1.0065\n",
            "    batch 2200/2500  loss=1.1036\n",
            "    batch 2250/2500  loss=1.5226\n",
            "    batch 2300/2500  loss=1.1812\n",
            "    batch 2350/2500  loss=0.6009\n",
            "    batch 2400/2500  loss=1.3073\n",
            "    batch 2450/2500  loss=1.3235\n",
            "  train_loss=19.6976  val_loss=2.8189  val_CER=0.7164  val_WER=1.0404\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 10:\n",
            "    batch 0/2500  loss=1.3942\n",
            "    batch 50/2500  loss=1.1779\n",
            "    batch 100/2500  loss=2.0741\n",
            "    batch 150/2500  loss=1.3650\n",
            "    batch 200/2500  loss=2.2172\n",
            "    batch 250/2500  loss=2.5995\n",
            "    batch 300/2500  loss=2.1190\n",
            "    batch 350/2500  loss=2.2746\n",
            "    batch 400/2500  loss=1.4794\n",
            "    batch 450/2500  loss=1.0870\n",
            "    batch 500/2500  loss=2.6316\n",
            "    batch 550/2500  loss=1.8957\n",
            "    batch 600/2500  loss=1.3651\n",
            "    batch 650/2500  loss=1.7939\n",
            "    batch 700/2500  loss=1.0546\n",
            "    batch 750/2500  loss=2.1189\n",
            "    batch 800/2500  loss=1.5572\n",
            "    batch 850/2500  loss=1.2776\n",
            "    batch 900/2500  loss=0.6244\n",
            "    batch 950/2500  loss=1.1617\n",
            "    batch 1000/2500  loss=1.7708\n",
            "    batch 1050/2500  loss=1.0248\n",
            "    batch 1100/2500  loss=1.7780\n",
            "    batch 1150/2500  loss=2.5295\n",
            "    batch 1200/2500  loss=1.1082\n",
            "    batch 1250/2500  loss=1.3375\n",
            "    batch 1300/2500  loss=1.8233\n",
            "    batch 1350/2500  loss=1.8138\n",
            "    batch 1400/2500  loss=1.3317\n",
            "    batch 1450/2500  loss=1.2718\n",
            "    batch 1500/2500  loss=1.7818\n",
            "    batch 1550/2500  loss=1.8344\n",
            "    batch 1600/2500  loss=1.0673\n",
            "    batch 1650/2500  loss=1.8525\n",
            "    batch 1700/2500  loss=1.0963\n",
            "    batch 1750/2500  loss=1.4583\n",
            "    batch 1800/2500  loss=1.9621\n",
            "    batch 1850/2500  loss=1.7089\n",
            "    batch 1900/2500  loss=1.7642\n",
            "    batch 1950/2500  loss=1.7236\n",
            "    batch 2000/2500  loss=1.0211\n",
            "    batch 2050/2500  loss=1.1004\n",
            "    batch 2100/2500  loss=2.5013\n",
            "    batch 2150/2500  loss=1.3027\n",
            "    batch 2200/2500  loss=2.2360\n",
            "    batch 2250/2500  loss=1.1120\n",
            "    batch 2300/2500  loss=2.2765\n",
            "    batch 2350/2500  loss=1.1002\n",
            "    batch 2400/2500  loss=0.9761\n",
            "    batch 2450/2500  loss=1.4492\n",
            "  train_loss=18.6772  val_loss=2.7678  val_CER=0.7297  val_WER=0.9936\n",
            "\n",
            "Epoch 11:\n",
            "    batch 0/2500  loss=1.5634\n",
            "    batch 50/2500  loss=1.3136\n",
            "    batch 100/2500  loss=1.8622\n",
            "    batch 150/2500  loss=1.7080\n",
            "    batch 200/2500  loss=1.0283\n",
            "    batch 250/2500  loss=1.7472\n",
            "    batch 300/2500  loss=1.0700\n",
            "    batch 350/2500  loss=0.7998\n",
            "    batch 400/2500  loss=1.7873\n",
            "    batch 450/2500  loss=1.2624\n",
            "    batch 500/2500  loss=1.7230\n",
            "    batch 550/2500  loss=1.1181\n",
            "    batch 600/2500  loss=0.3128\n",
            "    batch 650/2500  loss=2.5639\n",
            "    batch 700/2500  loss=1.8450\n",
            "    batch 750/2500  loss=1.3960\n",
            "    batch 800/2500  loss=1.7211\n",
            "    batch 850/2500  loss=1.8372\n",
            "    batch 900/2500  loss=1.4185\n",
            "    batch 950/2500  loss=0.6609\n",
            "    batch 1000/2500  loss=0.7742\n",
            "    batch 1050/2500  loss=1.1514\n",
            "    batch 1100/2500  loss=1.1747\n",
            "    batch 1150/2500  loss=1.4959\n",
            "    batch 1200/2500  loss=0.8545\n",
            "    batch 1250/2500  loss=1.4648\n",
            "    batch 1300/2500  loss=1.0260\n",
            "    batch 1350/2500  loss=1.4319\n",
            "    batch 1400/2500  loss=622.1650\n",
            "    batch 1450/2500  loss=1.4448\n",
            "    batch 1500/2500  loss=1.7020\n",
            "    batch 1550/2500  loss=1.0888\n",
            "    batch 1600/2500  loss=1.5181\n",
            "    batch 1650/2500  loss=1.2336\n",
            "    batch 1700/2500  loss=1.6513\n",
            "    batch 1750/2500  loss=1.4340\n",
            "    batch 1800/2500  loss=0.4679\n",
            "    batch 1850/2500  loss=1.1302\n",
            "    batch 1900/2500  loss=0.8683\n",
            "    batch 1950/2500  loss=474.3053\n",
            "    batch 2000/2500  loss=0.7056\n",
            "    batch 2050/2500  loss=0.8609\n",
            "    batch 2100/2500  loss=0.9778\n",
            "    batch 2150/2500  loss=1.5311\n",
            "    batch 2200/2500  loss=1.0485\n",
            "    batch 2250/2500  loss=1.3189\n",
            "    batch 2300/2500  loss=1.0224\n",
            "    batch 2350/2500  loss=1.8088\n",
            "    batch 2400/2500  loss=1.5455\n",
            "    batch 2450/2500  loss=0.9331\n",
            "  train_loss=11.2067  val_loss=2.8154  val_CER=0.7137  val_WER=1.0021\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 12:\n",
            "    batch 0/2500  loss=0.4322\n",
            "    batch 50/2500  loss=1.0731\n",
            "    batch 100/2500  loss=2.3335\n",
            "    batch 150/2500  loss=1.8656\n",
            "    batch 200/2500  loss=0.9840\n",
            "    batch 250/2500  loss=1.0869\n",
            "    batch 300/2500  loss=1.6225\n",
            "    batch 350/2500  loss=1.6441\n",
            "    batch 400/2500  loss=0.5358\n",
            "    batch 450/2500  loss=0.8480\n",
            "    batch 500/2500  loss=1.3782\n",
            "    batch 550/2500  loss=2.2898\n",
            "    batch 600/2500  loss=1.1084\n",
            "    batch 650/2500  loss=0.7672\n",
            "    batch 700/2500  loss=1.5670\n",
            "    batch 750/2500  loss=1.5881\n",
            "    batch 800/2500  loss=1.1628\n",
            "    batch 850/2500  loss=0.3817\n",
            "    batch 900/2500  loss=0.3041\n",
            "    batch 950/2500  loss=1.3280\n",
            "    batch 1000/2500  loss=0.1313\n",
            "    batch 1050/2500  loss=1.0909\n",
            "    batch 1100/2500  loss=1.7599\n",
            "    batch 1150/2500  loss=0.9894\n",
            "    batch 1200/2500  loss=0.4518\n",
            "    batch 1250/2500  loss=1.1881\n",
            "    batch 1300/2500  loss=1.6589\n",
            "    batch 1350/2500  loss=1.0633\n",
            "    batch 1400/2500  loss=1.3925\n",
            "    batch 1450/2500  loss=1.7061\n",
            "    batch 1500/2500  loss=1.1481\n",
            "    batch 1550/2500  loss=1.5568\n",
            "    batch 1600/2500  loss=1.1648\n",
            "    batch 1650/2500  loss=1.2115\n",
            "    batch 1700/2500  loss=1.5510\n",
            "    batch 1750/2500  loss=0.7789\n",
            "    batch 1800/2500  loss=1.3895\n",
            "    batch 1850/2500  loss=1.4157\n",
            "    batch 1900/2500  loss=1.7478\n",
            "    batch 1950/2500  loss=1.0227\n",
            "    batch 2000/2500  loss=1.6732\n",
            "    batch 2050/2500  loss=1.0572\n",
            "    batch 2100/2500  loss=1.8623\n",
            "    batch 2150/2500  loss=1.3051\n",
            "    batch 2200/2500  loss=1.4029\n",
            "    batch 2250/2500  loss=1.2580\n",
            "    batch 2300/2500  loss=0.6270\n",
            "    batch 2350/2500  loss=1.3372\n",
            "    batch 2400/2500  loss=1.0758\n",
            "    batch 2450/2500  loss=1.3867\n",
            "  train_loss=14.8430  val_loss=2.9597  val_CER=0.6701  val_WER=1.0340\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 13:\n",
            "    batch 0/2500  loss=1.1199\n",
            "    batch 50/2500  loss=0.6440\n",
            "    batch 100/2500  loss=0.5897\n",
            "    batch 150/2500  loss=0.8295\n",
            "    batch 200/2500  loss=1.1118\n",
            "    batch 250/2500  loss=1.2783\n",
            "    batch 300/2500  loss=0.6202\n",
            "    batch 350/2500  loss=0.9930\n",
            "    batch 400/2500  loss=0.7589\n",
            "    batch 450/2500  loss=0.9619\n",
            "    batch 500/2500  loss=1.5101\n",
            "    batch 550/2500  loss=1.3828\n",
            "    batch 600/2500  loss=0.7734\n",
            "    batch 650/2500  loss=1.1688\n",
            "    batch 700/2500  loss=0.4207\n",
            "    batch 750/2500  loss=1.0081\n",
            "    batch 800/2500  loss=1.0079\n",
            "    batch 850/2500  loss=0.9032\n",
            "    batch 900/2500  loss=16.1799\n",
            "    batch 950/2500  loss=0.6834\n",
            "    batch 1000/2500  loss=0.5636\n",
            "    batch 1050/2500  loss=1.0960\n",
            "    batch 1100/2500  loss=1.5599\n",
            "    batch 1150/2500  loss=1.2153\n",
            "    batch 1200/2500  loss=1.9262\n",
            "    batch 1250/2500  loss=1.1006\n",
            "    batch 1300/2500  loss=1.1886\n",
            "    batch 1350/2500  loss=0.4634\n",
            "    batch 1400/2500  loss=0.0902\n",
            "    batch 1450/2500  loss=1.6412\n",
            "    batch 1500/2500  loss=0.7478\n",
            "    batch 1550/2500  loss=2.6288\n",
            "    batch 1600/2500  loss=2.0101\n",
            "    batch 1650/2500  loss=0.6797\n",
            "    batch 1700/2500  loss=0.2523\n",
            "    batch 1750/2500  loss=1.1193\n",
            "    batch 1800/2500  loss=0.9447\n",
            "    batch 1850/2500  loss=2.2899\n",
            "    batch 1900/2500  loss=0.6506\n",
            "    batch 1950/2500  loss=1.6372\n",
            "    batch 2000/2500  loss=0.7820\n",
            "    batch 2050/2500  loss=1.0950\n",
            "    batch 2100/2500  loss=0.5290\n",
            "    batch 2150/2500  loss=0.2002\n",
            "    batch 2200/2500  loss=1.6903\n",
            "    batch 2250/2500  loss=1.4190\n",
            "    batch 2300/2500  loss=1.5913\n",
            "    batch 2350/2500  loss=1.6574\n",
            "    batch 2400/2500  loss=0.5203\n",
            "    batch 2450/2500  loss=1.8082\n",
            "  train_loss=8.3450  val_loss=2.9120  val_CER=0.6843  val_WER=1.0135\n",
            "\n",
            "Epoch 14:\n",
            "    batch 0/2500  loss=0.8871\n",
            "    batch 50/2500  loss=1.0839\n",
            "    batch 100/2500  loss=0.4010\n",
            "    batch 150/2500  loss=1.6875\n",
            "    batch 200/2500  loss=0.9682\n",
            "    batch 250/2500  loss=1.3804\n",
            "    batch 300/2500  loss=1.3873\n",
            "    batch 350/2500  loss=2.5562\n",
            "    batch 400/2500  loss=0.9669\n",
            "    batch 450/2500  loss=0.4572\n",
            "    batch 500/2500  loss=1.0145\n",
            "    batch 550/2500  loss=0.7320\n",
            "    batch 600/2500  loss=0.3379\n",
            "    batch 650/2500  loss=0.7356\n",
            "    batch 700/2500  loss=1.2573\n",
            "    batch 750/2500  loss=0.4606\n",
            "    batch 800/2500  loss=1.1877\n",
            "    batch 850/2500  loss=0.6352\n",
            "    batch 900/2500  loss=1.2453\n",
            "    batch 950/2500  loss=1.1206\n",
            "    batch 1000/2500  loss=0.3977\n",
            "    batch 1050/2500  loss=0.5813\n",
            "    batch 1100/2500  loss=1.3381\n",
            "    batch 1150/2500  loss=0.7978\n",
            "    batch 1200/2500  loss=0.8033\n",
            "    batch 1250/2500  loss=1.0475\n",
            "    batch 1300/2500  loss=0.9438\n",
            "    batch 1350/2500  loss=0.9913\n",
            "    batch 1400/2500  loss=0.9476\n",
            "    batch 1450/2500  loss=0.9561\n",
            "    batch 1500/2500  loss=0.7112\n",
            "    batch 1550/2500  loss=1.1355\n",
            "    batch 1600/2500  loss=1.3412\n",
            "    batch 1650/2500  loss=1.0104\n",
            "    batch 1700/2500  loss=0.3544\n",
            "    batch 1750/2500  loss=1.2006\n",
            "    batch 1800/2500  loss=1.5485\n",
            "    batch 1850/2500  loss=0.7830\n",
            "    batch 1900/2500  loss=0.6985\n",
            "    batch 1950/2500  loss=1.9667\n",
            "    batch 2000/2500  loss=1.2204\n",
            "    batch 2050/2500  loss=0.8231\n",
            "    batch 2100/2500  loss=1.5834\n",
            "    batch 2150/2500  loss=1.0329\n",
            "    batch 2200/2500  loss=1.0898\n",
            "    batch 2250/2500  loss=1.2683\n",
            "    batch 2300/2500  loss=1.3094\n",
            "    batch 2350/2500  loss=0.7768\n",
            "    batch 2400/2500  loss=0.6867\n",
            "    batch 2450/2500  loss=1.4643\n",
            "  train_loss=4.8894  val_loss=2.9807  val_CER=0.6781  val_WER=1.0192\n",
            "\n",
            "Epoch 15:\n",
            "    batch 0/2500  loss=1.2479\n",
            "    batch 50/2500  loss=0.3021\n",
            "    batch 100/2500  loss=1.0933\n",
            "    batch 150/2500  loss=1.3161\n",
            "    batch 200/2500  loss=0.5936\n",
            "    batch 250/2500  loss=4.1655\n",
            "    batch 300/2500  loss=0.8051\n",
            "    batch 350/2500  loss=1.1583\n",
            "    batch 400/2500  loss=0.6641\n",
            "    batch 450/2500  loss=1.6218\n",
            "    batch 500/2500  loss=0.8378\n",
            "    batch 550/2500  loss=0.8101\n",
            "    batch 600/2500  loss=1.6756\n",
            "    batch 650/2500  loss=0.5218\n",
            "    batch 700/2500  loss=0.4045\n",
            "    batch 750/2500  loss=0.7090\n",
            "    batch 800/2500  loss=0.7954\n",
            "    batch 850/2500  loss=1.3260\n",
            "    batch 900/2500  loss=1.8160\n",
            "    batch 950/2500  loss=0.8050\n",
            "    batch 1000/2500  loss=0.3852\n",
            "    batch 1050/2500  loss=830.6528\n",
            "    batch 1100/2500  loss=0.9898\n",
            "    batch 1150/2500  loss=0.7160\n",
            "    batch 1200/2500  loss=0.6465\n",
            "    batch 1250/2500  loss=2.0903\n",
            "    batch 1300/2500  loss=0.5482\n",
            "    batch 1350/2500  loss=1.6560\n",
            "    batch 1400/2500  loss=1.2530\n",
            "    batch 1450/2500  loss=0.0028\n",
            "    batch 1500/2500  loss=0.7815\n",
            "    batch 1550/2500  loss=1.0233\n",
            "    batch 1600/2500  loss=1.5682\n",
            "    batch 1650/2500  loss=1.0643\n",
            "    batch 1700/2500  loss=0.6976\n",
            "    batch 1750/2500  loss=0.6320\n",
            "    batch 1800/2500  loss=0.7091\n",
            "    batch 1850/2500  loss=1.0799\n",
            "    batch 1900/2500  loss=0.7421\n",
            "    batch 1950/2500  loss=0.7667\n",
            "    batch 2000/2500  loss=0.5661\n",
            "    batch 2050/2500  loss=1.3460\n",
            "    batch 2100/2500  loss=0.4174\n",
            "    batch 2150/2500  loss=1.3124\n",
            "    batch 2200/2500  loss=1.0202\n",
            "    batch 2250/2500  loss=0.7839\n",
            "    batch 2300/2500  loss=1.7240\n",
            "    batch 2350/2500  loss=1.4955\n",
            "    batch 2400/2500  loss=0.7011\n",
            "    batch 2450/2500  loss=0.8924\n",
            "  train_loss=7.7349  val_loss=2.8905  val_CER=0.6924  val_WER=1.0038\n",
            "\n",
            "Epoch 16:\n",
            "    batch 0/2500  loss=1.6013\n",
            "    batch 50/2500  loss=0.6696\n",
            "    batch 100/2500  loss=0.7699\n",
            "    batch 150/2500  loss=0.3382\n",
            "    batch 200/2500  loss=0.6565\n",
            "    batch 250/2500  loss=0.7303\n",
            "    batch 300/2500  loss=0.6359\n",
            "    batch 350/2500  loss=0.5532\n",
            "    batch 400/2500  loss=1.1019\n",
            "    batch 450/2500  loss=0.4216\n",
            "    batch 500/2500  loss=0.4339\n",
            "    batch 550/2500  loss=0.7738\n",
            "    batch 600/2500  loss=0.7079\n",
            "    batch 650/2500  loss=0.4308\n",
            "    batch 700/2500  loss=0.7259\n",
            "    batch 750/2500  loss=1.4689\n",
            "    batch 800/2500  loss=0.5530\n",
            "    batch 850/2500  loss=0.2127\n",
            "    batch 900/2500  loss=0.6382\n",
            "    batch 950/2500  loss=0.9511\n",
            "    batch 1000/2500  loss=0.8951\n",
            "    batch 1050/2500  loss=0.7224\n",
            "    batch 1100/2500  loss=0.6034\n",
            "    batch 1150/2500  loss=0.5931\n",
            "    batch 1200/2500  loss=1.5482\n",
            "    batch 1250/2500  loss=1.5048\n",
            "    batch 1300/2500  loss=1.0033\n",
            "    batch 1350/2500  loss=1.5922\n",
            "    batch 1400/2500  loss=1.3027\n",
            "    batch 1450/2500  loss=0.5344\n",
            "    batch 1500/2500  loss=0.6652\n",
            "    batch 1550/2500  loss=1.0909\n",
            "    batch 1600/2500  loss=0.8874\n",
            "    batch 1650/2500  loss=0.8752\n",
            "    batch 1700/2500  loss=1.1397\n",
            "    batch 1750/2500  loss=1.4581\n",
            "    batch 1800/2500  loss=0.9181\n",
            "    batch 1850/2500  loss=1.3085\n",
            "    batch 1900/2500  loss=1.3190\n",
            "    batch 1950/2500  loss=0.8623\n",
            "    batch 2000/2500  loss=1.2956\n",
            "    batch 2050/2500  loss=0.3049\n",
            "    batch 2100/2500  loss=1.0533\n",
            "    batch 2150/2500  loss=0.3971\n",
            "    batch 2200/2500  loss=0.7025\n",
            "    batch 2250/2500  loss=1.2369\n",
            "    batch 2300/2500  loss=0.7116\n",
            "    batch 2350/2500  loss=1.4021\n",
            "    batch 2400/2500  loss=0.8603\n",
            "    batch 2450/2500  loss=1.2081\n",
            "  train_loss=7.4097  val_loss=3.0227  val_CER=0.6784  val_WER=1.0162\n",
            "\n",
            "Epoch 17:\n",
            "    batch 0/2500  loss=0.6200\n",
            "    batch 50/2500  loss=0.8671\n",
            "    batch 100/2500  loss=0.5352\n",
            "    batch 150/2500  loss=0.4295\n",
            "    batch 200/2500  loss=0.9549\n",
            "    batch 250/2500  loss=0.7224\n",
            "    batch 300/2500  loss=1.0873\n",
            "    batch 350/2500  loss=0.6190\n",
            "    batch 400/2500  loss=0.9434\n",
            "    batch 450/2500  loss=0.6936\n",
            "    batch 500/2500  loss=1.1370\n",
            "    batch 550/2500  loss=1.3919\n",
            "    batch 600/2500  loss=1.0227\n",
            "    batch 650/2500  loss=1.2663\n",
            "    batch 700/2500  loss=1.2611\n",
            "    batch 750/2500  loss=0.1499\n",
            "    batch 800/2500  loss=0.4049\n",
            "    batch 850/2500  loss=1.0230\n",
            "    batch 900/2500  loss=1.0221\n",
            "    batch 950/2500  loss=0.3912\n",
            "    batch 1000/2500  loss=1.0244\n",
            "    batch 1050/2500  loss=1.0679\n",
            "    batch 1100/2500  loss=0.8427\n",
            "    batch 1150/2500  loss=0.4520\n",
            "    batch 1200/2500  loss=0.9815\n",
            "    batch 1250/2500  loss=1.1708\n",
            "    batch 1300/2500  loss=1.0914\n",
            "    batch 1350/2500  loss=0.8119\n",
            "    batch 1400/2500  loss=0.9418\n",
            "    batch 1450/2500  loss=1.2444\n",
            "    batch 1500/2500  loss=0.7264\n",
            "    batch 1550/2500  loss=0.5499\n",
            "    batch 1600/2500  loss=0.4744\n",
            "    batch 1650/2500  loss=1.0359\n",
            "    batch 1700/2500  loss=0.6893\n",
            "    batch 1750/2500  loss=1.0455\n",
            "    batch 1800/2500  loss=1.4176\n",
            "    batch 1850/2500  loss=0.4084\n",
            "    batch 1900/2500  loss=0.0303\n",
            "    batch 1950/2500  loss=1.0335\n",
            "    batch 2000/2500  loss=0.6131\n",
            "    batch 2050/2500  loss=0.3434\n",
            "    batch 2100/2500  loss=1.0292\n",
            "    batch 2150/2500  loss=0.0012\n",
            "    batch 2200/2500  loss=1.2776\n",
            "    batch 2250/2500  loss=0.0620\n",
            "    batch 2300/2500  loss=0.3305\n",
            "    batch 2350/2500  loss=0.9842\n",
            "    batch 2400/2500  loss=0.0091\n",
            "    batch 2450/2500  loss=0.6898\n",
            "  train_loss=4.7015  val_loss=3.1326  val_CER=0.6614  val_WER=1.0083\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 18:\n",
            "    batch 0/2500  loss=0.3533\n",
            "    batch 50/2500  loss=1.1413\n",
            "    batch 100/2500  loss=0.2103\n",
            "    batch 150/2500  loss=0.6532\n",
            "    batch 200/2500  loss=1.2111\n",
            "    batch 250/2500  loss=0.8524\n",
            "    batch 300/2500  loss=0.5198\n",
            "    batch 350/2500  loss=0.4426\n",
            "    batch 400/2500  loss=0.7191\n",
            "    batch 450/2500  loss=1.5840\n",
            "    batch 500/2500  loss=0.0121\n",
            "    batch 550/2500  loss=0.0647\n",
            "    batch 600/2500  loss=0.9180\n",
            "    batch 650/2500  loss=1.1658\n",
            "    batch 700/2500  loss=1.0873\n",
            "    batch 750/2500  loss=0.3840\n",
            "    batch 800/2500  loss=0.9954\n",
            "    batch 850/2500  loss=1.5960\n",
            "    batch 900/2500  loss=0.9187\n",
            "    batch 950/2500  loss=0.7089\n",
            "    batch 1000/2500  loss=0.8048\n",
            "    batch 1050/2500  loss=0.5663\n",
            "    batch 1100/2500  loss=0.7025\n",
            "    batch 1150/2500  loss=0.9510\n",
            "    batch 1200/2500  loss=1.3058\n",
            "    batch 1250/2500  loss=0.6891\n",
            "    batch 1300/2500  loss=0.9051\n",
            "    batch 1350/2500  loss=0.5115\n",
            "    batch 1400/2500  loss=0.8884\n",
            "    batch 1450/2500  loss=0.4287\n",
            "    batch 1500/2500  loss=0.4273\n",
            "    batch 1550/2500  loss=0.4234\n",
            "    batch 1600/2500  loss=0.5785\n",
            "    batch 1650/2500  loss=1.1211\n",
            "    batch 1700/2500  loss=0.8492\n",
            "    batch 1750/2500  loss=0.8165\n",
            "    batch 1800/2500  loss=0.3976\n",
            "    batch 1850/2500  loss=442.1722\n",
            "    batch 1900/2500  loss=0.8667\n",
            "    batch 1950/2500  loss=1.2996\n",
            "    batch 2000/2500  loss=0.3679\n",
            "    batch 2050/2500  loss=1.8027\n",
            "    batch 2100/2500  loss=0.9620\n",
            "    batch 2150/2500  loss=0.7998\n",
            "    batch 2200/2500  loss=0.7325\n",
            "    batch 2250/2500  loss=0.3154\n",
            "    batch 2300/2500  loss=0.0996\n",
            "    batch 2350/2500  loss=0.4810\n",
            "    batch 2400/2500  loss=0.6076\n",
            "    batch 2450/2500  loss=0.2714\n",
            "  train_loss=7.4584  val_loss=3.1610  val_CER=0.6627  val_WER=1.0539\n",
            "\n",
            "Epoch 19:\n",
            "    batch 0/2500  loss=0.7413\n",
            "    batch 50/2500  loss=0.2864\n",
            "    batch 100/2500  loss=0.4313\n",
            "    batch 150/2500  loss=0.5582\n",
            "    batch 200/2500  loss=0.4673\n",
            "    batch 250/2500  loss=1.0437\n",
            "    batch 300/2500  loss=0.8506\n",
            "    batch 350/2500  loss=0.7382\n",
            "    batch 400/2500  loss=1.0940\n",
            "    batch 450/2500  loss=0.4892\n",
            "    batch 500/2500  loss=0.6236\n",
            "    batch 550/2500  loss=0.6584\n",
            "    batch 600/2500  loss=6.1444\n",
            "    batch 650/2500  loss=0.7745\n",
            "    batch 700/2500  loss=1.1039\n",
            "    batch 750/2500  loss=0.0846\n",
            "    batch 800/2500  loss=0.7345\n",
            "    batch 850/2500  loss=1.1660\n",
            "    batch 900/2500  loss=0.7238\n",
            "    batch 950/2500  loss=0.4892\n",
            "    batch 1000/2500  loss=0.6878\n",
            "    batch 1050/2500  loss=0.6571\n",
            "    batch 1100/2500  loss=1.0677\n",
            "    batch 1150/2500  loss=0.7244\n",
            "    batch 1200/2500  loss=0.7098\n",
            "    batch 1250/2500  loss=0.5678\n",
            "    batch 1300/2500  loss=0.3262\n",
            "    batch 1350/2500  loss=0.1788\n",
            "    batch 1400/2500  loss=0.5291\n",
            "    batch 1450/2500  loss=0.7976\n",
            "    batch 1500/2500  loss=0.9077\n",
            "    batch 1550/2500  loss=0.6374\n",
            "    batch 1600/2500  loss=0.8090\n",
            "    batch 1650/2500  loss=0.7291\n",
            "    batch 1700/2500  loss=0.3990\n",
            "    batch 1750/2500  loss=1.2499\n",
            "    batch 1800/2500  loss=0.3885\n",
            "    batch 1850/2500  loss=0.9945\n",
            "    batch 1900/2500  loss=0.1503\n",
            "    batch 1950/2500  loss=0.6610\n",
            "    batch 2000/2500  loss=0.2176\n",
            "    batch 2050/2500  loss=1.2541\n",
            "    batch 2100/2500  loss=1.2122\n",
            "    batch 2150/2500  loss=0.5281\n",
            "    batch 2200/2500  loss=0.7021\n",
            "    batch 2250/2500  loss=1.1423\n",
            "    batch 2300/2500  loss=0.6926\n",
            "    batch 2350/2500  loss=1.1264\n",
            "    batch 2400/2500  loss=0.7261\n",
            "    batch 2450/2500  loss=0.9528\n",
            "  train_loss=8.7875  val_loss=3.2398  val_CER=0.6592  val_WER=1.0364\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 20:\n",
            "    batch 0/2500  loss=0.3744\n",
            "    batch 50/2500  loss=0.9170\n",
            "    batch 100/2500  loss=0.1333\n",
            "    batch 150/2500  loss=0.3154\n",
            "    batch 200/2500  loss=0.3271\n",
            "    batch 250/2500  loss=0.5978\n",
            "    batch 300/2500  loss=0.5538\n",
            "    batch 350/2500  loss=0.6141\n",
            "    batch 400/2500  loss=0.8461\n",
            "    batch 450/2500  loss=0.2504\n",
            "    batch 500/2500  loss=1.0329\n",
            "    batch 550/2500  loss=0.4182\n",
            "    batch 600/2500  loss=0.5768\n",
            "    batch 650/2500  loss=0.0467\n",
            "    batch 700/2500  loss=58.3697\n",
            "    batch 750/2500  loss=0.4636\n",
            "    batch 800/2500  loss=0.2149\n",
            "    batch 850/2500  loss=63.9627\n",
            "    batch 900/2500  loss=0.4663\n",
            "    batch 950/2500  loss=1.0698\n",
            "    batch 1000/2500  loss=1.6416\n",
            "    batch 1050/2500  loss=1.3463\n",
            "    batch 1100/2500  loss=1.0348\n",
            "    batch 1150/2500  loss=1.1412\n",
            "    batch 1200/2500  loss=0.0866\n",
            "    batch 1250/2500  loss=0.8943\n",
            "    batch 1300/2500  loss=0.8276\n",
            "    batch 1350/2500  loss=0.8294\n",
            "    batch 1400/2500  loss=0.1899\n",
            "    batch 1450/2500  loss=0.5073\n",
            "    batch 1500/2500  loss=1.2669\n",
            "    batch 1550/2500  loss=0.6546\n",
            "    batch 1600/2500  loss=0.4018\n",
            "    batch 1650/2500  loss=0.6836\n",
            "    batch 1700/2500  loss=0.4544\n",
            "    batch 1750/2500  loss=1.0243\n",
            "    batch 1800/2500  loss=1.0841\n",
            "    batch 1850/2500  loss=0.9824\n",
            "    batch 1900/2500  loss=0.7542\n",
            "    batch 1950/2500  loss=1.1130\n",
            "    batch 2000/2500  loss=0.1027\n",
            "    batch 2050/2500  loss=0.6074\n",
            "    batch 2100/2500  loss=0.9122\n",
            "    batch 2150/2500  loss=0.4028\n",
            "    batch 2200/2500  loss=0.7502\n",
            "    batch 2250/2500  loss=1.2304\n",
            "    batch 2300/2500  loss=0.3952\n",
            "    batch 2350/2500  loss=0.6303\n",
            "    batch 2400/2500  loss=1.1285\n",
            "    batch 2450/2500  loss=0.4461\n",
            "  train_loss=4.2849  val_loss=3.1329  val_CER=0.6680  val_WER=1.0088\n",
            "\n",
            "Epoch 21:\n",
            "    batch 0/2500  loss=0.7022\n",
            "    batch 50/2500  loss=0.8331\n",
            "    batch 100/2500  loss=1.2381\n",
            "    batch 150/2500  loss=0.7560\n",
            "    batch 200/2500  loss=0.8938\n",
            "    batch 250/2500  loss=1.7761\n",
            "    batch 300/2500  loss=0.6365\n",
            "    batch 350/2500  loss=0.4730\n",
            "    batch 400/2500  loss=0.6030\n",
            "    batch 450/2500  loss=0.2564\n",
            "    batch 500/2500  loss=0.3469\n",
            "    batch 550/2500  loss=0.9629\n",
            "    batch 600/2500  loss=1.1084\n",
            "    batch 650/2500  loss=0.9450\n",
            "    batch 700/2500  loss=0.6617\n",
            "    batch 750/2500  loss=0.9588\n",
            "    batch 800/2500  loss=0.6330\n",
            "    batch 850/2500  loss=0.1626\n",
            "    batch 900/2500  loss=0.7047\n",
            "    batch 950/2500  loss=0.7508\n",
            "    batch 1000/2500  loss=0.6125\n",
            "    batch 1050/2500  loss=1.2536\n",
            "    batch 1100/2500  loss=0.3902\n",
            "    batch 1150/2500  loss=0.3239\n",
            "    batch 1200/2500  loss=0.5640\n",
            "    batch 1250/2500  loss=0.0541\n",
            "    batch 1300/2500  loss=1.0372\n",
            "    batch 1350/2500  loss=0.6663\n",
            "    batch 1400/2500  loss=0.3394\n",
            "    batch 1450/2500  loss=0.4094\n",
            "    batch 1500/2500  loss=0.8014\n",
            "    batch 1550/2500  loss=1.4723\n",
            "    batch 1600/2500  loss=0.8904\n",
            "    batch 1650/2500  loss=0.4132\n",
            "    batch 1700/2500  loss=0.9263\n",
            "    batch 1750/2500  loss=1.1107\n",
            "    batch 1800/2500  loss=0.0866\n",
            "    batch 1850/2500  loss=0.9876\n",
            "    batch 1900/2500  loss=0.0407\n",
            "    batch 1950/2500  loss=1.2781\n",
            "    batch 2000/2500  loss=0.2761\n",
            "    batch 2050/2500  loss=0.0367\n",
            "    batch 2100/2500  loss=0.7250\n",
            "    batch 2150/2500  loss=0.7964\n",
            "    batch 2200/2500  loss=0.2028\n",
            "    batch 2250/2500  loss=1350.7944\n",
            "    batch 2300/2500  loss=0.3220\n",
            "    batch 2350/2500  loss=0.8796\n",
            "    batch 2400/2500  loss=0.3475\n",
            "    batch 2450/2500  loss=0.8184\n",
            "  train_loss=6.5422  val_loss=3.2983  val_CER=0.6555  val_WER=1.0126\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 22:\n",
            "    batch 0/2500  loss=1.0068\n",
            "    batch 50/2500  loss=0.7111\n",
            "    batch 100/2500  loss=0.0170\n",
            "    batch 150/2500  loss=1.1682\n",
            "    batch 200/2500  loss=0.3891\n",
            "    batch 250/2500  loss=0.6208\n",
            "    batch 300/2500  loss=0.0714\n",
            "    batch 350/2500  loss=0.3797\n",
            "    batch 400/2500  loss=0.5247\n",
            "    batch 450/2500  loss=0.3290\n",
            "    batch 500/2500  loss=1.1274\n",
            "    batch 550/2500  loss=0.9972\n",
            "    batch 600/2500  loss=0.6253\n",
            "    batch 650/2500  loss=0.3849\n",
            "    batch 700/2500  loss=0.8157\n",
            "    batch 750/2500  loss=0.6998\n",
            "    batch 800/2500  loss=0.8630\n",
            "    batch 850/2500  loss=1.0393\n",
            "    batch 900/2500  loss=1.0205\n",
            "    batch 950/2500  loss=0.8251\n",
            "    batch 1000/2500  loss=0.2474\n",
            "    batch 1050/2500  loss=0.8923\n",
            "    batch 1100/2500  loss=0.7123\n",
            "    batch 1150/2500  loss=0.6849\n",
            "    batch 1200/2500  loss=1.1653\n",
            "    batch 1250/2500  loss=1.0903\n",
            "    batch 1300/2500  loss=0.5941\n",
            "    batch 1350/2500  loss=0.5529\n",
            "    batch 1400/2500  loss=0.2802\n",
            "    batch 1450/2500  loss=1.1890\n",
            "    batch 1500/2500  loss=0.5973\n",
            "    batch 1550/2500  loss=0.3913\n",
            "    batch 1600/2500  loss=0.3334\n",
            "    batch 1650/2500  loss=0.2797\n",
            "    batch 1700/2500  loss=0.4150\n",
            "    batch 1750/2500  loss=1.2931\n",
            "    batch 1800/2500  loss=0.7095\n",
            "    batch 1850/2500  loss=1.2165\n",
            "    batch 1900/2500  loss=0.6798\n",
            "    batch 1950/2500  loss=0.6911\n",
            "    batch 2000/2500  loss=0.8193\n",
            "    batch 2050/2500  loss=0.8748\n",
            "    batch 2100/2500  loss=0.9870\n",
            "    batch 2150/2500  loss=0.6404\n",
            "    batch 2200/2500  loss=0.0680\n",
            "    batch 2250/2500  loss=0.9424\n",
            "    batch 2300/2500  loss=0.3255\n",
            "    batch 2350/2500  loss=0.6563\n",
            "    batch 2400/2500  loss=0.8048\n",
            "    batch 2450/2500  loss=1.3002\n",
            "  train_loss=4.5684  val_loss=3.3783  val_CER=0.6589  val_WER=0.9995\n",
            "\n",
            "Epoch 23:\n",
            "    batch 0/2500  loss=1.0379\n",
            "    batch 50/2500  loss=0.8464\n",
            "    batch 100/2500  loss=0.7777\n",
            "    batch 150/2500  loss=0.1125\n",
            "    batch 200/2500  loss=0.8236\n",
            "    batch 250/2500  loss=0.4299\n",
            "    batch 300/2500  loss=0.5195\n",
            "    batch 350/2500  loss=0.3184\n",
            "    batch 400/2500  loss=0.5456\n",
            "    batch 450/2500  loss=0.5346\n",
            "    batch 500/2500  loss=0.8870\n",
            "    batch 550/2500  loss=0.6364\n",
            "    batch 600/2500  loss=0.8243\n",
            "    batch 650/2500  loss=0.0035\n",
            "    batch 700/2500  loss=0.6830\n",
            "    batch 750/2500  loss=0.5534\n",
            "    batch 800/2500  loss=0.0947\n",
            "    batch 850/2500  loss=0.8061\n",
            "    batch 900/2500  loss=0.7218\n",
            "    batch 950/2500  loss=0.1681\n",
            "    batch 1000/2500  loss=0.0829\n",
            "    batch 1050/2500  loss=9.0753\n",
            "    batch 1100/2500  loss=0.6503\n",
            "    batch 1150/2500  loss=0.3892\n",
            "    batch 1200/2500  loss=0.4728\n",
            "    batch 1250/2500  loss=1.3184\n",
            "    batch 1300/2500  loss=0.1935\n",
            "    batch 1350/2500  loss=1.0623\n",
            "    batch 1400/2500  loss=0.5703\n",
            "    batch 1450/2500  loss=0.4692\n",
            "    batch 1500/2500  loss=0.7514\n",
            "    batch 1550/2500  loss=0.5819\n",
            "    batch 1600/2500  loss=0.0037\n",
            "    batch 1650/2500  loss=0.3840\n",
            "    batch 1700/2500  loss=0.0243\n",
            "    batch 1750/2500  loss=0.9406\n",
            "    batch 1800/2500  loss=0.4590\n",
            "    batch 1850/2500  loss=0.2075\n",
            "    batch 1900/2500  loss=0.4661\n",
            "    batch 1950/2500  loss=0.3257\n",
            "    batch 2000/2500  loss=0.9995\n",
            "    batch 2050/2500  loss=1.2769\n",
            "    batch 2100/2500  loss=0.2191\n",
            "    batch 2150/2500  loss=0.6875\n",
            "    batch 2200/2500  loss=0.3697\n",
            "    batch 2250/2500  loss=0.8002\n",
            "    batch 2300/2500  loss=0.2331\n",
            "    batch 2350/2500  loss=0.9233\n",
            "    batch 2400/2500  loss=0.8743\n",
            "    batch 2450/2500  loss=0.3202\n",
            "  train_loss=4.6145  val_loss=3.3652  val_CER=0.6622  val_WER=1.0171\n",
            "\n",
            "Epoch 24:\n",
            "    batch 0/2500  loss=0.5604\n",
            "    batch 50/2500  loss=0.9949\n",
            "    batch 100/2500  loss=0.3130\n",
            "    batch 150/2500  loss=0.0543\n",
            "    batch 200/2500  loss=1.3686\n",
            "    batch 250/2500  loss=0.4942\n",
            "    batch 300/2500  loss=0.5310\n",
            "    batch 350/2500  loss=0.3731\n",
            "    batch 400/2500  loss=0.1144\n",
            "    batch 450/2500  loss=0.5446\n",
            "    batch 500/2500  loss=0.3942\n",
            "    batch 550/2500  loss=0.2739\n",
            "    batch 600/2500  loss=0.5051\n",
            "    batch 650/2500  loss=0.0326\n",
            "    batch 700/2500  loss=0.0038\n",
            "    batch 750/2500  loss=0.7404\n",
            "    batch 800/2500  loss=0.1344\n",
            "    batch 850/2500  loss=0.8994\n",
            "    batch 900/2500  loss=0.7272\n",
            "    batch 950/2500  loss=0.7374\n",
            "    batch 1000/2500  loss=1.1022\n",
            "    batch 1050/2500  loss=0.2454\n",
            "    batch 1100/2500  loss=0.0753\n",
            "    batch 1150/2500  loss=0.7167\n",
            "    batch 1200/2500  loss=0.1455\n",
            "    batch 1250/2500  loss=0.3393\n",
            "    batch 1300/2500  loss=0.3973\n",
            "    batch 1350/2500  loss=0.4190\n",
            "    batch 1400/2500  loss=1.4113\n",
            "    batch 1450/2500  loss=0.6310\n",
            "    batch 1500/2500  loss=0.3690\n",
            "    batch 1550/2500  loss=0.8815\n",
            "    batch 1600/2500  loss=0.6680\n",
            "    batch 1650/2500  loss=0.0639\n",
            "    batch 1700/2500  loss=0.1827\n",
            "    batch 1750/2500  loss=0.5035\n",
            "    batch 1800/2500  loss=0.2333\n",
            "    batch 1850/2500  loss=0.3908\n",
            "    batch 1900/2500  loss=0.0493\n",
            "    batch 1950/2500  loss=0.7311\n",
            "    batch 2000/2500  loss=0.1037\n",
            "    batch 2050/2500  loss=0.0888\n",
            "    batch 2100/2500  loss=0.4791\n",
            "    batch 2150/2500  loss=0.4023\n",
            "    batch 2200/2500  loss=0.9058\n",
            "    batch 2250/2500  loss=0.8067\n",
            "    batch 2300/2500  loss=0.8367\n",
            "    batch 2350/2500  loss=0.2158\n",
            "    batch 2400/2500  loss=0.5367\n",
            "    batch 2450/2500  loss=1.4807\n",
            "  train_loss=3.4964  val_loss=3.4115  val_CER=0.6562  val_WER=1.0090\n",
            "\n",
            "Epoch 25:\n",
            "    batch 0/2500  loss=0.0448\n",
            "    batch 50/2500  loss=0.3901\n",
            "    batch 100/2500  loss=0.3227\n",
            "    batch 150/2500  loss=0.4274\n",
            "    batch 200/2500  loss=0.5137\n",
            "    batch 250/2500  loss=0.5480\n",
            "    batch 300/2500  loss=0.2197\n",
            "    batch 350/2500  loss=0.2065\n",
            "    batch 400/2500  loss=0.0598\n",
            "    batch 450/2500  loss=0.4536\n",
            "    batch 500/2500  loss=0.3452\n",
            "    batch 550/2500  loss=0.3689\n",
            "    batch 600/2500  loss=0.7519\n",
            "    batch 650/2500  loss=0.0308\n",
            "    batch 700/2500  loss=0.2713\n",
            "    batch 750/2500  loss=0.1751\n",
            "    batch 800/2500  loss=0.5987\n",
            "    batch 850/2500  loss=0.0077\n",
            "    batch 900/2500  loss=0.6571\n",
            "    batch 950/2500  loss=0.2731\n",
            "    batch 1000/2500  loss=1.2667\n",
            "    batch 1050/2500  loss=0.7512\n",
            "    batch 1100/2500  loss=0.6812\n",
            "    batch 1150/2500  loss=0.0272\n",
            "    batch 1200/2500  loss=0.7206\n",
            "    batch 1250/2500  loss=0.3161\n",
            "    batch 1300/2500  loss=0.3655\n",
            "    batch 1350/2500  loss=0.1387\n",
            "    batch 1400/2500  loss=0.2486\n",
            "    batch 1450/2500  loss=0.0356\n",
            "    batch 1500/2500  loss=0.9852\n",
            "    batch 1550/2500  loss=0.5277\n",
            "    batch 1600/2500  loss=0.4383\n",
            "    batch 1650/2500  loss=0.0206\n",
            "    batch 1700/2500  loss=0.6798\n",
            "    batch 1750/2500  loss=0.4551\n",
            "    batch 1800/2500  loss=0.7456\n",
            "    batch 1850/2500  loss=0.7580\n",
            "    batch 1900/2500  loss=0.0353\n",
            "    batch 1950/2500  loss=0.4607\n",
            "    batch 2000/2500  loss=0.3452\n",
            "    batch 2050/2500  loss=0.3741\n",
            "    batch 2100/2500  loss=0.5823\n",
            "    batch 2150/2500  loss=0.5147\n",
            "    batch 2200/2500  loss=0.6489\n",
            "    batch 2250/2500  loss=0.8752\n",
            "    batch 2300/2500  loss=0.0444\n",
            "    batch 2350/2500  loss=0.4578\n",
            "    batch 2400/2500  loss=0.3336\n",
            "    batch 2450/2500  loss=0.7785\n",
            "  train_loss=4.2319  val_loss=3.5706  val_CER=0.6490  val_WER=1.0226\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 26:\n",
            "    batch 0/2500  loss=0.5664\n",
            "    batch 50/2500  loss=0.5131\n",
            "    batch 100/2500  loss=0.2550\n",
            "    batch 150/2500  loss=0.6892\n",
            "    batch 200/2500  loss=0.9018\n",
            "    batch 250/2500  loss=0.5796\n",
            "    batch 300/2500  loss=0.8307\n",
            "    batch 350/2500  loss=0.7106\n",
            "    batch 400/2500  loss=0.4692\n",
            "    batch 450/2500  loss=0.3816\n",
            "    batch 500/2500  loss=0.2850\n",
            "    batch 550/2500  loss=3.9573\n",
            "    batch 600/2500  loss=0.1926\n",
            "    batch 650/2500  loss=0.3088\n",
            "    batch 700/2500  loss=0.3841\n",
            "    batch 750/2500  loss=0.3517\n",
            "    batch 800/2500  loss=0.4450\n",
            "    batch 850/2500  loss=1.1199\n",
            "    batch 900/2500  loss=0.4246\n",
            "    batch 950/2500  loss=0.5271\n",
            "    batch 1000/2500  loss=0.5958\n",
            "    batch 1050/2500  loss=0.0709\n",
            "    batch 1100/2500  loss=0.0035\n",
            "    batch 1150/2500  loss=1.1491\n",
            "    batch 1200/2500  loss=0.0063\n",
            "    batch 1250/2500  loss=0.9660\n",
            "    batch 1300/2500  loss=0.0091\n",
            "    batch 1350/2500  loss=0.2883\n",
            "    batch 1400/2500  loss=0.6512\n",
            "    batch 1450/2500  loss=0.7527\n",
            "    batch 1500/2500  loss=0.6110\n",
            "    batch 1550/2500  loss=0.5769\n",
            "    batch 1600/2500  loss=0.0391\n",
            "    batch 1650/2500  loss=11.9223\n",
            "    batch 1700/2500  loss=0.5230\n",
            "    batch 1750/2500  loss=0.7243\n",
            "    batch 1800/2500  loss=0.3990\n",
            "    batch 1850/2500  loss=0.2183\n",
            "    batch 1900/2500  loss=0.0600\n",
            "    batch 1950/2500  loss=1.0052\n",
            "    batch 2000/2500  loss=0.8225\n",
            "    batch 2050/2500  loss=0.0506\n",
            "    batch 2100/2500  loss=0.6835\n",
            "    batch 2150/2500  loss=0.5527\n",
            "    batch 2200/2500  loss=0.4590\n",
            "    batch 2250/2500  loss=0.5762\n",
            "    batch 2300/2500  loss=0.9504\n",
            "    batch 2350/2500  loss=0.4008\n",
            "    batch 2400/2500  loss=0.4238\n",
            "    batch 2450/2500  loss=0.8225\n",
            "  train_loss=2.9694  val_loss=3.7525  val_CER=0.6524  val_WER=1.0067\n",
            "\n",
            "Epoch 27:\n",
            "    batch 0/2500  loss=24.3102\n",
            "    batch 50/2500  loss=0.2916\n",
            "    batch 100/2500  loss=0.0737\n",
            "    batch 150/2500  loss=0.2777\n",
            "    batch 200/2500  loss=0.6891\n",
            "    batch 250/2500  loss=85.1524\n",
            "    batch 300/2500  loss=0.6675\n",
            "    batch 350/2500  loss=0.0014\n",
            "    batch 400/2500  loss=0.0075\n",
            "    batch 450/2500  loss=0.3938\n",
            "    batch 500/2500  loss=0.8273\n",
            "    batch 550/2500  loss=0.6796\n",
            "    batch 600/2500  loss=0.9915\n",
            "    batch 650/2500  loss=0.5720\n",
            "    batch 700/2500  loss=0.5069\n",
            "    batch 750/2500  loss=0.3925\n",
            "    batch 800/2500  loss=0.2779\n",
            "    batch 850/2500  loss=0.4007\n",
            "    batch 900/2500  loss=0.0040\n",
            "    batch 950/2500  loss=0.3690\n",
            "    batch 1000/2500  loss=0.0063\n",
            "    batch 1050/2500  loss=1.5043\n",
            "    batch 1100/2500  loss=0.6546\n",
            "    batch 1150/2500  loss=0.2469\n",
            "    batch 1200/2500  loss=0.7383\n",
            "    batch 1250/2500  loss=0.1066\n",
            "    batch 1300/2500  loss=0.2577\n",
            "    batch 1350/2500  loss=0.2354\n",
            "    batch 1400/2500  loss=1229.7659\n",
            "    batch 1450/2500  loss=0.6448\n",
            "    batch 1500/2500  loss=1.0770\n",
            "    batch 1550/2500  loss=0.9279\n",
            "    batch 1600/2500  loss=0.6854\n",
            "    batch 1650/2500  loss=0.0643\n",
            "    batch 1700/2500  loss=0.5186\n",
            "    batch 1750/2500  loss=0.6599\n",
            "    batch 1800/2500  loss=0.2848\n",
            "    batch 1850/2500  loss=0.2408\n",
            "    batch 1900/2500  loss=0.3674\n",
            "    batch 1950/2500  loss=0.6124\n",
            "    batch 2000/2500  loss=1.0985\n",
            "    batch 2050/2500  loss=0.3864\n",
            "    batch 2100/2500  loss=0.3547\n",
            "    batch 2150/2500  loss=0.2131\n",
            "    batch 2200/2500  loss=0.4942\n",
            "    batch 2250/2500  loss=0.6372\n",
            "    batch 2300/2500  loss=0.8006\n",
            "    batch 2350/2500  loss=0.5658\n",
            "    batch 2400/2500  loss=0.6745\n",
            "    batch 2450/2500  loss=0.1620\n",
            "  train_loss=4.3795  val_loss=3.7033  val_CER=0.6577  val_WER=1.0109\n",
            "\n",
            "Epoch 28:\n",
            "    batch 0/2500  loss=0.0415\n",
            "    batch 50/2500  loss=0.6488\n",
            "    batch 100/2500  loss=0.3045\n",
            "    batch 150/2500  loss=0.1462\n",
            "    batch 200/2500  loss=0.1632\n",
            "    batch 250/2500  loss=0.5428\n",
            "    batch 300/2500  loss=0.1860\n",
            "    batch 350/2500  loss=0.7335\n",
            "    batch 400/2500  loss=0.4965\n",
            "    batch 450/2500  loss=0.3760\n",
            "    batch 500/2500  loss=0.7316\n",
            "    batch 550/2500  loss=0.2475\n",
            "    batch 600/2500  loss=0.6636\n",
            "    batch 650/2500  loss=0.3289\n",
            "    batch 700/2500  loss=0.6505\n",
            "    batch 750/2500  loss=0.4953\n",
            "    batch 800/2500  loss=0.1469\n",
            "    batch 850/2500  loss=1.0154\n",
            "    batch 900/2500  loss=0.2596\n",
            "    batch 950/2500  loss=0.8200\n",
            "    batch 1000/2500  loss=0.2303\n",
            "    batch 1050/2500  loss=0.1923\n",
            "    batch 1100/2500  loss=0.1020\n",
            "    batch 1150/2500  loss=0.0982\n",
            "    batch 1200/2500  loss=1.0250\n",
            "    batch 1250/2500  loss=0.3942\n",
            "    batch 1300/2500  loss=0.5283\n",
            "    batch 1350/2500  loss=0.3779\n",
            "    batch 1400/2500  loss=1.2759\n",
            "    batch 1450/2500  loss=0.5945\n",
            "    batch 1500/2500  loss=0.0583\n",
            "    batch 1550/2500  loss=0.3600\n",
            "    batch 1600/2500  loss=0.1434\n",
            "    batch 1650/2500  loss=0.6119\n",
            "    batch 1700/2500  loss=0.5777\n",
            "    batch 1750/2500  loss=0.6872\n",
            "    batch 1800/2500  loss=0.1622\n",
            "    batch 1850/2500  loss=0.6368\n",
            "    batch 1900/2500  loss=0.6204\n",
            "    batch 1950/2500  loss=0.5304\n",
            "    batch 2000/2500  loss=0.1153\n",
            "    batch 2050/2500  loss=0.7925\n",
            "    batch 2100/2500  loss=0.2661\n",
            "    batch 2150/2500  loss=0.1318\n",
            "    batch 2200/2500  loss=0.1736\n",
            "    batch 2250/2500  loss=0.6176\n",
            "    batch 2300/2500  loss=0.0875\n",
            "    batch 2350/2500  loss=0.4836\n",
            "    batch 2400/2500  loss=0.4501\n",
            "    batch 2450/2500  loss=0.1691\n",
            "  train_loss=5.4149  val_loss=3.7959  val_CER=0.6477  val_WER=0.9976\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "\n",
            "Epoch 29:\n",
            "    batch 0/2500  loss=0.0149\n",
            "    batch 50/2500  loss=0.2896\n",
            "    batch 100/2500  loss=2.1695\n",
            "    batch 150/2500  loss=0.1296\n",
            "    batch 200/2500  loss=0.8009\n",
            "    batch 250/2500  loss=0.2077\n",
            "    batch 300/2500  loss=0.1299\n",
            "    batch 350/2500  loss=0.0254\n",
            "    batch 400/2500  loss=0.5068\n",
            "    batch 450/2500  loss=0.7835\n",
            "    batch 500/2500  loss=0.4394\n",
            "    batch 550/2500  loss=0.5955\n",
            "    batch 600/2500  loss=0.3593\n",
            "    batch 650/2500  loss=0.5483\n",
            "    batch 700/2500  loss=0.5158\n",
            "    batch 750/2500  loss=0.4075\n",
            "    batch 800/2500  loss=0.9725\n",
            "    batch 850/2500  loss=1.0378\n",
            "    batch 900/2500  loss=0.5388\n",
            "    batch 950/2500  loss=0.7184\n",
            "    batch 1000/2500  loss=0.4503\n",
            "    batch 1050/2500  loss=0.4124\n",
            "    batch 1100/2500  loss=0.8660\n",
            "    batch 1150/2500  loss=0.2264\n",
            "    batch 1200/2500  loss=0.6642\n",
            "    batch 1250/2500  loss=0.3424\n",
            "    batch 1300/2500  loss=0.7759\n",
            "    batch 1350/2500  loss=1.0401\n",
            "    batch 1400/2500  loss=0.5732\n",
            "    batch 1450/2500  loss=0.6329\n",
            "    batch 1500/2500  loss=0.7269\n",
            "    batch 1550/2500  loss=0.2256\n",
            "    batch 1600/2500  loss=0.5552\n",
            "    batch 1650/2500  loss=0.8609\n",
            "    batch 1700/2500  loss=0.7156\n",
            "    batch 1750/2500  loss=0.0173\n",
            "    batch 1800/2500  loss=0.1484\n",
            "    batch 1850/2500  loss=0.0193\n",
            "    batch 1900/2500  loss=0.6512\n",
            "    batch 1950/2500  loss=0.3289\n",
            "    batch 2000/2500  loss=0.0679\n",
            "    batch 2050/2500  loss=0.4062\n",
            "    batch 2100/2500  loss=0.0157\n",
            "    batch 2150/2500  loss=0.3145\n",
            "    batch 2200/2500  loss=0.2030\n",
            "    batch 2250/2500  loss=0.4989\n",
            "    batch 2300/2500  loss=0.3950\n",
            "    batch 2350/2500  loss=0.1693\n",
            "    batch 2400/2500  loss=0.4190\n",
            "    batch 2450/2500  loss=0.3876\n",
            "  train_loss=6.8313  val_loss=4.0031  val_CER=0.6542  val_WER=1.0036\n",
            "\n",
            "Epoch 30:\n",
            "    batch 0/2500  loss=0.1089\n",
            "    batch 50/2500  loss=0.1522\n",
            "    batch 100/2500  loss=0.2933\n",
            "    batch 150/2500  loss=0.0589\n",
            "    batch 200/2500  loss=0.3896\n",
            "    batch 250/2500  loss=0.1265\n",
            "    batch 300/2500  loss=0.6031\n",
            "    batch 350/2500  loss=0.1218\n",
            "    batch 400/2500  loss=0.4325\n",
            "    batch 450/2500  loss=0.3965\n",
            "    batch 500/2500  loss=0.0973\n",
            "    batch 550/2500  loss=0.6281\n",
            "    batch 600/2500  loss=0.4028\n",
            "    batch 650/2500  loss=0.0289\n",
            "    batch 700/2500  loss=0.1722\n",
            "    batch 750/2500  loss=0.9699\n",
            "    batch 800/2500  loss=0.8023\n",
            "    batch 850/2500  loss=0.2477\n",
            "    batch 900/2500  loss=0.3474\n",
            "    batch 950/2500  loss=0.7403\n",
            "    batch 1000/2500  loss=0.7570\n",
            "    batch 1050/2500  loss=0.0411\n",
            "    batch 1100/2500  loss=0.3529\n",
            "    batch 1150/2500  loss=0.3948\n",
            "    batch 1200/2500  loss=0.3615\n",
            "    batch 1250/2500  loss=0.6915\n",
            "    batch 1300/2500  loss=0.2560\n",
            "    batch 1350/2500  loss=0.2811\n",
            "    batch 1400/2500  loss=0.1677\n",
            "    batch 1450/2500  loss=0.5375\n",
            "    batch 1500/2500  loss=0.6614\n",
            "    batch 1550/2500  loss=0.4609\n",
            "    batch 1600/2500  loss=0.1420\n",
            "    batch 1650/2500  loss=0.7550\n",
            "    batch 1700/2500  loss=0.0765\n",
            "    batch 1750/2500  loss=0.1808\n",
            "    batch 1800/2500  loss=0.1603\n",
            "    batch 1850/2500  loss=0.5821\n",
            "    batch 1900/2500  loss=0.6096\n",
            "    batch 1950/2500  loss=0.8078\n",
            "    batch 2000/2500  loss=0.3004\n",
            "    batch 2050/2500  loss=0.6362\n",
            "    batch 2100/2500  loss=0.1288\n",
            "    batch 2150/2500  loss=0.1771\n",
            "    batch 2200/2500  loss=0.4724\n",
            "    batch 2250/2500  loss=0.0075\n",
            "    batch 2300/2500  loss=0.3594\n",
            "    batch 2350/2500  loss=0.4500\n",
            "    batch 2400/2500  loss=0.1611\n",
            "    batch 2450/2500  loss=0.1371\n",
            "  train_loss=7.7680  val_loss=3.8830  val_CER=0.6581  val_WER=1.0029\n",
            "\n",
            "Epoch 31:\n",
            "    batch 0/2500  loss=0.0006\n",
            "    batch 50/2500  loss=0.1772\n",
            "    batch 100/2500  loss=0.2827\n",
            "    batch 150/2500  loss=0.0112\n",
            "    batch 200/2500  loss=0.3435\n",
            "    batch 250/2500  loss=0.3820\n",
            "    batch 300/2500  loss=0.3198\n",
            "    batch 350/2500  loss=0.0139\n",
            "    batch 400/2500  loss=0.7236\n",
            "    batch 450/2500  loss=0.9891\n",
            "    batch 500/2500  loss=0.2940\n",
            "    batch 550/2500  loss=0.4006\n",
            "    batch 600/2500  loss=0.8409\n",
            "    batch 650/2500  loss=0.2968\n",
            "    batch 700/2500  loss=0.4437\n",
            "    batch 750/2500  loss=0.2732\n",
            "    batch 800/2500  loss=0.1415\n",
            "    batch 850/2500  loss=0.0667\n",
            "    batch 900/2500  loss=0.1538\n",
            "    batch 950/2500  loss=0.6861\n",
            "    batch 1000/2500  loss=0.4836\n",
            "    batch 1050/2500  loss=0.4906\n",
            "    batch 1100/2500  loss=0.6171\n",
            "    batch 1150/2500  loss=0.1461\n",
            "    batch 1200/2500  loss=0.4444\n",
            "    batch 1250/2500  loss=1.1300\n",
            "    batch 1300/2500  loss=0.2631\n",
            "    batch 1350/2500  loss=0.4678\n",
            "    batch 1400/2500  loss=0.4682\n",
            "    batch 1450/2500  loss=0.4340\n",
            "    batch 1500/2500  loss=0.0245\n",
            "    batch 1550/2500  loss=0.2737\n",
            "    batch 1600/2500  loss=0.0535\n",
            "    batch 1650/2500  loss=1.0075\n",
            "    batch 1700/2500  loss=0.7797\n",
            "    batch 1750/2500  loss=0.1164\n",
            "    batch 1800/2500  loss=1.0006\n",
            "    batch 1850/2500  loss=0.5862\n",
            "    batch 1900/2500  loss=0.4005\n",
            "    batch 1950/2500  loss=0.4587\n",
            "    batch 2000/2500  loss=0.5225\n",
            "    batch 2050/2500  loss=0.7036\n",
            "    batch 2100/2500  loss=0.1878\n",
            "    batch 2150/2500  loss=0.2655\n",
            "    batch 2200/2500  loss=0.4591\n",
            "    batch 2250/2500  loss=0.6443\n",
            "    batch 2300/2500  loss=0.4152\n",
            "    batch 2350/2500  loss=0.3171\n",
            "    batch 2400/2500  loss=0.5762\n",
            "    batch 2450/2500  loss=0.2151\n",
            "  train_loss=1.9820  val_loss=4.0882  val_CER=0.6529  val_WER=1.0088\n",
            "\n",
            "Epoch 32:\n",
            "    batch 0/2500  loss=0.2481\n",
            "    batch 50/2500  loss=0.2633\n",
            "    batch 100/2500  loss=0.0441\n",
            "    batch 150/2500  loss=0.0916\n",
            "    batch 200/2500  loss=0.1859\n",
            "    batch 250/2500  loss=0.3824\n",
            "    batch 300/2500  loss=0.8469\n",
            "    batch 350/2500  loss=0.9744\n",
            "    batch 400/2500  loss=0.1485\n",
            "    batch 450/2500  loss=0.3996\n",
            "    batch 500/2500  loss=0.1741\n",
            "    batch 550/2500  loss=0.4081\n",
            "    batch 600/2500  loss=0.5197\n",
            "    batch 650/2500  loss=0.1724\n",
            "    batch 700/2500  loss=0.9660\n",
            "    batch 750/2500  loss=1.0882\n",
            "    batch 800/2500  loss=0.2709\n",
            "    batch 850/2500  loss=0.9277\n",
            "    batch 900/2500  loss=0.5599\n",
            "    batch 950/2500  loss=0.1985\n",
            "    batch 1000/2500  loss=0.3864\n",
            "    batch 1050/2500  loss=0.5039\n",
            "    batch 1100/2500  loss=0.3522\n",
            "    batch 1150/2500  loss=0.0757\n",
            "    batch 1200/2500  loss=0.1250\n",
            "    batch 1250/2500  loss=0.8119\n",
            "    batch 1300/2500  loss=0.6497\n",
            "    batch 1350/2500  loss=0.0573\n",
            "    batch 1400/2500  loss=0.3297\n",
            "    batch 1450/2500  loss=0.2368\n",
            "    batch 1500/2500  loss=0.3129\n",
            "    batch 1550/2500  loss=0.1738\n",
            "    batch 1600/2500  loss=0.1699\n",
            "    batch 1650/2500  loss=0.4261\n",
            "    batch 1700/2500  loss=0.2216\n",
            "    batch 1750/2500  loss=0.0668\n",
            "    batch 1800/2500  loss=0.3732\n",
            "    batch 1850/2500  loss=0.2351\n",
            "    batch 1900/2500  loss=1.0474\n",
            "    batch 1950/2500  loss=1.0338\n",
            "    batch 2000/2500  loss=0.0135\n",
            "    batch 2050/2500  loss=0.3222\n",
            "    batch 2100/2500  loss=0.4649\n",
            "    batch 2150/2500  loss=0.3391\n",
            "    batch 2200/2500  loss=0.1842\n",
            "    batch 2250/2500  loss=0.1488\n",
            "    batch 2300/2500  loss=0.1828\n",
            "    batch 2350/2500  loss=0.1002\n",
            "    batch 2400/2500  loss=0.1689\n",
            "    batch 2450/2500  loss=0.4299\n",
            "  train_loss=4.3078  val_loss=4.2791  val_CER=0.6589  val_WER=1.0076\n",
            "\n",
            "Epoch 33:\n",
            "    batch 0/2500  loss=0.1047\n",
            "    batch 50/2500  loss=0.4602\n",
            "    batch 100/2500  loss=0.4899\n",
            "    batch 150/2500  loss=0.2734\n",
            "    batch 200/2500  loss=0.1772\n",
            "    batch 250/2500  loss=0.3668\n",
            "    batch 300/2500  loss=0.4282\n",
            "    batch 350/2500  loss=0.3976\n",
            "    batch 400/2500  loss=0.5257\n",
            "    batch 450/2500  loss=0.0406\n",
            "    batch 500/2500  loss=0.8127\n",
            "    batch 550/2500  loss=0.5225\n",
            "    batch 600/2500  loss=0.4917\n",
            "    batch 650/2500  loss=0.9223\n",
            "    batch 700/2500  loss=0.8228\n",
            "    batch 750/2500  loss=0.2380\n",
            "    batch 800/2500  loss=0.6155\n",
            "    batch 850/2500  loss=0.0951\n",
            "    batch 900/2500  loss=0.8396\n",
            "    batch 950/2500  loss=0.2891\n",
            "    batch 1000/2500  loss=0.3657\n",
            "    batch 1050/2500  loss=0.0539\n",
            "    batch 1100/2500  loss=0.5423\n",
            "    batch 1150/2500  loss=0.4810\n",
            "    batch 1200/2500  loss=0.3122\n",
            "    batch 1250/2500  loss=0.2493\n",
            "    batch 1300/2500  loss=0.4405\n",
            "    batch 1350/2500  loss=0.2786\n",
            "    batch 1400/2500  loss=0.3348\n",
            "    batch 1450/2500  loss=0.5797\n",
            "    batch 1500/2500  loss=0.0116\n",
            "    batch 1550/2500  loss=0.4239\n",
            "    batch 1600/2500  loss=0.0015\n",
            "    batch 1650/2500  loss=0.3183\n",
            "    batch 1700/2500  loss=2.4204\n",
            "    batch 1750/2500  loss=0.3894\n",
            "    batch 1800/2500  loss=0.7405\n",
            "    batch 1850/2500  loss=0.1562\n",
            "    batch 1900/2500  loss=0.1522\n",
            "    batch 1950/2500  loss=1.3816\n",
            "    batch 2000/2500  loss=0.2480\n",
            "    batch 2050/2500  loss=0.4033\n",
            "    batch 2100/2500  loss=0.2661\n",
            "    batch 2150/2500  loss=0.3340\n",
            "    batch 2200/2500  loss=0.4835\n",
            "    batch 2250/2500  loss=0.3842\n",
            "    batch 2300/2500  loss=0.0246\n",
            "    batch 2350/2500  loss=0.0636\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3606537139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexp_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-490534452.py\u001b[0m in \u001b[0;36mtrain_experiment\u001b[0;34m(exp_name, base_dir)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mblank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBLANK_IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             )\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m     def backward(\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     ):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "res = train_experiment(\"D2_synth\", DATA_ROOTS[\"D2_synth\"])\n",
        "results.append(res)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-ebkxPMnrWLR",
        "outputId": "dfa65fc3-df65-42e3-a416-4b5bb799bb71"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "########## D2_synth on HICMA_Plus_Synthetic ##########\n",
            "  -> Building dataloaders...\n",
            "  -> Dataloaders ready. Building model...\n",
            "  -> Model ready. Starting training...\n",
            "\n",
            "Epoch 01:\n",
            "    batch 0/3415  loss=3498.1519\n",
            "    batch 50/3415  loss=1176.8870\n",
            "    batch 100/3415  loss=975.6541\n",
            "    batch 150/3415  loss=457.6769\n",
            "    batch 200/3415  loss=109.7695\n",
            "    batch 250/3415  loss=160.3979\n",
            "    batch 300/3415  loss=77.8753\n",
            "    batch 350/3415  loss=25.0032\n",
            "    batch 400/3415  loss=80.7116\n",
            "    batch 450/3415  loss=29.6122\n",
            "    batch 500/3415  loss=13.1184\n",
            "    batch 550/3415  loss=9.3345\n",
            "    batch 600/3415  loss=34.6656\n",
            "    batch 650/3415  loss=25.8660\n",
            "    batch 700/3415  loss=15.4306\n",
            "    batch 750/3415  loss=28.2290\n",
            "    batch 800/3415  loss=17.5991\n",
            "    batch 850/3415  loss=18.7326\n",
            "    batch 900/3415  loss=11.4186\n",
            "    batch 950/3415  loss=20.0316\n",
            "    batch 1000/3415  loss=11.9456\n",
            "    batch 1050/3415  loss=11.2969\n",
            "    batch 1100/3415  loss=5.7521\n",
            "    batch 1150/3415  loss=5.4778\n",
            "    batch 1200/3415  loss=29.0756\n",
            "    batch 1250/3415  loss=27.5062\n",
            "    batch 1300/3415  loss=41.7133\n",
            "    batch 1350/3415  loss=5.1140\n",
            "    batch 1400/3415  loss=34.9233\n",
            "    batch 1450/3415  loss=5.7338\n",
            "    batch 1500/3415  loss=5.9559\n",
            "    batch 1550/3415  loss=6.7545\n",
            "    batch 1600/3415  loss=7.2111\n",
            "    batch 1650/3415  loss=5.7118\n",
            "    batch 1700/3415  loss=23.8798\n",
            "    batch 1750/3415  loss=27.6604\n",
            "    batch 1800/3415  loss=12.1040\n",
            "    batch 1850/3415  loss=8.9079\n",
            "    batch 1900/3415  loss=28.3196\n",
            "    batch 1950/3415  loss=7.9636\n",
            "    batch 2000/3415  loss=23.8217\n",
            "    batch 2050/3415  loss=18.0558\n",
            "    batch 2100/3415  loss=9.5499\n",
            "    batch 2150/3415  loss=21.5537\n",
            "    batch 2200/3415  loss=5.5301\n",
            "    batch 2250/3415  loss=5.9176\n",
            "    batch 2300/3415  loss=4.6215\n",
            "    batch 2350/3415  loss=4.0199\n",
            "    batch 2400/3415  loss=11.5374\n",
            "    batch 2450/3415  loss=4.2244\n",
            "    batch 2500/3415  loss=4.8261\n",
            "    batch 2550/3415  loss=4.1862\n",
            "    batch 2600/3415  loss=12.2203\n",
            "    batch 2650/3415  loss=10.1430\n",
            "    batch 2700/3415  loss=4.8970\n",
            "    batch 2750/3415  loss=20.4006\n",
            "    batch 2800/3415  loss=11.3737\n",
            "    batch 2850/3415  loss=8.3015\n",
            "    batch 2900/3415  loss=6.2888\n",
            "    batch 2950/3415  loss=13.3552\n",
            "    batch 3000/3415  loss=21.1273\n",
            "    batch 3050/3415  loss=4.7072\n",
            "    batch 3100/3415  loss=5.6258\n",
            "    batch 3150/3415  loss=6.4516\n",
            "    batch 3200/3415  loss=8.1446\n",
            "    batch 3250/3415  loss=5.5428\n",
            "    batch 3300/3415  loss=13.6222\n",
            "    batch 3350/3415  loss=5.2122\n",
            "    batch 3400/3415  loss=10.2195\n",
            "  train_loss=80.4266  val_loss=4.2200  val_CER=0.8691  val_WER=0.9942\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 02:\n",
            "    batch 0/3415  loss=13.4429\n",
            "    batch 50/3415  loss=9.8100\n",
            "    batch 100/3415  loss=4.6492\n",
            "    batch 150/3415  loss=5.5346\n",
            "    batch 200/3415  loss=14.5907\n",
            "    batch 250/3415  loss=13.2047\n",
            "    batch 300/3415  loss=23.2160\n",
            "    batch 350/3415  loss=10.6656\n",
            "    batch 400/3415  loss=5.4310\n",
            "    batch 450/3415  loss=21.1858\n",
            "    batch 500/3415  loss=11.6351\n",
            "    batch 550/3415  loss=16.1991\n",
            "    batch 600/3415  loss=3.9660\n",
            "    batch 650/3415  loss=4.2825\n",
            "    batch 700/3415  loss=20.0145\n",
            "    batch 750/3415  loss=6.7083\n",
            "    batch 800/3415  loss=4.0625\n",
            "    batch 850/3415  loss=9.1223\n",
            "    batch 900/3415  loss=4.5328\n",
            "    batch 950/3415  loss=4.5104\n",
            "    batch 1000/3415  loss=5.2945\n",
            "    batch 1050/3415  loss=10.3816\n",
            "    batch 1100/3415  loss=11.2457\n",
            "    batch 1150/3415  loss=10.1002\n",
            "    batch 1200/3415  loss=9.2722\n",
            "    batch 1250/3415  loss=11.6186\n",
            "    batch 1300/3415  loss=3.9263\n",
            "    batch 1350/3415  loss=12.2197\n",
            "    batch 1400/3415  loss=8.0661\n",
            "    batch 1450/3415  loss=7.9939\n",
            "    batch 1500/3415  loss=4.8322\n",
            "    batch 1550/3415  loss=10.3695\n",
            "    batch 1600/3415  loss=19.3251\n",
            "    batch 1650/3415  loss=7.0826\n",
            "    batch 1700/3415  loss=15.4363\n",
            "    batch 1750/3415  loss=3.6237\n",
            "    batch 1800/3415  loss=14.5709\n",
            "    batch 1850/3415  loss=3.8433\n",
            "    batch 1900/3415  loss=4.0198\n",
            "    batch 1950/3415  loss=3.7694\n",
            "    batch 2000/3415  loss=20.3070\n",
            "    batch 2050/3415  loss=3.9180\n",
            "    batch 2100/3415  loss=23.0839\n",
            "    batch 2150/3415  loss=12.2120\n",
            "    batch 2200/3415  loss=9.2854\n",
            "    batch 2250/3415  loss=13.7317\n",
            "    batch 2300/3415  loss=7.6287\n",
            "    batch 2350/3415  loss=3.6695\n",
            "    batch 2400/3415  loss=9.7067\n",
            "    batch 2450/3415  loss=4.6964\n",
            "    batch 2500/3415  loss=4.9095\n",
            "    batch 2550/3415  loss=4.7227\n",
            "    batch 2600/3415  loss=4.1464\n",
            "    batch 2650/3415  loss=3.7066\n",
            "    batch 2700/3415  loss=8.8840\n",
            "    batch 2750/3415  loss=9.0109\n",
            "    batch 2800/3415  loss=3.7294\n",
            "    batch 2850/3415  loss=3.6771\n",
            "    batch 2900/3415  loss=3.9027\n",
            "    batch 2950/3415  loss=3.7386\n",
            "    batch 3000/3415  loss=8.5399\n",
            "    batch 3050/3415  loss=12.2425\n",
            "    batch 3100/3415  loss=18.1559\n",
            "    batch 3150/3415  loss=4.0508\n",
            "    batch 3200/3415  loss=8.3627\n",
            "    batch 3250/3415  loss=3.7080\n",
            "    batch 3300/3415  loss=3.7026\n",
            "    batch 3350/3415  loss=3.5818\n",
            "    batch 3400/3415  loss=8.2036\n",
            "  train_loss=8.6061  val_loss=4.0652  val_CER=0.9286  val_WER=0.9997\n",
            "\n",
            "Epoch 03:\n",
            "    batch 0/3415  loss=7.0240\n",
            "    batch 50/3415  loss=4.9727\n",
            "    batch 100/3415  loss=9.3785\n",
            "    batch 150/3415  loss=3.6453\n",
            "    batch 200/3415  loss=14.2004\n",
            "    batch 250/3415  loss=5.3867\n",
            "    batch 300/3415  loss=3.7972\n",
            "    batch 350/3415  loss=4.5363\n",
            "    batch 400/3415  loss=7.8637\n",
            "    batch 450/3415  loss=8.9734\n",
            "    batch 500/3415  loss=4.0825\n",
            "    batch 550/3415  loss=8.9005\n",
            "    batch 600/3415  loss=6.4899\n",
            "    batch 650/3415  loss=4.4364\n",
            "    batch 700/3415  loss=8.9161\n",
            "    batch 750/3415  loss=3.3787\n",
            "    batch 800/3415  loss=11.9103\n",
            "    batch 850/3415  loss=3.7218\n",
            "    batch 900/3415  loss=11.3949\n",
            "    batch 950/3415  loss=5.2506\n",
            "    batch 1000/3415  loss=4.6292\n",
            "    batch 1050/3415  loss=3.5155\n",
            "    batch 1100/3415  loss=9.3760\n",
            "    batch 1150/3415  loss=6.9037\n",
            "    batch 1200/3415  loss=11.7890\n",
            "    batch 1250/3415  loss=5.3835\n",
            "    batch 1300/3415  loss=3.5664\n",
            "    batch 1350/3415  loss=7.0126\n",
            "    batch 1400/3415  loss=3.5160\n",
            "    batch 1450/3415  loss=3.5976\n",
            "    batch 1500/3415  loss=3.6555\n",
            "    batch 1550/3415  loss=3.4560\n",
            "    batch 1600/3415  loss=4.0090\n",
            "    batch 1650/3415  loss=3.4824\n",
            "    batch 1700/3415  loss=3.7196\n",
            "    batch 1750/3415  loss=9.5444\n",
            "    batch 1800/3415  loss=3.6732\n",
            "    batch 1850/3415  loss=10.7228\n",
            "    batch 1900/3415  loss=10.3325\n",
            "    batch 1950/3415  loss=3.9135\n",
            "    batch 2000/3415  loss=7.6875\n",
            "    batch 2050/3415  loss=3.6183\n",
            "    batch 2100/3415  loss=3.6813\n",
            "    batch 2150/3415  loss=3.5954\n",
            "    batch 2200/3415  loss=10.2842\n",
            "    batch 2250/3415  loss=3.5781\n",
            "    batch 2300/3415  loss=10.0010\n",
            "    batch 2350/3415  loss=11.8574\n",
            "    batch 2400/3415  loss=8.9844\n",
            "    batch 2450/3415  loss=4.1673\n",
            "    batch 2500/3415  loss=4.1360\n",
            "    batch 2550/3415  loss=13.3441\n",
            "    batch 2600/3415  loss=11.9408\n",
            "    batch 2650/3415  loss=8.7399\n",
            "    batch 2700/3415  loss=5.0250\n",
            "    batch 2750/3415  loss=3.4566\n",
            "    batch 2800/3415  loss=9.6006\n",
            "    batch 2850/3415  loss=4.9704\n",
            "    batch 2900/3415  loss=8.7921\n",
            "    batch 2950/3415  loss=7.2641\n",
            "    batch 3000/3415  loss=4.7400\n",
            "    batch 3050/3415  loss=5.5750\n",
            "    batch 3100/3415  loss=17.1339\n",
            "    batch 3150/3415  loss=4.0630\n",
            "    batch 3200/3415  loss=6.3706\n",
            "    batch 3250/3415  loss=5.9343\n",
            "    batch 3300/3415  loss=9.4843\n",
            "    batch 3350/3415  loss=3.4329\n",
            "    batch 3400/3415  loss=3.5101\n",
            "  train_loss=7.0833  val_loss=3.9584  val_CER=0.9596  val_WER=1.0000\n",
            "\n",
            "Epoch 04:\n",
            "    batch 0/3415  loss=10.0702\n",
            "    batch 50/3415  loss=3.6364\n",
            "    batch 100/3415  loss=7.0255\n",
            "    batch 150/3415  loss=6.3184\n",
            "    batch 200/3415  loss=4.2280\n",
            "    batch 250/3415  loss=4.8304\n",
            "    batch 300/3415  loss=9.0111\n",
            "    batch 350/3415  loss=8.4427\n",
            "    batch 400/3415  loss=3.4319\n",
            "    batch 450/3415  loss=6.0805\n",
            "    batch 500/3415  loss=3.4150\n",
            "    batch 550/3415  loss=6.7060\n",
            "    batch 600/3415  loss=7.4741\n",
            "    batch 650/3415  loss=6.3530\n",
            "    batch 700/3415  loss=4.1308\n",
            "    batch 750/3415  loss=7.0104\n",
            "    batch 800/3415  loss=7.2421\n",
            "    batch 850/3415  loss=3.7146\n",
            "    batch 900/3415  loss=5.6448\n",
            "    batch 950/3415  loss=7.6907\n",
            "    batch 1000/3415  loss=5.9850\n",
            "    batch 1050/3415  loss=3.8885\n",
            "    batch 1100/3415  loss=3.2878\n",
            "    batch 1150/3415  loss=4.4920\n",
            "    batch 1200/3415  loss=3.2472\n",
            "    batch 1250/3415  loss=4.4268\n",
            "    batch 1300/3415  loss=4.0638\n",
            "    batch 1350/3415  loss=8.7785\n",
            "    batch 1400/3415  loss=5.3738\n",
            "    batch 1450/3415  loss=6.1430\n",
            "    batch 1500/3415  loss=9.9360\n",
            "    batch 1550/3415  loss=4.4957\n",
            "    batch 1600/3415  loss=3.4331\n",
            "    batch 1650/3415  loss=7.9713\n",
            "    batch 1700/3415  loss=4.7586\n",
            "    batch 1750/3415  loss=4.4422\n",
            "    batch 1800/3415  loss=4.4749\n",
            "    batch 1850/3415  loss=5.1737\n",
            "    batch 1900/3415  loss=8.4976\n",
            "    batch 1950/3415  loss=4.9792\n",
            "    batch 2000/3415  loss=3.5558\n",
            "    batch 2050/3415  loss=3.3849\n",
            "    batch 2100/3415  loss=10.2999\n",
            "    batch 2150/3415  loss=4.4490\n",
            "    batch 2200/3415  loss=3.4645\n",
            "    batch 2250/3415  loss=3.7028\n",
            "    batch 2300/3415  loss=6.3561\n",
            "    batch 2350/3415  loss=3.3490\n",
            "    batch 2400/3415  loss=3.3040\n",
            "    batch 2450/3415  loss=3.5905\n",
            "    batch 2500/3415  loss=6.8770\n",
            "    batch 2550/3415  loss=7.0721\n",
            "    batch 2600/3415  loss=7.9319\n",
            "    batch 2650/3415  loss=9.8994\n",
            "    batch 2700/3415  loss=3.4972\n",
            "    batch 2750/3415  loss=6.8431\n",
            "    batch 2800/3415  loss=11.8019\n",
            "    batch 2850/3415  loss=10.5452\n",
            "    batch 2900/3415  loss=7.5063\n",
            "    batch 2950/3415  loss=5.7781\n",
            "    batch 3000/3415  loss=7.9634\n",
            "    batch 3050/3415  loss=3.7173\n",
            "    batch 3100/3415  loss=3.5552\n",
            "    batch 3150/3415  loss=5.7725\n",
            "    batch 3200/3415  loss=8.2293\n",
            "    batch 3250/3415  loss=6.9935\n",
            "    batch 3300/3415  loss=14.5152\n",
            "    batch 3350/3415  loss=3.3356\n",
            "    batch 3400/3415  loss=3.5055\n",
            "  train_loss=6.1572  val_loss=3.8118  val_CER=0.9074  val_WER=1.0009\n",
            "\n",
            "Epoch 05:\n",
            "    batch 0/3415  loss=4.1679\n",
            "    batch 50/3415  loss=3.2565\n",
            "    batch 100/3415  loss=8.9293\n",
            "    batch 150/3415  loss=7.1755\n",
            "    batch 200/3415  loss=3.5842\n",
            "    batch 250/3415  loss=3.4872\n",
            "    batch 300/3415  loss=6.9312\n",
            "    batch 350/3415  loss=6.0245\n",
            "    batch 400/3415  loss=9.6278\n",
            "    batch 450/3415  loss=14.7165\n",
            "    batch 500/3415  loss=3.4660\n",
            "    batch 550/3415  loss=6.3923\n",
            "    batch 600/3415  loss=8.2452\n",
            "    batch 650/3415  loss=3.9789\n",
            "    batch 700/3415  loss=7.3334\n",
            "    batch 750/3415  loss=3.3679\n",
            "    batch 800/3415  loss=5.5687\n",
            "    batch 850/3415  loss=5.9057\n",
            "    batch 900/3415  loss=7.2306\n",
            "    batch 950/3415  loss=3.4183\n",
            "    batch 1000/3415  loss=3.3328\n",
            "    batch 1050/3415  loss=3.8699\n",
            "    batch 1100/3415  loss=3.3672\n",
            "    batch 1150/3415  loss=7.4275\n",
            "    batch 1200/3415  loss=3.3930\n",
            "    batch 1250/3415  loss=3.4700\n",
            "    batch 1300/3415  loss=3.3585\n",
            "    batch 1350/3415  loss=8.2818\n",
            "    batch 1400/3415  loss=5.6701\n",
            "    batch 1450/3415  loss=4.5362\n",
            "    batch 1500/3415  loss=11.2559\n",
            "    batch 1550/3415  loss=9.0404\n",
            "    batch 1600/3415  loss=6.4481\n",
            "    batch 1650/3415  loss=9.1484\n",
            "    batch 1700/3415  loss=3.3565\n",
            "    batch 1750/3415  loss=3.3127\n",
            "    batch 1800/3415  loss=3.6661\n",
            "    batch 1850/3415  loss=3.6320\n",
            "    batch 1900/3415  loss=5.0633\n",
            "    batch 1950/3415  loss=6.6909\n",
            "    batch 2000/3415  loss=8.5745\n",
            "    batch 2050/3415  loss=5.2354\n",
            "    batch 2100/3415  loss=4.0976\n",
            "    batch 2150/3415  loss=5.5250\n",
            "    batch 2200/3415  loss=3.5147\n",
            "    batch 2250/3415  loss=3.3412\n",
            "    batch 2300/3415  loss=3.4253\n",
            "    batch 2350/3415  loss=7.7270\n",
            "    batch 2400/3415  loss=3.5007\n",
            "    batch 2450/3415  loss=6.3042\n",
            "    batch 2500/3415  loss=4.8807\n",
            "    batch 2550/3415  loss=6.1838\n",
            "    batch 2600/3415  loss=5.6485\n",
            "    batch 2650/3415  loss=6.0715\n",
            "    batch 2700/3415  loss=3.5253\n",
            "    batch 2750/3415  loss=3.4293\n",
            "    batch 2800/3415  loss=5.5939\n",
            "    batch 2850/3415  loss=7.5241\n",
            "    batch 2900/3415  loss=3.3926\n",
            "    batch 2950/3415  loss=6.6899\n",
            "    batch 3000/3415  loss=3.4088\n",
            "    batch 3050/3415  loss=6.5755\n",
            "    batch 3100/3415  loss=3.3907\n",
            "    batch 3150/3415  loss=7.5543\n",
            "    batch 3200/3415  loss=7.0242\n",
            "    batch 3250/3415  loss=5.2358\n",
            "    batch 3300/3415  loss=4.6623\n",
            "    batch 3350/3415  loss=5.6282\n",
            "    batch 3400/3415  loss=3.3596\n",
            "  train_loss=5.6476  val_loss=3.7999  val_CER=0.9781  val_WER=0.9996\n",
            ">> Unfreezing Swin backbone for fine-tuning.\n",
            "\n",
            "Epoch 06:\n",
            "    batch 0/3415  loss=5.4271\n",
            "    batch 50/3415  loss=3.4050\n",
            "    batch 100/3415  loss=3.3283\n",
            "    batch 150/3415  loss=3.2870\n",
            "    batch 200/3415  loss=3.2834\n",
            "    batch 250/3415  loss=3.2935\n",
            "    batch 300/3415  loss=3.3127\n",
            "    batch 350/3415  loss=2.9634\n",
            "    batch 400/3415  loss=3.0418\n",
            "    batch 450/3415  loss=3.1480\n",
            "    batch 500/3415  loss=2.9490\n",
            "    batch 550/3415  loss=2.8954\n",
            "    batch 600/3415  loss=3.1502\n",
            "    batch 650/3415  loss=3.1695\n",
            "    batch 700/3415  loss=2.9685\n",
            "    batch 750/3415  loss=3.0939\n",
            "    batch 800/3415  loss=3.1159\n",
            "    batch 850/3415  loss=2.7761\n",
            "    batch 900/3415  loss=2.8493\n",
            "    batch 950/3415  loss=2.8756\n",
            "    batch 1000/3415  loss=2.9068\n",
            "    batch 1050/3415  loss=3.4899\n",
            "    batch 1100/3415  loss=2.8567\n",
            "    batch 1150/3415  loss=2.5367\n",
            "    batch 1200/3415  loss=3.0826\n",
            "    batch 1250/3415  loss=3.1023\n",
            "    batch 1300/3415  loss=3.0199\n",
            "    batch 1350/3415  loss=2.9908\n",
            "    batch 1400/3415  loss=3.1585\n",
            "    batch 1450/3415  loss=2.8532\n",
            "    batch 1500/3415  loss=2.8660\n",
            "    batch 1550/3415  loss=2.8583\n",
            "    batch 1600/3415  loss=3.2512\n",
            "    batch 1650/3415  loss=2.6470\n",
            "    batch 1700/3415  loss=2.8875\n",
            "    batch 1750/3415  loss=2.7928\n",
            "    batch 1800/3415  loss=2.5966\n",
            "    batch 1850/3415  loss=2.9982\n",
            "    batch 1900/3415  loss=2.5112\n",
            "    batch 1950/3415  loss=2.4240\n",
            "    batch 2000/3415  loss=3.1321\n",
            "    batch 2050/3415  loss=3.2589\n",
            "    batch 2100/3415  loss=2.8916\n",
            "    batch 2150/3415  loss=2.7652\n",
            "    batch 2200/3415  loss=2.7005\n",
            "    batch 2250/3415  loss=2.8085\n",
            "    batch 2300/3415  loss=2.5519\n",
            "    batch 2350/3415  loss=2.8883\n",
            "    batch 2400/3415  loss=2.3284\n",
            "    batch 2450/3415  loss=2.6681\n",
            "    batch 2500/3415  loss=1.9884\n",
            "    batch 2550/3415  loss=3.0501\n",
            "    batch 2600/3415  loss=2.3757\n",
            "    batch 2650/3415  loss=2.4771\n",
            "    batch 2700/3415  loss=2.2663\n",
            "    batch 2750/3415  loss=2.2596\n",
            "    batch 2800/3415  loss=2.4700\n",
            "    batch 2850/3415  loss=2.4890\n",
            "    batch 2900/3415  loss=2.1626\n",
            "    batch 2950/3415  loss=2.6348\n",
            "    batch 3000/3415  loss=1.5285\n",
            "    batch 3050/3415  loss=2.4861\n",
            "    batch 3100/3415  loss=2.6916\n",
            "    batch 3150/3415  loss=2.3316\n",
            "    batch 3200/3415  loss=1.8905\n",
            "    batch 3250/3415  loss=1.8932\n",
            "    batch 3300/3415  loss=2.2517\n",
            "    batch 3350/3415  loss=2.2567\n",
            "    batch 3400/3415  loss=2.5110\n",
            "  train_loss=2.7854  val_loss=2.9044  val_CER=0.7530  val_WER=1.0278\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 07:\n",
            "    batch 0/3415  loss=2.5498\n",
            "    batch 50/3415  loss=1.7724\n",
            "    batch 100/3415  loss=2.9454\n",
            "    batch 150/3415  loss=2.0983\n",
            "    batch 200/3415  loss=2.1863\n",
            "    batch 250/3415  loss=2.0306\n",
            "    batch 300/3415  loss=2.0104\n",
            "    batch 350/3415  loss=2.6205\n",
            "    batch 400/3415  loss=1.8769\n",
            "    batch 450/3415  loss=2.2350\n",
            "    batch 500/3415  loss=2.1073\n",
            "    batch 550/3415  loss=2.1443\n",
            "    batch 600/3415  loss=2.3230\n",
            "    batch 650/3415  loss=1.1878\n",
            "    batch 700/3415  loss=0.8414\n",
            "    batch 750/3415  loss=2.0701\n",
            "    batch 800/3415  loss=2.4477\n",
            "    batch 850/3415  loss=1.2865\n",
            "    batch 900/3415  loss=2.3545\n",
            "    batch 950/3415  loss=1.7972\n",
            "    batch 1000/3415  loss=2.2091\n",
            "    batch 1050/3415  loss=2.2093\n",
            "    batch 1100/3415  loss=1.8175\n",
            "    batch 1150/3415  loss=2.6292\n",
            "    batch 1200/3415  loss=1.5643\n",
            "    batch 1250/3415  loss=1.7909\n",
            "    batch 1300/3415  loss=1.5114\n",
            "    batch 1350/3415  loss=2.4740\n",
            "    batch 1400/3415  loss=2.5605\n",
            "    batch 1450/3415  loss=1.7983\n",
            "    batch 1500/3415  loss=2.5350\n",
            "    batch 1550/3415  loss=2.7276\n",
            "    batch 1600/3415  loss=2.6226\n",
            "    batch 1650/3415  loss=1.0175\n",
            "    batch 1700/3415  loss=2.4224\n",
            "    batch 1750/3415  loss=2.1548\n",
            "    batch 1800/3415  loss=2.4560\n",
            "    batch 1850/3415  loss=1.9404\n",
            "    batch 1900/3415  loss=1.6204\n",
            "    batch 1950/3415  loss=2.1678\n",
            "    batch 2000/3415  loss=2.7289\n",
            "    batch 2050/3415  loss=2.5399\n",
            "    batch 2100/3415  loss=1.6673\n",
            "    batch 2150/3415  loss=2.3562\n",
            "    batch 2200/3415  loss=2.1815\n",
            "    batch 2250/3415  loss=1.7690\n",
            "    batch 2300/3415  loss=1.7587\n",
            "    batch 2350/3415  loss=1.5427\n",
            "    batch 2400/3415  loss=1.7477\n",
            "    batch 2450/3415  loss=2.1911\n",
            "    batch 2500/3415  loss=2.3368\n",
            "    batch 2550/3415  loss=1.6134\n",
            "    batch 2600/3415  loss=2.1129\n",
            "    batch 2650/3415  loss=1.8227\n",
            "    batch 2700/3415  loss=2.1498\n",
            "    batch 2750/3415  loss=1.9439\n",
            "    batch 2800/3415  loss=1.4493\n",
            "    batch 2850/3415  loss=1.8757\n",
            "    batch 2900/3415  loss=1.3364\n",
            "    batch 2950/3415  loss=1.5672\n",
            "    batch 3000/3415  loss=1.1229\n",
            "    batch 3050/3415  loss=2.2167\n",
            "    batch 3100/3415  loss=1.9613\n",
            "    batch 3150/3415  loss=1.5946\n",
            "    batch 3200/3415  loss=2.6792\n",
            "    batch 3250/3415  loss=2.4161\n",
            "    batch 3300/3415  loss=1.2033\n",
            "    batch 3350/3415  loss=2.0416\n",
            "    batch 3400/3415  loss=1.6575\n",
            "  train_loss=1.9897  val_loss=2.7700  val_CER=0.7460  val_WER=1.0026\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 08:\n",
            "    batch 0/3415  loss=1.2947\n",
            "    batch 50/3415  loss=1.7286\n",
            "    batch 100/3415  loss=1.6831\n",
            "    batch 150/3415  loss=2.2331\n",
            "    batch 200/3415  loss=1.9614\n",
            "    batch 250/3415  loss=1.1798\n",
            "    batch 300/3415  loss=1.4302\n",
            "    batch 350/3415  loss=1.3839\n",
            "    batch 400/3415  loss=1.8414\n",
            "    batch 450/3415  loss=1.8193\n",
            "    batch 500/3415  loss=2.0999\n",
            "    batch 550/3415  loss=2.4583\n",
            "    batch 600/3415  loss=2.0140\n",
            "    batch 650/3415  loss=1.7603\n",
            "    batch 700/3415  loss=1.6883\n",
            "    batch 750/3415  loss=1.7649\n",
            "    batch 800/3415  loss=2.0909\n",
            "    batch 850/3415  loss=2.1426\n",
            "    batch 900/3415  loss=1.5169\n",
            "    batch 950/3415  loss=1.7413\n",
            "    batch 1000/3415  loss=2.0066\n",
            "    batch 1050/3415  loss=1.8354\n",
            "    batch 1100/3415  loss=1.7552\n",
            "    batch 1150/3415  loss=1.7067\n",
            "    batch 1200/3415  loss=1.8070\n",
            "    batch 1250/3415  loss=1.0358\n",
            "    batch 1300/3415  loss=1.5612\n",
            "    batch 1350/3415  loss=1.3754\n",
            "    batch 1400/3415  loss=1.5612\n",
            "    batch 1450/3415  loss=1.2669\n",
            "    batch 1500/3415  loss=2.4770\n",
            "    batch 1550/3415  loss=2.5151\n",
            "    batch 1600/3415  loss=1.7574\n",
            "    batch 1650/3415  loss=1.8280\n",
            "    batch 1700/3415  loss=2.4166\n",
            "    batch 1750/3415  loss=1.3566\n",
            "    batch 1800/3415  loss=1.0377\n",
            "    batch 1850/3415  loss=2.2008\n",
            "    batch 1900/3415  loss=1.9445\n",
            "    batch 1950/3415  loss=1.0383\n",
            "    batch 2000/3415  loss=2.4060\n",
            "    batch 2050/3415  loss=1.9438\n",
            "    batch 2100/3415  loss=2.1154\n",
            "    batch 2150/3415  loss=2.1422\n",
            "    batch 2200/3415  loss=1.8623\n",
            "    batch 2250/3415  loss=1.9815\n",
            "    batch 2300/3415  loss=2.3955\n",
            "    batch 2350/3415  loss=1.5996\n",
            "    batch 2400/3415  loss=0.9245\n",
            "    batch 2450/3415  loss=1.5446\n",
            "    batch 2500/3415  loss=1.7908\n",
            "    batch 2550/3415  loss=1.2133\n",
            "    batch 2600/3415  loss=1.9966\n",
            "    batch 2650/3415  loss=1.3381\n",
            "    batch 2700/3415  loss=1.6778\n",
            "    batch 2750/3415  loss=1.1781\n",
            "    batch 2800/3415  loss=1.1225\n",
            "    batch 2850/3415  loss=1.7317\n",
            "    batch 2900/3415  loss=1.3752\n",
            "    batch 2950/3415  loss=2.0012\n",
            "    batch 3000/3415  loss=1.9617\n",
            "    batch 3050/3415  loss=1.3532\n",
            "    batch 3100/3415  loss=2.5840\n",
            "    batch 3150/3415  loss=1.4551\n",
            "    batch 3200/3415  loss=1.5527\n",
            "    batch 3250/3415  loss=1.9904\n",
            "    batch 3300/3415  loss=1.4423\n",
            "    batch 3350/3415  loss=1.0088\n",
            "    batch 3400/3415  loss=2.3611\n",
            "  train_loss=1.7330  val_loss=2.7469  val_CER=0.7133  val_WER=1.0220\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 09:\n",
            "    batch 0/3415  loss=1.5826\n",
            "    batch 50/3415  loss=1.7907\n",
            "    batch 100/3415  loss=1.2551\n",
            "    batch 150/3415  loss=2.6665\n",
            "    batch 200/3415  loss=1.7009\n",
            "    batch 250/3415  loss=1.3673\n",
            "    batch 300/3415  loss=0.9957\n",
            "    batch 350/3415  loss=1.6806\n",
            "    batch 400/3415  loss=1.8277\n",
            "    batch 450/3415  loss=2.3706\n",
            "    batch 500/3415  loss=1.7302\n",
            "    batch 550/3415  loss=1.3488\n",
            "    batch 600/3415  loss=1.4367\n",
            "    batch 650/3415  loss=1.5469\n",
            "    batch 700/3415  loss=1.4368\n",
            "    batch 750/3415  loss=1.2959\n",
            "    batch 800/3415  loss=1.0326\n",
            "    batch 850/3415  loss=0.5475\n",
            "    batch 900/3415  loss=1.7743\n",
            "    batch 950/3415  loss=2.1010\n",
            "    batch 1000/3415  loss=1.2936\n",
            "    batch 1050/3415  loss=1.9400\n",
            "    batch 1100/3415  loss=1.5680\n",
            "    batch 1150/3415  loss=1.4033\n",
            "    batch 1200/3415  loss=1.4166\n",
            "    batch 1250/3415  loss=1.3363\n",
            "    batch 1300/3415  loss=1.4970\n",
            "    batch 1350/3415  loss=2.3029\n",
            "    batch 1400/3415  loss=1.8738\n",
            "    batch 1450/3415  loss=1.6374\n",
            "    batch 1500/3415  loss=1.6343\n",
            "    batch 1550/3415  loss=1.2594\n",
            "    batch 1600/3415  loss=0.8248\n",
            "    batch 1650/3415  loss=1.3992\n",
            "    batch 1700/3415  loss=1.1493\n",
            "    batch 1750/3415  loss=1.1975\n",
            "    batch 1800/3415  loss=1.7960\n",
            "    batch 1850/3415  loss=1.3594\n",
            "    batch 1900/3415  loss=1.0070\n",
            "    batch 1950/3415  loss=0.7403\n",
            "    batch 2000/3415  loss=2.1717\n",
            "    batch 2050/3415  loss=1.4002\n",
            "    batch 2100/3415  loss=1.7956\n",
            "    batch 2150/3415  loss=1.5958\n",
            "    batch 2200/3415  loss=2.0497\n",
            "    batch 2250/3415  loss=1.9536\n",
            "    batch 2300/3415  loss=1.2558\n",
            "    batch 2350/3415  loss=1.3645\n",
            "    batch 2400/3415  loss=2.1520\n",
            "    batch 2450/3415  loss=1.2650\n",
            "    batch 2500/3415  loss=0.9748\n",
            "    batch 2550/3415  loss=1.3807\n",
            "    batch 2600/3415  loss=0.6924\n",
            "    batch 2650/3415  loss=0.7305\n",
            "    batch 2700/3415  loss=0.7753\n",
            "    batch 2750/3415  loss=1.6560\n",
            "    batch 2800/3415  loss=1.4786\n",
            "    batch 2850/3415  loss=1.3726\n",
            "    batch 2900/3415  loss=0.9736\n",
            "    batch 2950/3415  loss=1.5034\n",
            "    batch 3000/3415  loss=0.7299\n",
            "    batch 3050/3415  loss=1.3239\n",
            "    batch 3100/3415  loss=1.4781\n",
            "    batch 3150/3415  loss=1.3635\n",
            "    batch 3200/3415  loss=1.8036\n",
            "    batch 3250/3415  loss=1.9467\n",
            "    batch 3300/3415  loss=1.5906\n",
            "    batch 3350/3415  loss=1.3168\n",
            "    batch 3400/3415  loss=1.4851\n",
            "  train_loss=1.5608  val_loss=2.7105  val_CER=0.6885  val_WER=1.0162\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 10:\n",
            "    batch 0/3415  loss=0.9602\n",
            "    batch 50/3415  loss=0.5540\n",
            "    batch 100/3415  loss=1.6581\n",
            "    batch 150/3415  loss=1.7177\n",
            "    batch 200/3415  loss=1.8027\n",
            "    batch 250/3415  loss=1.2927\n",
            "    batch 300/3415  loss=1.5488\n",
            "    batch 350/3415  loss=1.7181\n",
            "    batch 400/3415  loss=1.4260\n",
            "    batch 450/3415  loss=1.3238\n",
            "    batch 500/3415  loss=0.9332\n",
            "    batch 550/3415  loss=1.9525\n",
            "    batch 600/3415  loss=1.2022\n",
            "    batch 650/3415  loss=1.4019\n",
            "    batch 700/3415  loss=0.9749\n",
            "    batch 750/3415  loss=1.9335\n",
            "    batch 800/3415  loss=2.3896\n",
            "    batch 850/3415  loss=1.7498\n",
            "    batch 900/3415  loss=0.7720\n",
            "    batch 950/3415  loss=1.5742\n",
            "    batch 1000/3415  loss=1.0206\n",
            "    batch 1050/3415  loss=1.3072\n",
            "    batch 1100/3415  loss=1.3872\n",
            "    batch 1150/3415  loss=1.3911\n",
            "    batch 1200/3415  loss=2.2246\n",
            "    batch 1250/3415  loss=1.6559\n",
            "    batch 1300/3415  loss=2.0479\n",
            "    batch 1350/3415  loss=1.7847\n",
            "    batch 1400/3415  loss=1.4265\n",
            "    batch 1450/3415  loss=0.4052\n",
            "    batch 1500/3415  loss=1.1893\n",
            "    batch 1550/3415  loss=1.9508\n",
            "    batch 1600/3415  loss=1.1866\n",
            "    batch 1650/3415  loss=1.2844\n",
            "    batch 1700/3415  loss=1.1595\n",
            "    batch 1750/3415  loss=1.6991\n",
            "    batch 1800/3415  loss=1.2879\n",
            "    batch 1850/3415  loss=0.6538\n",
            "    batch 1900/3415  loss=1.9710\n",
            "    batch 1950/3415  loss=2.2769\n",
            "    batch 2000/3415  loss=1.6575\n",
            "    batch 2050/3415  loss=1.0610\n",
            "    batch 2100/3415  loss=1.2922\n",
            "    batch 2150/3415  loss=1.4540\n",
            "    batch 2200/3415  loss=1.7811\n",
            "    batch 2250/3415  loss=1.0179\n",
            "    batch 2300/3415  loss=1.4429\n",
            "    batch 2350/3415  loss=0.9940\n",
            "    batch 2400/3415  loss=1.3710\n",
            "    batch 2450/3415  loss=0.9614\n",
            "    batch 2500/3415  loss=2.1999\n",
            "    batch 2550/3415  loss=1.1879\n",
            "    batch 2600/3415  loss=1.2565\n",
            "    batch 2650/3415  loss=1.0223\n",
            "    batch 2700/3415  loss=1.8019\n",
            "    batch 2750/3415  loss=1.6115\n",
            "    batch 2800/3415  loss=1.3059\n",
            "    batch 2850/3415  loss=0.6246\n",
            "    batch 2900/3415  loss=1.5739\n",
            "    batch 2950/3415  loss=1.1511\n",
            "    batch 3000/3415  loss=1.6652\n",
            "    batch 3050/3415  loss=1.3831\n",
            "    batch 3100/3415  loss=2.3276\n",
            "    batch 3150/3415  loss=1.8634\n",
            "    batch 3200/3415  loss=1.6370\n",
            "    batch 3250/3415  loss=1.2335\n",
            "    batch 3300/3415  loss=2.1329\n",
            "    batch 3350/3415  loss=1.6119\n",
            "    batch 3400/3415  loss=1.0734\n",
            "  train_loss=1.4124  val_loss=2.6796  val_CER=0.6857  val_WER=0.9805\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 11:\n",
            "    batch 0/3415  loss=2.0752\n",
            "    batch 50/3415  loss=1.3704\n",
            "    batch 100/3415  loss=1.0549\n",
            "    batch 150/3415  loss=1.3578\n",
            "    batch 200/3415  loss=1.2378\n",
            "    batch 250/3415  loss=1.7870\n",
            "    batch 300/3415  loss=1.9464\n",
            "    batch 350/3415  loss=1.1901\n",
            "    batch 400/3415  loss=0.6293\n",
            "    batch 450/3415  loss=1.4891\n",
            "    batch 500/3415  loss=0.5506\n",
            "    batch 550/3415  loss=1.1959\n",
            "    batch 600/3415  loss=0.9142\n",
            "    batch 650/3415  loss=1.0639\n",
            "    batch 700/3415  loss=1.5115\n",
            "    batch 750/3415  loss=1.0302\n",
            "    batch 800/3415  loss=1.3171\n",
            "    batch 850/3415  loss=1.3285\n",
            "    batch 900/3415  loss=1.1824\n",
            "    batch 950/3415  loss=1.6664\n",
            "    batch 1000/3415  loss=1.0959\n",
            "    batch 1050/3415  loss=1.6253\n",
            "    batch 1100/3415  loss=0.9537\n",
            "    batch 1150/3415  loss=1.5881\n",
            "    batch 1200/3415  loss=1.2804\n",
            "    batch 1250/3415  loss=0.3306\n",
            "    batch 1300/3415  loss=2.0339\n",
            "    batch 1350/3415  loss=1.0391\n",
            "    batch 1400/3415  loss=2.1006\n",
            "    batch 1450/3415  loss=1.9226\n",
            "    batch 1500/3415  loss=0.9635\n",
            "    batch 1550/3415  loss=0.2542\n",
            "    batch 1600/3415  loss=0.9438\n",
            "    batch 1650/3415  loss=1.8622\n",
            "    batch 1700/3415  loss=0.9643\n",
            "    batch 1750/3415  loss=1.4499\n",
            "    batch 1800/3415  loss=2.2924\n",
            "    batch 1850/3415  loss=1.0965\n",
            "    batch 1900/3415  loss=2.1788\n",
            "    batch 1950/3415  loss=1.1713\n",
            "    batch 2000/3415  loss=1.2370\n",
            "    batch 2050/3415  loss=2.2135\n",
            "    batch 2100/3415  loss=1.2946\n",
            "    batch 2150/3415  loss=0.7062\n",
            "    batch 2200/3415  loss=1.3262\n",
            "    batch 2250/3415  loss=0.9901\n",
            "    batch 2300/3415  loss=0.6516\n",
            "    batch 2350/3415  loss=1.5891\n",
            "    batch 2400/3415  loss=0.8011\n",
            "    batch 2450/3415  loss=0.8465\n",
            "    batch 2500/3415  loss=1.2467\n",
            "    batch 2550/3415  loss=1.1054\n",
            "    batch 2600/3415  loss=1.3630\n",
            "    batch 2650/3415  loss=1.0780\n",
            "    batch 2700/3415  loss=0.7409\n",
            "    batch 2750/3415  loss=1.8788\n",
            "    batch 2800/3415  loss=1.2586\n",
            "    batch 2850/3415  loss=0.9022\n",
            "    batch 2900/3415  loss=1.0357\n",
            "    batch 2950/3415  loss=1.7958\n",
            "    batch 3000/3415  loss=0.4088\n",
            "    batch 3050/3415  loss=1.3452\n",
            "    batch 3100/3415  loss=1.1080\n",
            "    batch 3150/3415  loss=1.0490\n",
            "    batch 3200/3415  loss=1.6452\n",
            "    batch 3250/3415  loss=1.5196\n",
            "    batch 3300/3415  loss=0.8014\n",
            "    batch 3350/3415  loss=0.6890\n",
            "    batch 3400/3415  loss=1.0485\n",
            "  train_loss=1.2932  val_loss=2.6049  val_CER=0.6553  val_WER=0.9408\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 12:\n",
            "    batch 0/3415  loss=1.4674\n",
            "    batch 50/3415  loss=1.0591\n",
            "    batch 100/3415  loss=1.0166\n",
            "    batch 150/3415  loss=0.9000\n",
            "    batch 200/3415  loss=0.9987\n",
            "    batch 250/3415  loss=1.3580\n",
            "    batch 300/3415  loss=1.2277\n",
            "    batch 350/3415  loss=0.4916\n",
            "    batch 400/3415  loss=1.8848\n",
            "    batch 450/3415  loss=1.1960\n",
            "    batch 500/3415  loss=2.0118\n",
            "    batch 550/3415  loss=0.9768\n",
            "    batch 600/3415  loss=0.9139\n",
            "    batch 650/3415  loss=0.9966\n",
            "    batch 700/3415  loss=0.8093\n",
            "    batch 750/3415  loss=0.9333\n",
            "    batch 800/3415  loss=1.1807\n",
            "    batch 850/3415  loss=0.2047\n",
            "    batch 900/3415  loss=1.5221\n",
            "    batch 950/3415  loss=0.9162\n",
            "    batch 1000/3415  loss=0.3782\n",
            "    batch 1050/3415  loss=1.5084\n",
            "    batch 1100/3415  loss=1.5786\n",
            "    batch 1150/3415  loss=0.3703\n",
            "    batch 1200/3415  loss=0.8875\n",
            "    batch 1250/3415  loss=1.9499\n",
            "    batch 1300/3415  loss=1.6945\n",
            "    batch 1350/3415  loss=1.0251\n",
            "    batch 1400/3415  loss=0.8528\n",
            "    batch 1450/3415  loss=1.0015\n",
            "    batch 1500/3415  loss=1.6781\n",
            "    batch 1550/3415  loss=1.0822\n",
            "    batch 1600/3415  loss=1.1223\n",
            "    batch 1650/3415  loss=1.2191\n",
            "    batch 1700/3415  loss=0.5694\n",
            "    batch 1750/3415  loss=1.3205\n",
            "    batch 1800/3415  loss=2.1402\n",
            "    batch 1850/3415  loss=0.8739\n",
            "    batch 1900/3415  loss=0.7119\n",
            "    batch 1950/3415  loss=0.6960\n",
            "    batch 2000/3415  loss=0.5143\n",
            "    batch 2050/3415  loss=1.3710\n",
            "    batch 2100/3415  loss=1.4242\n",
            "    batch 2150/3415  loss=0.5446\n",
            "    batch 2200/3415  loss=0.7063\n",
            "    batch 2250/3415  loss=0.4868\n",
            "    batch 2300/3415  loss=1.3485\n",
            "    batch 2350/3415  loss=1.4276\n",
            "    batch 2400/3415  loss=1.3900\n",
            "    batch 2450/3415  loss=1.5263\n",
            "    batch 2500/3415  loss=1.5205\n",
            "    batch 2550/3415  loss=1.1512\n",
            "    batch 2600/3415  loss=1.2345\n",
            "    batch 2650/3415  loss=1.2085\n",
            "    batch 2700/3415  loss=1.3349\n",
            "    batch 2750/3415  loss=1.0238\n",
            "    batch 2800/3415  loss=1.3218\n",
            "    batch 2850/3415  loss=1.5071\n",
            "    batch 2900/3415  loss=1.0743\n",
            "    batch 2950/3415  loss=1.7632\n",
            "    batch 3000/3415  loss=1.5447\n",
            "    batch 3050/3415  loss=0.9854\n",
            "    batch 3100/3415  loss=1.4712\n",
            "    batch 3150/3415  loss=0.6722\n",
            "    batch 3200/3415  loss=0.6287\n",
            "    batch 3250/3415  loss=1.6185\n",
            "    batch 3300/3415  loss=0.8178\n",
            "    batch 3350/3415  loss=1.0082\n",
            "    batch 3400/3415  loss=1.0050\n",
            "  train_loss=1.1902  val_loss=2.5376  val_CER=0.6155  val_WER=0.9238\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 13:\n",
            "    batch 0/3415  loss=1.2340\n",
            "    batch 50/3415  loss=1.2291\n",
            "    batch 100/3415  loss=1.5724\n",
            "    batch 150/3415  loss=1.4021\n",
            "    batch 200/3415  loss=1.3720\n",
            "    batch 250/3415  loss=0.8616\n",
            "    batch 300/3415  loss=1.2497\n",
            "    batch 350/3415  loss=1.0497\n",
            "    batch 400/3415  loss=0.7228\n",
            "    batch 450/3415  loss=0.6882\n",
            "    batch 500/3415  loss=0.8594\n",
            "    batch 550/3415  loss=0.1644\n",
            "    batch 600/3415  loss=1.0540\n",
            "    batch 650/3415  loss=0.9897\n",
            "    batch 700/3415  loss=0.5636\n",
            "    batch 750/3415  loss=1.6474\n",
            "    batch 800/3415  loss=1.1400\n",
            "    batch 850/3415  loss=1.0746\n",
            "    batch 900/3415  loss=0.7558\n",
            "    batch 950/3415  loss=0.9194\n",
            "    batch 1000/3415  loss=1.2470\n",
            "    batch 1050/3415  loss=1.4153\n",
            "    batch 1100/3415  loss=0.8517\n",
            "    batch 1150/3415  loss=0.9874\n",
            "    batch 1200/3415  loss=0.9900\n",
            "    batch 1250/3415  loss=1.0395\n",
            "    batch 1300/3415  loss=0.6917\n",
            "    batch 1350/3415  loss=0.6956\n",
            "    batch 1400/3415  loss=1.6205\n",
            "    batch 1450/3415  loss=1.3129\n",
            "    batch 1500/3415  loss=0.9060\n",
            "    batch 1550/3415  loss=1.4085\n",
            "    batch 1600/3415  loss=0.9171\n",
            "    batch 1650/3415  loss=0.3018\n",
            "    batch 1700/3415  loss=0.7763\n",
            "    batch 1750/3415  loss=1.6182\n",
            "    batch 1800/3415  loss=1.6424\n",
            "    batch 1850/3415  loss=1.1240\n",
            "    batch 1900/3415  loss=1.7701\n",
            "    batch 1950/3415  loss=0.9911\n",
            "    batch 2000/3415  loss=1.6692\n",
            "    batch 2050/3415  loss=0.5870\n",
            "    batch 2100/3415  loss=1.1716\n",
            "    batch 2150/3415  loss=0.6771\n",
            "    batch 2200/3415  loss=1.5799\n",
            "    batch 2250/3415  loss=1.3301\n",
            "    batch 2300/3415  loss=0.8838\n",
            "    batch 2350/3415  loss=1.2217\n",
            "    batch 2400/3415  loss=0.6795\n",
            "    batch 2450/3415  loss=0.9003\n",
            "    batch 2500/3415  loss=0.5850\n",
            "    batch 2550/3415  loss=0.4542\n",
            "    batch 2600/3415  loss=1.2455\n",
            "    batch 2650/3415  loss=1.6457\n",
            "    batch 2700/3415  loss=1.2716\n",
            "    batch 2750/3415  loss=0.7906\n",
            "    batch 2800/3415  loss=1.1243\n",
            "    batch 2850/3415  loss=0.8182\n",
            "    batch 2900/3415  loss=1.6056\n",
            "    batch 2950/3415  loss=1.1215\n",
            "    batch 3000/3415  loss=0.9951\n",
            "    batch 3050/3415  loss=0.6013\n",
            "    batch 3100/3415  loss=2.1146\n",
            "    batch 3150/3415  loss=1.1813\n",
            "    batch 3200/3415  loss=1.2783\n",
            "    batch 3250/3415  loss=1.1926\n",
            "    batch 3300/3415  loss=1.2445\n",
            "    batch 3350/3415  loss=0.7894\n",
            "    batch 3400/3415  loss=0.7744\n",
            "  train_loss=1.1119  val_loss=2.5079  val_CER=0.6395  val_WER=0.9106\n",
            "\n",
            "Epoch 14:\n",
            "    batch 0/3415  loss=1.6930\n",
            "    batch 50/3415  loss=0.4991\n",
            "    batch 100/3415  loss=1.7857\n",
            "    batch 150/3415  loss=0.5935\n",
            "    batch 200/3415  loss=1.2726\n",
            "    batch 250/3415  loss=1.1776\n",
            "    batch 300/3415  loss=0.7030\n",
            "    batch 350/3415  loss=1.3479\n",
            "    batch 400/3415  loss=0.2863\n",
            "    batch 450/3415  loss=1.3643\n",
            "    batch 500/3415  loss=0.3644\n",
            "    batch 550/3415  loss=0.8630\n",
            "    batch 600/3415  loss=1.1498\n",
            "    batch 650/3415  loss=0.7364\n",
            "    batch 700/3415  loss=0.9234\n",
            "    batch 750/3415  loss=1.1430\n",
            "    batch 800/3415  loss=0.8561\n",
            "    batch 850/3415  loss=1.4780\n",
            "    batch 900/3415  loss=1.0050\n",
            "    batch 950/3415  loss=1.0530\n",
            "    batch 1000/3415  loss=0.8704\n",
            "    batch 1050/3415  loss=0.9096\n",
            "    batch 1100/3415  loss=0.3566\n",
            "    batch 1150/3415  loss=1.4751\n",
            "    batch 1200/3415  loss=1.1534\n",
            "    batch 1250/3415  loss=0.9171\n",
            "    batch 1300/3415  loss=1.0447\n",
            "    batch 1350/3415  loss=1.3056\n",
            "    batch 1400/3415  loss=1.2278\n",
            "    batch 1450/3415  loss=1.7457\n",
            "    batch 1500/3415  loss=1.5419\n",
            "    batch 1550/3415  loss=1.9480\n",
            "    batch 1600/3415  loss=1.2726\n",
            "    batch 1650/3415  loss=1.1954\n",
            "    batch 1700/3415  loss=1.2295\n",
            "    batch 1750/3415  loss=0.8213\n",
            "    batch 1800/3415  loss=0.4486\n",
            "    batch 1850/3415  loss=1.7903\n",
            "    batch 1900/3415  loss=0.6615\n",
            "    batch 1950/3415  loss=0.8790\n",
            "    batch 2000/3415  loss=0.4920\n",
            "    batch 2050/3415  loss=1.8460\n",
            "    batch 2100/3415  loss=1.0097\n",
            "    batch 2150/3415  loss=0.9657\n",
            "    batch 2200/3415  loss=1.1839\n",
            "    batch 2250/3415  loss=0.5831\n",
            "    batch 2300/3415  loss=0.9629\n",
            "    batch 2350/3415  loss=0.6449\n",
            "    batch 2400/3415  loss=1.0336\n",
            "    batch 2450/3415  loss=1.1092\n",
            "    batch 2500/3415  loss=1.1502\n",
            "    batch 2550/3415  loss=1.2932\n",
            "    batch 2600/3415  loss=0.6556\n",
            "    batch 2650/3415  loss=0.7819\n",
            "    batch 2700/3415  loss=0.9175\n",
            "    batch 2750/3415  loss=1.5288\n",
            "    batch 2800/3415  loss=0.6819\n",
            "    batch 2850/3415  loss=0.8135\n",
            "    batch 2900/3415  loss=1.2498\n",
            "    batch 2950/3415  loss=1.3984\n",
            "    batch 3000/3415  loss=0.9650\n",
            "    batch 3050/3415  loss=1.8803\n",
            "    batch 3100/3415  loss=0.4065\n",
            "    batch 3150/3415  loss=0.8549\n",
            "    batch 3200/3415  loss=0.8873\n",
            "    batch 3250/3415  loss=1.5938\n",
            "    batch 3300/3415  loss=0.9977\n",
            "    batch 3350/3415  loss=0.6947\n",
            "    batch 3400/3415  loss=1.0359\n",
            "  train_loss=1.0456  val_loss=2.5153  val_CER=0.5795  val_WER=0.8951\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 15:\n",
            "    batch 0/3415  loss=1.7079\n",
            "    batch 50/3415  loss=0.7980\n",
            "    batch 100/3415  loss=0.3155\n",
            "    batch 150/3415  loss=0.9629\n",
            "    batch 200/3415  loss=0.8185\n",
            "    batch 250/3415  loss=0.9316\n",
            "    batch 300/3415  loss=0.6001\n",
            "    batch 350/3415  loss=1.2391\n",
            "    batch 400/3415  loss=0.9354\n",
            "    batch 450/3415  loss=1.0168\n",
            "    batch 500/3415  loss=0.9230\n",
            "    batch 550/3415  loss=0.7333\n",
            "    batch 600/3415  loss=1.5411\n",
            "    batch 650/3415  loss=1.4184\n",
            "    batch 700/3415  loss=0.6084\n",
            "    batch 750/3415  loss=0.8031\n",
            "    batch 800/3415  loss=0.9368\n",
            "    batch 850/3415  loss=1.4854\n",
            "    batch 900/3415  loss=0.6340\n",
            "    batch 950/3415  loss=1.4529\n",
            "    batch 1000/3415  loss=1.3361\n",
            "    batch 1050/3415  loss=1.0940\n",
            "    batch 1100/3415  loss=0.4082\n",
            "    batch 1150/3415  loss=0.3321\n",
            "    batch 1200/3415  loss=1.1283\n",
            "    batch 1250/3415  loss=0.8718\n",
            "    batch 1300/3415  loss=0.6920\n",
            "    batch 1350/3415  loss=0.7526\n",
            "    batch 1400/3415  loss=0.3567\n",
            "    batch 1450/3415  loss=1.4103\n",
            "    batch 1500/3415  loss=0.2657\n",
            "    batch 1550/3415  loss=1.0331\n",
            "    batch 1600/3415  loss=0.7695\n",
            "    batch 1650/3415  loss=0.9915\n",
            "    batch 1700/3415  loss=1.1461\n",
            "    batch 1750/3415  loss=1.4493\n",
            "    batch 1800/3415  loss=0.9886\n",
            "    batch 1850/3415  loss=0.5535\n",
            "    batch 1900/3415  loss=1.0107\n",
            "    batch 1950/3415  loss=0.7582\n",
            "    batch 2000/3415  loss=0.9209\n",
            "    batch 2050/3415  loss=0.7854\n",
            "    batch 2100/3415  loss=1.3523\n",
            "    batch 2150/3415  loss=1.7475\n",
            "    batch 2200/3415  loss=1.4667\n",
            "    batch 2250/3415  loss=1.0315\n",
            "    batch 2300/3415  loss=1.5449\n",
            "    batch 2350/3415  loss=1.0849\n",
            "    batch 2400/3415  loss=0.5888\n",
            "    batch 2450/3415  loss=1.0432\n",
            "    batch 2500/3415  loss=0.4920\n",
            "    batch 2550/3415  loss=0.6895\n",
            "    batch 2600/3415  loss=0.9338\n",
            "    batch 2650/3415  loss=0.6487\n",
            "    batch 2700/3415  loss=0.5150\n",
            "    batch 2750/3415  loss=0.6717\n",
            "    batch 2800/3415  loss=0.9212\n",
            "    batch 2850/3415  loss=0.4609\n",
            "    batch 2900/3415  loss=0.3542\n",
            "    batch 2950/3415  loss=0.8852\n",
            "    batch 3000/3415  loss=1.0055\n",
            "    batch 3050/3415  loss=0.9269\n",
            "    batch 3100/3415  loss=1.2909\n",
            "    batch 3150/3415  loss=0.7317\n",
            "    batch 3200/3415  loss=0.6088\n",
            "    batch 3250/3415  loss=0.8822\n",
            "    batch 3300/3415  loss=1.2275\n",
            "    batch 3350/3415  loss=1.0118\n",
            "    batch 3400/3415  loss=1.2799\n",
            "  train_loss=0.9891  val_loss=2.4694  val_CER=0.5789  val_WER=0.8823\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 16:\n",
            "    batch 0/3415  loss=1.3148\n",
            "    batch 50/3415  loss=1.0701\n",
            "    batch 100/3415  loss=1.9598\n",
            "    batch 150/3415  loss=1.3511\n",
            "    batch 200/3415  loss=0.3022\n",
            "    batch 250/3415  loss=0.9932\n",
            "    batch 300/3415  loss=1.0041\n",
            "    batch 350/3415  loss=1.7885\n",
            "    batch 400/3415  loss=0.4658\n",
            "    batch 450/3415  loss=0.6802\n",
            "    batch 500/3415  loss=0.8394\n",
            "    batch 550/3415  loss=1.2139\n",
            "    batch 600/3415  loss=0.4460\n",
            "    batch 650/3415  loss=0.9879\n",
            "    batch 700/3415  loss=1.1670\n",
            "    batch 750/3415  loss=0.5466\n",
            "    batch 800/3415  loss=0.7437\n",
            "    batch 850/3415  loss=1.1497\n",
            "    batch 900/3415  loss=1.3483\n",
            "    batch 950/3415  loss=0.8850\n",
            "    batch 1000/3415  loss=1.2051\n",
            "    batch 1050/3415  loss=1.1021\n",
            "    batch 1100/3415  loss=1.0767\n",
            "    batch 1150/3415  loss=0.7070\n",
            "    batch 1200/3415  loss=0.5369\n",
            "    batch 1250/3415  loss=0.6337\n",
            "    batch 1300/3415  loss=1.1732\n",
            "    batch 1350/3415  loss=0.7753\n",
            "    batch 1400/3415  loss=0.8562\n",
            "    batch 1450/3415  loss=0.6758\n",
            "    batch 1500/3415  loss=1.6221\n",
            "    batch 1550/3415  loss=0.9830\n",
            "    batch 1600/3415  loss=0.9198\n",
            "    batch 1650/3415  loss=1.6494\n",
            "    batch 1700/3415  loss=1.0557\n",
            "    batch 1750/3415  loss=0.7692\n",
            "    batch 1800/3415  loss=1.0004\n",
            "    batch 1850/3415  loss=0.7633\n",
            "    batch 1900/3415  loss=0.5074\n",
            "    batch 1950/3415  loss=0.4214\n",
            "    batch 2000/3415  loss=0.2263\n",
            "    batch 2050/3415  loss=0.8952\n",
            "    batch 2100/3415  loss=0.8926\n",
            "    batch 2150/3415  loss=1.0633\n",
            "    batch 2200/3415  loss=0.7227\n",
            "    batch 2250/3415  loss=1.2578\n",
            "    batch 2300/3415  loss=1.3247\n",
            "    batch 2350/3415  loss=0.9742\n",
            "    batch 2400/3415  loss=0.9373\n",
            "    batch 2450/3415  loss=1.6953\n",
            "    batch 2500/3415  loss=1.2064\n",
            "    batch 2550/3415  loss=0.4610\n",
            "    batch 2600/3415  loss=0.8397\n",
            "    batch 2650/3415  loss=1.1785\n",
            "    batch 2700/3415  loss=1.0254\n",
            "    batch 2750/3415  loss=1.0869\n",
            "    batch 2800/3415  loss=0.3157\n",
            "    batch 2850/3415  loss=0.7035\n",
            "    batch 2900/3415  loss=1.0146\n",
            "    batch 2950/3415  loss=0.8865\n",
            "    batch 3000/3415  loss=1.4683\n",
            "    batch 3050/3415  loss=0.7021\n",
            "    batch 3100/3415  loss=1.1342\n",
            "    batch 3150/3415  loss=0.6371\n",
            "    batch 3200/3415  loss=0.7507\n",
            "    batch 3250/3415  loss=1.0712\n",
            "    batch 3300/3415  loss=0.1510\n",
            "    batch 3350/3415  loss=0.7298\n",
            "    batch 3400/3415  loss=1.5166\n",
            "  train_loss=0.9335  val_loss=2.4905  val_CER=0.5634  val_WER=0.8790\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 17:\n",
            "    batch 0/3415  loss=1.0762\n",
            "    batch 50/3415  loss=1.1363\n",
            "    batch 100/3415  loss=0.8472\n",
            "    batch 150/3415  loss=1.4019\n",
            "    batch 200/3415  loss=1.4319\n",
            "    batch 250/3415  loss=0.8773\n",
            "    batch 300/3415  loss=1.3279\n",
            "    batch 350/3415  loss=0.5602\n",
            "    batch 400/3415  loss=0.5341\n",
            "    batch 450/3415  loss=1.0838\n",
            "    batch 500/3415  loss=0.7024\n",
            "    batch 550/3415  loss=1.7263\n",
            "    batch 600/3415  loss=0.7373\n",
            "    batch 650/3415  loss=0.8370\n",
            "    batch 700/3415  loss=0.8844\n",
            "    batch 750/3415  loss=1.1327\n",
            "    batch 800/3415  loss=1.4659\n",
            "    batch 850/3415  loss=0.5721\n",
            "    batch 900/3415  loss=0.7454\n",
            "    batch 950/3415  loss=0.5661\n",
            "    batch 1000/3415  loss=1.3425\n",
            "    batch 1050/3415  loss=1.0411\n",
            "    batch 1100/3415  loss=1.3875\n",
            "    batch 1150/3415  loss=0.7193\n",
            "    batch 1200/3415  loss=0.4765\n",
            "    batch 1250/3415  loss=0.9357\n",
            "    batch 1300/3415  loss=0.7148\n",
            "    batch 1350/3415  loss=1.1953\n",
            "    batch 1400/3415  loss=0.8604\n",
            "    batch 1450/3415  loss=1.2612\n",
            "    batch 1500/3415  loss=1.1718\n",
            "    batch 1550/3415  loss=1.6310\n",
            "    batch 1600/3415  loss=0.4362\n",
            "    batch 1650/3415  loss=1.4989\n",
            "    batch 1700/3415  loss=1.4786\n",
            "    batch 1750/3415  loss=0.9898\n",
            "    batch 1800/3415  loss=1.4192\n",
            "    batch 1850/3415  loss=0.3208\n",
            "    batch 1900/3415  loss=0.5369\n",
            "    batch 1950/3415  loss=1.5547\n",
            "    batch 2000/3415  loss=0.9303\n",
            "    batch 2050/3415  loss=1.1048\n",
            "    batch 2100/3415  loss=0.6342\n",
            "    batch 2150/3415  loss=0.8726\n",
            "    batch 2200/3415  loss=0.7100\n",
            "    batch 2250/3415  loss=0.6768\n",
            "    batch 2300/3415  loss=0.6236\n",
            "    batch 2350/3415  loss=0.9015\n",
            "    batch 2400/3415  loss=0.8121\n",
            "    batch 2450/3415  loss=1.1299\n",
            "    batch 2500/3415  loss=0.6088\n",
            "    batch 2550/3415  loss=0.7495\n",
            "    batch 2600/3415  loss=0.8999\n",
            "    batch 2650/3415  loss=1.0771\n",
            "    batch 2700/3415  loss=0.7274\n",
            "    batch 2750/3415  loss=1.1872\n",
            "    batch 2800/3415  loss=0.5997\n",
            "    batch 2850/3415  loss=1.0785\n",
            "    batch 2900/3415  loss=1.2813\n",
            "    batch 2950/3415  loss=1.1427\n",
            "    batch 3000/3415  loss=1.1408\n",
            "    batch 3050/3415  loss=0.7470\n",
            "    batch 3100/3415  loss=0.7548\n",
            "    batch 3150/3415  loss=0.7503\n",
            "    batch 3200/3415  loss=1.0853\n",
            "    batch 3250/3415  loss=0.5466\n",
            "    batch 3300/3415  loss=1.2859\n",
            "    batch 3350/3415  loss=0.5375\n",
            "    batch 3400/3415  loss=0.9634\n",
            "  train_loss=0.8842  val_loss=2.5183  val_CER=0.5592  val_WER=0.8744\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 18:\n",
            "    batch 0/3415  loss=0.3757\n",
            "    batch 50/3415  loss=0.6192\n",
            "    batch 100/3415  loss=0.7426\n",
            "    batch 150/3415  loss=0.3308\n",
            "    batch 200/3415  loss=1.1051\n",
            "    batch 250/3415  loss=1.3130\n",
            "    batch 300/3415  loss=0.6594\n",
            "    batch 350/3415  loss=1.5051\n",
            "    batch 400/3415  loss=0.8828\n",
            "    batch 450/3415  loss=0.7133\n",
            "    batch 500/3415  loss=0.9637\n",
            "    batch 550/3415  loss=0.6750\n",
            "    batch 600/3415  loss=1.1333\n",
            "    batch 650/3415  loss=0.3448\n",
            "    batch 700/3415  loss=1.0337\n",
            "    batch 750/3415  loss=0.9898\n",
            "    batch 800/3415  loss=1.3847\n",
            "    batch 850/3415  loss=0.8448\n",
            "    batch 900/3415  loss=0.3674\n",
            "    batch 950/3415  loss=1.0504\n",
            "    batch 1000/3415  loss=0.3185\n",
            "    batch 1050/3415  loss=0.0649\n",
            "    batch 1100/3415  loss=0.6576\n",
            "    batch 1150/3415  loss=0.8365\n",
            "    batch 1200/3415  loss=0.9588\n",
            "    batch 1250/3415  loss=0.6962\n",
            "    batch 1300/3415  loss=1.4945\n",
            "    batch 1350/3415  loss=1.0344\n",
            "    batch 1400/3415  loss=1.2714\n",
            "    batch 1450/3415  loss=0.7993\n",
            "    batch 1500/3415  loss=0.7439\n",
            "    batch 1550/3415  loss=0.6315\n",
            "    batch 1600/3415  loss=0.4941\n",
            "    batch 1650/3415  loss=0.9618\n",
            "    batch 1700/3415  loss=1.7509\n",
            "    batch 1750/3415  loss=0.6195\n",
            "    batch 1800/3415  loss=0.8749\n",
            "    batch 1850/3415  loss=1.3514\n",
            "    batch 1900/3415  loss=0.6415\n",
            "    batch 1950/3415  loss=0.7884\n",
            "    batch 2000/3415  loss=0.8018\n",
            "    batch 2050/3415  loss=0.5478\n",
            "    batch 2100/3415  loss=1.3790\n",
            "    batch 2150/3415  loss=1.3837\n",
            "    batch 2200/3415  loss=0.3771\n",
            "    batch 2250/3415  loss=1.0973\n",
            "    batch 2300/3415  loss=1.2506\n",
            "    batch 2350/3415  loss=0.9333\n",
            "    batch 2400/3415  loss=0.3143\n",
            "    batch 2450/3415  loss=0.5485\n",
            "    batch 2500/3415  loss=0.7516\n",
            "    batch 2550/3415  loss=0.8244\n",
            "    batch 2600/3415  loss=0.9218\n",
            "    batch 2650/3415  loss=0.2348\n",
            "    batch 2700/3415  loss=0.3301\n",
            "    batch 2750/3415  loss=1.1818\n",
            "    batch 2800/3415  loss=0.4008\n",
            "    batch 2850/3415  loss=0.6663\n",
            "    batch 2900/3415  loss=1.0683\n",
            "    batch 2950/3415  loss=0.4323\n",
            "    batch 3000/3415  loss=0.3924\n",
            "    batch 3050/3415  loss=0.6133\n",
            "    batch 3100/3415  loss=0.5725\n",
            "    batch 3150/3415  loss=0.9227\n",
            "    batch 3200/3415  loss=0.7482\n",
            "    batch 3250/3415  loss=0.8564\n",
            "    batch 3300/3415  loss=0.4042\n",
            "    batch 3350/3415  loss=1.3991\n",
            "    batch 3400/3415  loss=0.6036\n",
            "  train_loss=0.8353  val_loss=2.4724  val_CER=0.5521  val_WER=0.8642\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 19:\n",
            "    batch 0/3415  loss=0.7661\n",
            "    batch 50/3415  loss=0.7773\n",
            "    batch 100/3415  loss=0.8797\n",
            "    batch 150/3415  loss=1.3528\n",
            "    batch 200/3415  loss=0.3293\n",
            "    batch 250/3415  loss=0.8846\n",
            "    batch 300/3415  loss=0.7393\n",
            "    batch 350/3415  loss=1.2493\n",
            "    batch 400/3415  loss=0.8374\n",
            "    batch 450/3415  loss=0.7141\n",
            "    batch 500/3415  loss=0.6170\n",
            "    batch 550/3415  loss=0.4516\n",
            "    batch 600/3415  loss=0.4092\n",
            "    batch 650/3415  loss=0.5368\n",
            "    batch 700/3415  loss=0.3868\n",
            "    batch 750/3415  loss=0.2981\n",
            "    batch 800/3415  loss=0.5479\n",
            "    batch 850/3415  loss=0.4781\n",
            "    batch 900/3415  loss=0.4775\n",
            "    batch 950/3415  loss=0.9591\n",
            "    batch 1000/3415  loss=0.9070\n",
            "    batch 1050/3415  loss=0.3447\n",
            "    batch 1100/3415  loss=1.0911\n",
            "    batch 1150/3415  loss=1.0707\n",
            "    batch 1200/3415  loss=1.2279\n",
            "    batch 1250/3415  loss=0.5874\n",
            "    batch 1300/3415  loss=0.3657\n",
            "    batch 1350/3415  loss=0.5461\n",
            "    batch 1400/3415  loss=1.0934\n",
            "    batch 1450/3415  loss=0.8806\n",
            "    batch 1500/3415  loss=1.2508\n",
            "    batch 1550/3415  loss=0.1964\n",
            "    batch 1600/3415  loss=0.9319\n",
            "    batch 1650/3415  loss=0.7534\n",
            "    batch 1700/3415  loss=1.2908\n",
            "    batch 1750/3415  loss=0.9838\n",
            "    batch 1800/3415  loss=0.8009\n",
            "    batch 1850/3415  loss=0.6802\n",
            "    batch 1900/3415  loss=0.6597\n",
            "    batch 1950/3415  loss=0.9309\n",
            "    batch 2000/3415  loss=1.5584\n",
            "    batch 2050/3415  loss=1.1147\n",
            "    batch 2100/3415  loss=1.3891\n",
            "    batch 2150/3415  loss=0.4336\n",
            "    batch 2200/3415  loss=0.6001\n",
            "    batch 2250/3415  loss=0.9054\n",
            "    batch 2300/3415  loss=0.2043\n",
            "    batch 2350/3415  loss=0.6954\n",
            "    batch 2400/3415  loss=0.9854\n",
            "    batch 2450/3415  loss=0.8495\n",
            "    batch 2500/3415  loss=0.4097\n",
            "    batch 2550/3415  loss=1.7426\n",
            "    batch 2600/3415  loss=0.2729\n",
            "    batch 2650/3415  loss=0.5930\n",
            "    batch 2700/3415  loss=1.2085\n",
            "    batch 2750/3415  loss=1.1068\n",
            "    batch 2800/3415  loss=0.8015\n",
            "    batch 2850/3415  loss=1.1061\n",
            "    batch 2900/3415  loss=0.5600\n",
            "    batch 2950/3415  loss=0.9640\n",
            "    batch 3000/3415  loss=0.8709\n",
            "    batch 3050/3415  loss=0.4775\n",
            "    batch 3100/3415  loss=0.5760\n",
            "    batch 3150/3415  loss=1.0685\n",
            "    batch 3200/3415  loss=0.8580\n",
            "    batch 3250/3415  loss=0.7184\n",
            "    batch 3300/3415  loss=0.5959\n",
            "    batch 3350/3415  loss=1.1598\n",
            "    batch 3400/3415  loss=0.8875\n",
            "  train_loss=0.7860  val_loss=2.5125  val_CER=0.5409  val_WER=0.8638\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 20:\n",
            "    batch 0/3415  loss=1.0125\n",
            "    batch 50/3415  loss=1.1401\n",
            "    batch 100/3415  loss=0.5185\n",
            "    batch 150/3415  loss=0.7510\n",
            "    batch 200/3415  loss=0.8670\n",
            "    batch 250/3415  loss=0.5074\n",
            "    batch 300/3415  loss=0.2261\n",
            "    batch 350/3415  loss=0.6879\n",
            "    batch 400/3415  loss=1.4153\n",
            "    batch 450/3415  loss=0.5749\n",
            "    batch 500/3415  loss=0.8009\n",
            "    batch 550/3415  loss=0.6374\n",
            "    batch 600/3415  loss=0.8487\n",
            "    batch 650/3415  loss=1.1546\n",
            "    batch 700/3415  loss=0.4767\n",
            "    batch 750/3415  loss=1.0856\n",
            "    batch 800/3415  loss=0.3644\n",
            "    batch 850/3415  loss=0.8446\n",
            "    batch 900/3415  loss=0.8244\n",
            "    batch 950/3415  loss=0.3883\n",
            "    batch 1000/3415  loss=0.5664\n",
            "    batch 1050/3415  loss=0.4404\n",
            "    batch 1100/3415  loss=0.7364\n",
            "    batch 1150/3415  loss=0.3465\n",
            "    batch 1200/3415  loss=0.5191\n",
            "    batch 1250/3415  loss=1.1052\n",
            "    batch 1300/3415  loss=0.6793\n",
            "    batch 1350/3415  loss=0.5011\n",
            "    batch 1400/3415  loss=0.0471\n",
            "    batch 1450/3415  loss=0.7710\n",
            "    batch 1500/3415  loss=0.6330\n",
            "    batch 1550/3415  loss=0.6803\n",
            "    batch 1600/3415  loss=0.8045\n",
            "    batch 1650/3415  loss=0.5054\n",
            "    batch 1700/3415  loss=0.7098\n",
            "    batch 1750/3415  loss=0.6840\n",
            "    batch 1800/3415  loss=1.1224\n",
            "    batch 1850/3415  loss=0.6335\n",
            "    batch 1900/3415  loss=0.5802\n",
            "    batch 1950/3415  loss=0.5368\n",
            "    batch 2000/3415  loss=0.9230\n",
            "    batch 2050/3415  loss=0.7870\n",
            "    batch 2100/3415  loss=0.7766\n",
            "    batch 2150/3415  loss=1.1951\n",
            "    batch 2200/3415  loss=0.9172\n",
            "    batch 2250/3415  loss=0.7958\n",
            "    batch 2300/3415  loss=1.1922\n",
            "    batch 2350/3415  loss=0.7361\n",
            "    batch 2400/3415  loss=0.3467\n",
            "    batch 2450/3415  loss=0.9615\n",
            "    batch 2500/3415  loss=0.4003\n",
            "    batch 2550/3415  loss=0.5289\n",
            "    batch 2600/3415  loss=1.2091\n",
            "    batch 2650/3415  loss=0.5712\n",
            "    batch 2700/3415  loss=1.0763\n",
            "    batch 2750/3415  loss=0.4409\n",
            "    batch 2800/3415  loss=0.6969\n",
            "    batch 2850/3415  loss=0.8949\n",
            "    batch 2900/3415  loss=0.6396\n",
            "    batch 2950/3415  loss=0.6838\n",
            "    batch 3000/3415  loss=1.0516\n",
            "    batch 3050/3415  loss=0.6996\n",
            "    batch 3100/3415  loss=0.9719\n",
            "    batch 3150/3415  loss=0.9507\n",
            "    batch 3200/3415  loss=0.5129\n",
            "    batch 3250/3415  loss=0.1236\n",
            "    batch 3300/3415  loss=0.3529\n",
            "    batch 3350/3415  loss=0.9731\n",
            "    batch 3400/3415  loss=1.3425\n",
            "  train_loss=0.7422  val_loss=2.5919  val_CER=0.5374  val_WER=0.8575\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 21:\n",
            "    batch 0/3415  loss=0.5983\n",
            "    batch 50/3415  loss=0.7338\n",
            "    batch 100/3415  loss=0.5973\n",
            "    batch 150/3415  loss=0.3532\n",
            "    batch 200/3415  loss=0.4773\n",
            "    batch 250/3415  loss=0.8650\n",
            "    batch 300/3415  loss=0.4822\n",
            "    batch 350/3415  loss=0.4302\n",
            "    batch 400/3415  loss=0.3782\n",
            "    batch 450/3415  loss=0.6795\n",
            "    batch 500/3415  loss=0.4475\n",
            "    batch 550/3415  loss=0.3720\n",
            "    batch 600/3415  loss=0.7066\n",
            "    batch 650/3415  loss=0.3885\n",
            "    batch 700/3415  loss=0.3403\n",
            "    batch 750/3415  loss=0.8638\n",
            "    batch 800/3415  loss=1.0650\n",
            "    batch 850/3415  loss=0.3686\n",
            "    batch 900/3415  loss=0.6407\n",
            "    batch 950/3415  loss=0.4605\n",
            "    batch 1000/3415  loss=0.9146\n",
            "    batch 1050/3415  loss=0.7546\n",
            "    batch 1100/3415  loss=0.2216\n",
            "    batch 1150/3415  loss=1.3064\n",
            "    batch 1200/3415  loss=0.6872\n",
            "    batch 1250/3415  loss=0.8287\n",
            "    batch 1300/3415  loss=0.4771\n",
            "    batch 1350/3415  loss=0.3258\n",
            "    batch 1400/3415  loss=1.2011\n",
            "    batch 1450/3415  loss=0.4193\n",
            "    batch 1500/3415  loss=0.8999\n",
            "    batch 1550/3415  loss=0.7120\n",
            "    batch 1600/3415  loss=0.8128\n",
            "    batch 1650/3415  loss=0.6932\n",
            "    batch 1700/3415  loss=0.4709\n",
            "    batch 1750/3415  loss=1.0633\n",
            "    batch 1800/3415  loss=1.0492\n",
            "    batch 1850/3415  loss=0.6835\n",
            "    batch 1900/3415  loss=0.8399\n",
            "    batch 1950/3415  loss=0.5510\n",
            "    batch 2000/3415  loss=0.8645\n",
            "    batch 2050/3415  loss=1.0922\n",
            "    batch 2100/3415  loss=1.0652\n",
            "    batch 2150/3415  loss=0.6708\n",
            "    batch 2200/3415  loss=1.1217\n",
            "    batch 2250/3415  loss=1.0883\n",
            "    batch 2300/3415  loss=0.6152\n",
            "    batch 2350/3415  loss=0.9540\n",
            "    batch 2400/3415  loss=0.8793\n",
            "    batch 2450/3415  loss=0.1643\n",
            "    batch 2500/3415  loss=0.5773\n",
            "    batch 2550/3415  loss=0.8992\n",
            "    batch 2600/3415  loss=0.9421\n",
            "    batch 2650/3415  loss=0.3955\n",
            "    batch 2700/3415  loss=0.8514\n",
            "    batch 2750/3415  loss=0.5039\n",
            "    batch 2800/3415  loss=1.1396\n",
            "    batch 2850/3415  loss=0.6134\n",
            "    batch 2900/3415  loss=1.3822\n",
            "    batch 2950/3415  loss=0.8901\n",
            "    batch 3000/3415  loss=0.4993\n",
            "    batch 3050/3415  loss=0.6111\n",
            "    batch 3100/3415  loss=0.7916\n",
            "    batch 3150/3415  loss=0.6049\n",
            "    batch 3200/3415  loss=0.4370\n",
            "    batch 3250/3415  loss=0.8128\n",
            "    batch 3300/3415  loss=1.4864\n",
            "    batch 3350/3415  loss=1.1557\n",
            "    batch 3400/3415  loss=0.3569\n",
            "  train_loss=0.6986  val_loss=2.6094  val_CER=0.5312  val_WER=0.8575\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "\n",
            "Epoch 22:\n",
            "    batch 0/3415  loss=0.6390\n",
            "    batch 50/3415  loss=0.9173\n",
            "    batch 100/3415  loss=0.8902\n",
            "    batch 150/3415  loss=0.2628\n",
            "    batch 200/3415  loss=0.5914\n",
            "    batch 250/3415  loss=0.8219\n",
            "    batch 300/3415  loss=0.2921\n",
            "    batch 350/3415  loss=0.7602\n",
            "    batch 400/3415  loss=0.9887\n",
            "    batch 450/3415  loss=1.0931\n",
            "    batch 500/3415  loss=0.7593\n",
            "    batch 550/3415  loss=0.7391\n",
            "    batch 600/3415  loss=0.4939\n",
            "    batch 650/3415  loss=0.6147\n",
            "    batch 700/3415  loss=1.0485\n",
            "    batch 750/3415  loss=0.2319\n",
            "    batch 800/3415  loss=0.9008\n",
            "    batch 850/3415  loss=0.5393\n",
            "    batch 900/3415  loss=0.6853\n",
            "    batch 950/3415  loss=0.3161\n",
            "    batch 1000/3415  loss=0.4693\n",
            "    batch 1050/3415  loss=0.5048\n",
            "    batch 1100/3415  loss=1.1678\n",
            "    batch 1150/3415  loss=0.6296\n",
            "    batch 1200/3415  loss=0.2309\n",
            "    batch 1250/3415  loss=0.9889\n",
            "    batch 1300/3415  loss=0.3807\n",
            "    batch 1350/3415  loss=0.4040\n",
            "    batch 1400/3415  loss=0.7563\n",
            "    batch 1450/3415  loss=0.6675\n",
            "    batch 1500/3415  loss=1.1175\n",
            "    batch 1550/3415  loss=1.0674\n",
            "    batch 1600/3415  loss=0.7221\n",
            "    batch 1650/3415  loss=1.0559\n",
            "    batch 1700/3415  loss=0.4408\n",
            "    batch 1750/3415  loss=0.6780\n",
            "    batch 1800/3415  loss=0.3136\n",
            "    batch 1850/3415  loss=0.9247\n",
            "    batch 1900/3415  loss=0.6941\n",
            "    batch 1950/3415  loss=1.0952\n",
            "    batch 2000/3415  loss=0.6576\n",
            "    batch 2050/3415  loss=0.5221\n",
            "    batch 2100/3415  loss=0.6183\n",
            "    batch 2150/3415  loss=0.5006\n",
            "    batch 2200/3415  loss=1.3768\n",
            "    batch 2250/3415  loss=0.4749\n",
            "    batch 2300/3415  loss=1.2496\n",
            "    batch 2350/3415  loss=0.4247\n",
            "    batch 2400/3415  loss=0.3274\n",
            "    batch 2450/3415  loss=0.7904\n",
            "    batch 2500/3415  loss=0.6473\n",
            "    batch 2550/3415  loss=0.5287\n",
            "    batch 2600/3415  loss=0.5929\n",
            "    batch 2650/3415  loss=0.7438\n",
            "    batch 2700/3415  loss=0.9256\n",
            "    batch 2750/3415  loss=0.2319\n",
            "    batch 2800/3415  loss=0.7038\n",
            "    batch 2850/3415  loss=0.4687\n",
            "    batch 2900/3415  loss=0.8351\n",
            "    batch 2950/3415  loss=0.9259\n",
            "    batch 3000/3415  loss=0.8894\n",
            "    batch 3050/3415  loss=0.2821\n",
            "    batch 3100/3415  loss=1.0809\n",
            "    batch 3150/3415  loss=0.2602\n",
            "    batch 3200/3415  loss=0.8210\n",
            "    batch 3250/3415  loss=0.8330\n",
            "    batch 3300/3415  loss=0.8993\n",
            "    batch 3350/3415  loss=0.6975\n",
            "    batch 3400/3415  loss=0.6745\n",
            "  train_loss=0.6576  val_loss=2.6194  val_CER=0.5348  val_WER=0.8577\n",
            "\n",
            "Epoch 23:\n",
            "    batch 0/3415  loss=0.6782\n",
            "    batch 50/3415  loss=0.7548\n",
            "    batch 100/3415  loss=0.1601\n",
            "    batch 150/3415  loss=0.9355\n",
            "    batch 200/3415  loss=0.5311\n",
            "    batch 250/3415  loss=0.7589\n",
            "    batch 300/3415  loss=0.6504\n",
            "    batch 350/3415  loss=0.7538\n",
            "    batch 400/3415  loss=1.1041\n",
            "    batch 450/3415  loss=0.4583\n",
            "    batch 500/3415  loss=0.1749\n",
            "    batch 550/3415  loss=0.3972\n",
            "    batch 600/3415  loss=0.4671\n",
            "    batch 650/3415  loss=0.6000\n",
            "    batch 700/3415  loss=0.5313\n",
            "    batch 750/3415  loss=0.4456\n",
            "    batch 800/3415  loss=0.4412\n",
            "    batch 850/3415  loss=0.4178\n",
            "    batch 900/3415  loss=1.0565\n",
            "    batch 950/3415  loss=1.1869\n",
            "    batch 1000/3415  loss=0.2299\n",
            "    batch 1050/3415  loss=0.4348\n",
            "    batch 1100/3415  loss=0.7745\n",
            "    batch 1150/3415  loss=0.8478\n",
            "    batch 1200/3415  loss=0.8055\n",
            "    batch 1250/3415  loss=0.6929\n",
            "    batch 1300/3415  loss=0.5812\n",
            "    batch 1350/3415  loss=1.0360\n",
            "    batch 1400/3415  loss=0.4345\n",
            "    batch 1450/3415  loss=0.3489\n",
            "    batch 1500/3415  loss=1.1318\n",
            "    batch 1550/3415  loss=0.8703\n",
            "    batch 1600/3415  loss=0.6059\n",
            "    batch 1650/3415  loss=0.8237\n",
            "    batch 1700/3415  loss=0.9528\n",
            "    batch 1750/3415  loss=0.8255\n",
            "    batch 1800/3415  loss=0.2968\n",
            "    batch 1850/3415  loss=1.1462\n",
            "    batch 1900/3415  loss=0.6128\n",
            "    batch 1950/3415  loss=0.6389\n",
            "    batch 2000/3415  loss=0.7047\n",
            "    batch 2050/3415  loss=0.7240\n",
            "    batch 2100/3415  loss=0.0008\n",
            "    batch 2150/3415  loss=0.3512\n",
            "    batch 2200/3415  loss=0.0480\n",
            "    batch 2250/3415  loss=0.8519\n",
            "    batch 2300/3415  loss=0.7897\n",
            "    batch 2350/3415  loss=0.6762\n",
            "    batch 2400/3415  loss=0.3796\n",
            "    batch 2450/3415  loss=0.3495\n",
            "    batch 2500/3415  loss=0.1452\n",
            "    batch 2550/3415  loss=0.3318\n",
            "    batch 2600/3415  loss=0.2520\n",
            "    batch 2650/3415  loss=0.1902\n",
            "    batch 2700/3415  loss=1.3022\n",
            "    batch 2750/3415  loss=0.6142\n",
            "    batch 2800/3415  loss=0.5277\n",
            "    batch 2850/3415  loss=0.6742\n",
            "    batch 2900/3415  loss=0.5517\n",
            "    batch 2950/3415  loss=0.4407\n",
            "    batch 3000/3415  loss=0.6487\n",
            "    batch 3050/3415  loss=1.0912\n",
            "    batch 3100/3415  loss=0.3041\n",
            "    batch 3150/3415  loss=0.8928\n",
            "    batch 3200/3415  loss=0.3813\n",
            "    batch 3250/3415  loss=0.3400\n",
            "    batch 3300/3415  loss=0.8652\n",
            "    batch 3350/3415  loss=0.2007\n",
            "    batch 3400/3415  loss=0.8984\n",
            "  train_loss=0.6159  val_loss=2.7683  val_CER=0.5349  val_WER=0.8608\n",
            "\n",
            "Epoch 24:\n",
            "    batch 0/3415  loss=0.2251\n",
            "    batch 50/3415  loss=0.3178\n",
            "    batch 100/3415  loss=0.3209\n",
            "    batch 150/3415  loss=0.4538\n",
            "    batch 200/3415  loss=0.6488\n",
            "    batch 250/3415  loss=0.1524\n",
            "    batch 300/3415  loss=0.8111\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3564476473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D2_synth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_ROOTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"D2_synth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-490534452.py\u001b[0m in \u001b[0;36mtrain_experiment\u001b[0;34m(exp_name, base_dir)\u001b[0m\n\u001b[1;32m     52\u001b[0m             )\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             loss = F.ctc_loss(\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   3069\u001b[0m             \u001b[0mzero_infinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_infinity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m         )\n\u001b[0;32m-> 3071\u001b[0;31m     return torch.ctc_loss(\n\u001b[0m\u001b[1;32m   3072\u001b[0m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 1) Paths, device, constants\n",
        "# ============================\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "ROOT      = Path(\"/content/drive/MyDrive/EECE693_Project\")\n",
        "CKPT_DIR  = ROOT / \"SwinCTC_Checkpoints\"\n",
        "TEST_ROOT = ROOT / \"HICMA_Plus_Synthetic\"   # base dir for test set\n",
        "\n",
        "print(\"Checkpoint dir:\", CKPT_DIR)\n",
        "print(\"Test base dir :\", TEST_ROOT)\n",
        "\n",
        "TARGET_HEIGHT      = 256\n",
        "PAD_DIVISOR        = 32\n",
        "MAX_WIDTH_RESIZED  = 1600\n",
        "\n",
        "# transform for eval\n",
        "resize_pad = ResizePadTo256(\n",
        "    target_height=TARGET_HEIGHT,\n",
        "    pad_divisor=PAD_DIVISOR,\n",
        "    max_width_resized=MAX_WIDTH_RESIZED,\n",
        ")\n",
        "eval_transform = resize_pad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENx17beb4Kls",
        "outputId": "dc705bbb-0c5c-4bcf-b82e-6d29feb6605a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Checkpoint dir: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints\n",
            "Test base dir : /content/drive/MyDrive/EECE693_Project/HICMA_Plus_Synthetic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2) Load model + TextEncoder from checkpoint\n",
        "#    (handles lazy self.proj creation)\n",
        "# ==========================================\n",
        "\n",
        "def load_model_and_encoder(ckpt_path: Path):\n",
        "    print(f\"\\nLoading checkpoint from: {ckpt_path}\")\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    chars      = ckpt[\"chars\"]\n",
        "    vocab_size = ckpt[\"vocab_size\"]\n",
        "    print(\"  vocab_size in ckpt:\", vocab_size)\n",
        "    print(\"  num chars in ckpt :\", len(chars))\n",
        "\n",
        "    BLANK_IDX = 0\n",
        "    stoi = {ch: i + 1 for i, ch in enumerate(chars)}\n",
        "    itos = {i + 1: ch for i, ch in enumerate(chars)}\n",
        "    text_encoder = TextEncoder(stoi, itos, blank_idx=BLANK_IDX)\n",
        "\n",
        "    # build model\n",
        "    model = CNNSwinCTC(vocab_size=vocab_size).to(device)\n",
        "\n",
        "    # dummy forward so that self.proj is created before loading weights\n",
        "    with torch.no_grad():\n",
        "        dummy = torch.zeros(1, 1, TARGET_HEIGHT, 512, device=device)\n",
        "        _ = model(dummy)\n",
        "\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.eval()\n",
        "    return model, text_encoder\n"
      ],
      "metadata": {
        "id": "KxQLIlmg5kHq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# 3) Build test loader for a given encoder\n",
        "# ======================================\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def make_test_loader(text_encoder: TextEncoder):\n",
        "    test_ds = HICMADataset(TEST_ROOT, \"test\", eval_transform, text_encoder)\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        collate_fn=ctc_collate,\n",
        "    )\n",
        "    print(\"  Test set size:\", len(test_ds))\n",
        "    return test_ds, test_loader\n"
      ],
      "metadata": {
        "id": "AjTtxLp_4Mqq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 4) Evaluate one checkpoint on the shared test set\n",
        "# ===============================================\n",
        "\n",
        "def evaluate_checkpoint_on_test(exp_name: str):\n",
        "    ckpt_path = CKPT_DIR / f\"{exp_name}_best.pt\"\n",
        "    model, text_encoder = load_model_and_encoder(ckpt_path)\n",
        "\n",
        "    test_ds, test_loader = make_test_loader(text_encoder)\n",
        "    test_loss, test_cer, test_wer = evaluate(model, test_loader, text_encoder)\n",
        "    print(f\"[{exp_name}] TEST_loss={test_loss:.4f}  \"\n",
        "          f\"TEST_CER={test_cer:.4f}  TEST_WER={test_wer:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"exp\": exp_name,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_cer\": test_cer,\n",
        "        \"test_wer\": test_wer,\n",
        "        \"model\": model,\n",
        "        \"text_encoder\": text_encoder,\n",
        "        \"test_ds\": test_ds,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "YvXIOkkw4O3-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 5) Run evaluation for all three checkpoints\n",
        "# ===============================================\n",
        "\n",
        "exps = [\"D0_preprocessed\", \"D1_augmented\", \"D2_synth\"]\n",
        "\n",
        "results_raw = []\n",
        "models_cache = {}\n",
        "\n",
        "for exp_name in exps:\n",
        "    res = evaluate_checkpoint_on_test(exp_name)\n",
        "    models_cache[exp_name] = {\n",
        "        \"model\": res.pop(\"model\"),\n",
        "        \"text_encoder\": res.pop(\"text_encoder\"),\n",
        "        \"test_ds\": res.pop(\"test_ds\"),\n",
        "    }\n",
        "    results_raw.append(res)\n",
        "\n",
        "results_test_df = pd.DataFrame(results_raw)\n",
        "results_test_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "ivHoFa7H6kZn",
        "outputId": "63c772a6-77dc-499e-ed04-1be4d674fb9e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading checkpoint from: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "  vocab_size in ckpt: 71\n",
            "  num chars in ckpt : 70\n",
            "  Test set size: 1418\n",
            "[D0_preprocessed] TEST_loss=4.7860  TEST_CER=0.7314  TEST_WER=1.2028\n",
            "\n",
            "Loading checkpoint from: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D1_augmented_best.pt\n",
            "  vocab_size in ckpt: 71\n",
            "  num chars in ckpt : 70\n",
            "  Test set size: 1418\n",
            "[D1_augmented] TEST_loss=5.4834  TEST_CER=0.7221  TEST_WER=1.0727\n",
            "\n",
            "Loading checkpoint from: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D2_synth_best.pt\n",
            "  vocab_size in ckpt: 71\n",
            "  num chars in ckpt : 70\n",
            "  Test set size: 1418\n",
            "[D2_synth] TEST_loss=2.5905  TEST_CER=0.5323  TEST_WER=0.8554\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               exp  test_loss  test_cer  test_wer\n",
              "0  D0_preprocessed   4.785981  0.731380  1.202804\n",
              "1     D1_augmented   5.483359  0.722073  1.072711\n",
              "2         D2_synth   2.590474  0.532336  0.855398"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-692444d8-219c-48cc-bf0e-6b94e33223e7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exp</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_cer</th>\n",
              "      <th>test_wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>D0_preprocessed</td>\n",
              "      <td>4.785981</td>\n",
              "      <td>0.731380</td>\n",
              "      <td>1.202804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D1_augmented</td>\n",
              "      <td>5.483359</td>\n",
              "      <td>0.722073</td>\n",
              "      <td>1.072711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>D2_synth</td>\n",
              "      <td>2.590474</td>\n",
              "      <td>0.532336</td>\n",
              "      <td>0.855398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-692444d8-219c-48cc-bf0e-6b94e33223e7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-692444d8-219c-48cc-bf0e-6b94e33223e7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-692444d8-219c-48cc-bf0e-6b94e33223e7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-28d94c15-af39-4314-ac36-a691eeee3c12\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-28d94c15-af39-4314-ac36-a691eeee3c12')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-28d94c15-af39-4314-ac36-a691eeee3c12 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_fa96d6f4-f51e-4bb9-8122-eff93847e245\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fa96d6f4-f51e-4bb9-8122-eff93847e245 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_test_df",
              "summary": "{\n  \"name\": \"results_test_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"exp\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"D0_preprocessed\",\n          \"D1_augmented\",\n          \"D2_synth\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5097115190335428,\n        \"min\": 2.590473679124216,\n        \"max\": 5.483359064807004,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4.785980602245573,\n          5.483359064807004,\n          2.590473679124216\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_cer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11232770593167317,\n        \"min\": 0.5323361866756025,\n        \"max\": 0.7313796569896731,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7313796569896731,\n          0.7220732933256161,\n          0.5323361866756025\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_wer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17551808333612626,\n        \"min\": 0.8553979834412657,\n        \"max\": 1.202803508484302,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.202803508484302,\n          1.0727108779408148,\n          0.8553979834412657\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}