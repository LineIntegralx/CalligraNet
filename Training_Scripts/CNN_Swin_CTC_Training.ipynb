{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN0IZWp2Zn9yoQYygTWRABP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LineIntegralx/CalligraNet/blob/main/Training_Scripts/CNN_Swin_CTC_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "2DuThtmRb3ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a989200a-132d-4154-8b44-ed543c8a0e70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tPiSrUUkJoMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819c5e36-b546-42fa-afe2-60ec6bba9109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Checkpoint dir: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints\n"
          ]
        }
      ],
      "source": [
        "!pip install -q timm\n",
        "\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import timm\n",
        "\n",
        "# ----------------- CONFIG -----------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "ROOT = Path(\"/content/drive/MyDrive/EECE693_Project\")\n",
        "\n",
        "DATA_ROOTS = {\n",
        "    \"D0_preprocessed\": ROOT / \"Preprocessed_HICMA\",\n",
        "    \"D1_augmented\":    ROOT / \"Augmented_HICMA\",\n",
        "    \"D2_synth\":        ROOT / \"HICMA_Plus_Synthetic\",\n",
        "}\n",
        "\n",
        "CKPT_DIR = ROOT / \"SwinCTC_Checkpoints\"\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Checkpoint dir:\", CKPT_DIR)\n",
        "\n",
        "BATCH_SIZE    = 8\n",
        "NUM_EPOCHS    = 40\n",
        "FREEZE_EPOCHS = 5          # epochs with Swin frozen\n",
        "LR_MAIN       = 3e-4       # CNN stem + CTC head\n",
        "LR_SWIN       = 1e-4       # Swin fine-tuning\n",
        "WEIGHT_DECAY  = 1e-2\n",
        "PATIENCE      = 7          # early stopping on val CER\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split_dfs(base_dir: Path):\n",
        "    dfs = {}\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        dfs[split] = pd.read_csv(base_dir / f\"{split}_labels.csv\")\n",
        "    return dfs\n",
        "\n",
        "for name, base in DATA_ROOTS.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    dfs = load_split_dfs(base)\n",
        "    for split, df in dfs.items():\n",
        "        df[\"label\"] = df[\"label\"].astype(str)\n",
        "        print(f\"{split}: {len(df)} rows, classes={df['class'].value_counts().to_dict()}\")\n",
        "        print(f\"  avg label len = {df['label'].str.len().mean():.1f}\")\n",
        "\n",
        "# ---- build char vocab from richest dataset (D2_synth) ----\n",
        "def build_vocab_from_dataset(base_dir: Path):\n",
        "    labels = []\n",
        "    for split in [\"train\", \"val\", \"test\"]:\n",
        "        df = pd.read_csv(base_dir / f\"{split}_labels.csv\")\n",
        "        labels.extend(df[\"label\"].astype(str).tolist())\n",
        "    all_text = \"\".join(labels)\n",
        "    char_counter = Counter(all_text)\n",
        "    chars = sorted(list(char_counter.keys()))\n",
        "    print(\"\\nNum unique chars:\", len(chars))\n",
        "    return chars\n",
        "\n",
        "chars = build_vocab_from_dataset(DATA_ROOTS[\"D2_synth\"])\n",
        "\n",
        "BLANK_IDX = 0\n",
        "stoi = {ch: i + 1 for i, ch in enumerate(chars)}   # chars start at 1\n",
        "itos = {i + 1: ch for i, ch in enumerate(chars)}\n",
        "vocab_size = len(chars) + 1  # + blank\n",
        "print(\"vocab_size (including blank):\", vocab_size)\n",
        "\n",
        "class TextEncoder:\n",
        "    def __init__(self, stoi, itos, blank_idx=0):\n",
        "        self.stoi = stoi\n",
        "        self.itos = itos\n",
        "        self.blank_idx = blank_idx\n",
        "\n",
        "    def encode(self, text: str):\n",
        "        return [self.stoi[c] for c in text if c in self.stoi]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return \"\".join(self.itos[i] for i in ids if i in self.itos)\n",
        "\n",
        "text_encoder = TextEncoder(stoi, itos, BLANK_IDX)\n"
      ],
      "metadata": {
        "id": "vNgmdFxcbKAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abfc00e-3e8a-4fad-ea53-0f005d8ef8bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== D0_preprocessed ===\n",
            "train: 4020 rows, classes={'Naskh': 2988, 'Thuluth': 808, 'Diwani': 190, 'Kufic': 21, 'Muhaquaq': 13}\n",
            "  avg label len = 42.5\n",
            "val: 502 rows, classes={'Naskh': 373, 'Thuluth': 101, 'Diwani': 23, 'Kufic': 3, 'Muhaquaq': 2}\n",
            "  avg label len = 41.6\n",
            "test: 503 rows, classes={'Naskh': 374, 'Thuluth': 101, 'Diwani': 24, 'Muhaquaq': 2, 'Kufic': 2}\n",
            "  avg label len = 44.5\n",
            "\n",
            "=== D1_augmented ===\n",
            "train: 20000 rows, classes={'Naskh': 4000, 'Thuluth': 4000, 'Diwani': 4000, 'Muhaquaq': 4000, 'Kufic': 4000}\n",
            "  avg label len = 34.4\n",
            "val: 502 rows, classes={'Naskh': 373, 'Thuluth': 101, 'Diwani': 23, 'Kufic': 3, 'Muhaquaq': 2}\n",
            "  avg label len = 41.6\n",
            "test: 503 rows, classes={'Naskh': 374, 'Thuluth': 101, 'Diwani': 24, 'Muhaquaq': 2, 'Kufic': 2}\n",
            "  avg label len = 44.5\n",
            "\n",
            "=== D2_synth ===\n",
            "train: 27316 rows, classes={'Kufic': 6923, 'Diwani': 6296, 'Thuluth': 6097, 'Naskh': 4000, 'Muhaquaq': 4000}\n",
            "  avg label len = 37.6\n",
            "val: 1416 rows, classes={'Naskh': 373, 'Kufic': 368, 'Thuluth': 363, 'Diwani': 310, 'Muhaquaq': 2}\n",
            "  avg label len = 44.7\n",
            "test: 1418 rows, classes={'Naskh': 374, 'Kufic': 368, 'Thuluth': 363, 'Diwani': 311, 'Muhaquaq': 2}\n",
            "  avg label len = 46.3\n",
            "\n",
            "Num unique chars: 70\n",
            "vocab_size (including blank): 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# No geometric transforms â€“ your preprocessing already did letterbox+padding.\n",
        "train_transform = T.Compose([\n",
        "    T.ToTensor(),                      # [1, H, W] in [0,1]\n",
        "    T.Normalize(mean=[0.5], std=[0.5]) # ~[-1,1]\n",
        "])\n",
        "\n",
        "eval_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "class HICMADataset(Dataset):\n",
        "    def __init__(self, base_dir: Path, split: str, transform, text_encoder: TextEncoder):\n",
        "        self.base_dir = base_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.text_encoder = text_encoder\n",
        "\n",
        "        df = pd.read_csv(self.base_dir / f\"{split}_labels.csv\")\n",
        "        self.df = df[[\"img_name\", \"class\", \"label\"]].copy()\n",
        "        self.df[\"label\"] = self.df[\"label\"].astype(str)\n",
        "\n",
        "        self.img_dir = self.base_dir / split / \"images\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_name = row[\"img_name\"]\n",
        "        cls      = row[\"class\"]\n",
        "        text     = row[\"label\"]\n",
        "\n",
        "        img_path = self.img_dir / img_name\n",
        "        image = Image.open(img_path).convert(\"L\")   # grayscale\n",
        "        image = self.transform(image)               # [1, H, W]\n",
        "\n",
        "        target = torch.tensor(self.text_encoder.encode(text), dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"target\": target,\n",
        "            \"text\": text,\n",
        "            \"class\": cls,\n",
        "            \"img_name\": img_name,\n",
        "        }\n",
        "\n",
        "def ctc_collate(batch):\n",
        "    images  = [b[\"image\"] for b in batch]    # each: [1, H_i, W_i]\n",
        "    targets = [b[\"target\"] for b in batch]\n",
        "\n",
        "    batch_size = len(images)\n",
        "    C = images[0].shape[0]\n",
        "    heights = [img.shape[1] for img in images]\n",
        "    widths  = [img.shape[2] for img in images]\n",
        "\n",
        "    max_h = max(heights)\n",
        "    max_w = max(widths)\n",
        "\n",
        "    # --- make them divisible by 4 for Swin patch size ---\n",
        "    def round_up(x, m=4):\n",
        "        return ((x + m - 1) // m) * m\n",
        "\n",
        "    max_h = round_up(max_h, 4)\n",
        "    max_w = round_up(max_w, 4)\n",
        "    # ----------------------------------------------------\n",
        "\n",
        "    pad_val = 1.0  # white in normalized space\n",
        "    padded = torch.full((batch_size, C, max_h, max_w),\n",
        "                        pad_val, dtype=images[0].dtype)\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        h, w = img.shape[1], img.shape[2]\n",
        "        padded[i, :, :h, :w] = img  # top-left placement\n",
        "\n",
        "    target_lengths = torch.tensor([t.size(0) for t in targets], dtype=torch.long)\n",
        "    targets_concat = torch.cat(targets, dim=0)\n",
        "\n",
        "    meta = {\n",
        "        \"texts\": [b[\"text\"] for b in batch],\n",
        "        \"img_names\": [b[\"img_name\"] for b in batch],\n",
        "        \"classes\": [b[\"class\"] for b in batch],\n",
        "        \"widths\": widths,\n",
        "        \"heights\": heights,\n",
        "    }\n",
        "\n",
        "    return padded, targets_concat, target_lengths, meta\n",
        "\n",
        "\n",
        "# Quick sanity check\n",
        "for name, base in DATA_ROOTS.items():\n",
        "    ds = HICMADataset(base, \"train\", train_transform, text_encoder)\n",
        "    print(f\"{name} train size:\", len(ds))\n",
        "    sample = ds[0]\n",
        "    print(f\"  one image shape: {sample['image'].shape}, text len={len(sample['text'])}\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "26TsucGGbNCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b201ff94-e9d6-4300-c855-90141c870b0e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D0_preprocessed train size: 4020\n",
            "  one image shape: torch.Size([1, 256, 4000]), text len=50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNSwinCTC(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN stem: downsample width only, keep height\n",
        "        self.cnn_stem = nn.Sequential(\n",
        "            # 1 x H x W -> 32 x H x (W/2)\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=(1, 2), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # 32 x H x W/2 -> 64 x H x (W/4)\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=(1, 2), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # 64 x H x W/4 -> 3 x H x (W/8)\n",
        "            nn.Conv2d(64, 3, kernel_size=3, stride=(1, 2), padding=1),\n",
        "            nn.BatchNorm2d(3),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        # Pretrained Swin-T backbone\n",
        "        self.swin = timm.create_model(\n",
        "            \"swin_tiny_patch4_window7_224\",\n",
        "            pretrained=True,\n",
        "            features_only=True,\n",
        "            out_indices=[-1],\n",
        "            in_chans=3,\n",
        "            img_size=256,          # our typical height\n",
        "            strict_img_size=False  # allow non-224 sizes\n",
        "        )\n",
        "        swin_out_ch = self.swin.feature_info[-1][\"num_chs\"]\n",
        "\n",
        "        self.proj = nn.Linear(swin_out_ch, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [B, 1, H, W]\n",
        "        returns: log_probs [T, B, C], T (int)\n",
        "        \"\"\"\n",
        "        x = self.cnn_stem(x)          # [B, 3, H, W/8]\n",
        "\n",
        "        feat = self.swin(x)[0]        # [B, Hs, Ws, Cs]  (channels-last from timm)\n",
        "        # average over height -> [B, Ws, Cs]\n",
        "        feat = feat.mean(dim=1)\n",
        "\n",
        "        # here: last dim = Cs (e.g. 768), matches swin_out_ch\n",
        "        feat = self.proj(feat)        # [B, T, hidden]\n",
        "        feat = self.dropout(feat)\n",
        "        logits = self.classifier(feat)  # [B, T, vocab_size]\n",
        "\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        T_len = logits.size(1)         # sequence length along width\n",
        "        return log_probs.permute(1, 0, 2), T_len  # [T, B, C], T\n"
      ],
      "metadata": {
        "id": "UKDezI1dbQ0w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def levenshtein(a, b):\n",
        "    \"\"\"Classic edit distance for CER/WER.\"\"\"\n",
        "    dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
        "    for i in range(len(a) + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(len(b) + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, len(a) + 1):\n",
        "        for j in range(1, len(b) + 1):\n",
        "            cost = 0 if a[i - 1] == b[j - 1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j] + 1,\n",
        "                dp[i][j-1] + 1,\n",
        "                dp[i-1][j-1] + cost,\n",
        "            )\n",
        "    return dp[-1][-1]\n",
        "\n",
        "def compute_cer(preds, gts):\n",
        "    total_dist, total_len = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        total_dist += levenshtein(p, g)\n",
        "        total_len  += len(g)\n",
        "    return total_dist / max(total_len, 1)\n",
        "\n",
        "def compute_wer(preds, gts):\n",
        "    total_dist, total_len = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        p_words = p.split()\n",
        "        g_words = g.split()\n",
        "        total_dist += levenshtein(p_words, g_words)\n",
        "        total_len  += len(g_words)\n",
        "    return total_dist / max(total_len, 1)\n",
        "\n",
        "def greedy_decode(log_probs, text_encoder: TextEncoder):\n",
        "    \"\"\"\n",
        "    log_probs: [T, B, C]\n",
        "    return: list of predicted strings\n",
        "    \"\"\"\n",
        "    max_ids = log_probs.argmax(dim=-1).transpose(0, 1)  # [B, T]\n",
        "    pred_strs = []\n",
        "    for seq in max_ids:\n",
        "        prev = BLANK_IDX\n",
        "        ids = []\n",
        "        for i in seq.tolist():\n",
        "            if i != prev and i != BLANK_IDX:\n",
        "                ids.append(i)\n",
        "            prev = i\n",
        "        pred_strs.append(text_encoder.decode(ids))\n",
        "    return pred_strs\n",
        "\n",
        "def evaluate(model, loader, text_encoder: TextEncoder):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds, all_gts = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, target_lengths, meta in loader:\n",
        "            images  = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            log_probs, T_len = model(images)  # [T, B, C]\n",
        "            input_lengths = torch.full(\n",
        "                (images.size(0),), T_len, dtype=torch.long, device=device\n",
        "            )\n",
        "\n",
        "            loss = F.ctc_loss(\n",
        "                log_probs, targets,\n",
        "                input_lengths, target_lengths,\n",
        "                blank=BLANK_IDX, zero_infinity=True\n",
        "            )\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            pred_strs = greedy_decode(log_probs.cpu(), text_encoder)\n",
        "            all_preds.extend(pred_strs)\n",
        "            all_gts.extend(meta[\"texts\"])\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    cer = compute_cer(all_preds, all_gts)\n",
        "    wer = compute_wer(all_preds, all_gts)\n",
        "    return avg_loss, cer, wer\n"
      ],
      "metadata": {
        "id": "JxR7sYgfbgib"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def make_dataloaders_for_experiment(base_dir: Path, text_encoder: TextEncoder):\n",
        "    \"\"\"Create train/val/test datasets + dataloaders for one experiment.\"\"\"\n",
        "    train_ds = HICMADataset(base_dir, \"train\", train_transform, text_encoder)\n",
        "    val_ds   = HICMADataset(base_dir, \"val\",   eval_transform,  text_encoder)\n",
        "    # Fixed test set from *original* HICMA (D0)\n",
        "    test_ds  = HICMADataset(DATA_ROOTS[\"D0_preprocessed\"], \"test\", eval_transform, text_encoder)\n",
        "\n",
        "    # num_workers=0 is safer with Drive in Colab\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=0, pin_memory=True, collate_fn=ctc_collate\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=0, pin_memory=True, collate_fn=ctc_collate\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=0, pin_memory=True, collate_fn=ctc_collate\n",
        "    )\n",
        "    return train_ds, val_ds, test_ds, train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "def train_experiment(exp_name: str, base_dir: Path):\n",
        "    print(f\"\\n\\n########## {exp_name} on {base_dir.name} ##########\")\n",
        "\n",
        "    print(\"  -> Building dataloaders...\")\n",
        "    train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = \\\n",
        "        make_dataloaders_for_experiment(base_dir, text_encoder)\n",
        "    print(\"  -> Dataloaders ready. Building model...\")\n",
        "\n",
        "    model = CNNSwinCTC(vocab_size=vocab_size).to(device)\n",
        "    print(\"  -> Model ready. Starting training...\")\n",
        "\n",
        "    # Stage 1: freeze Swin (only stem + head train at first)\n",
        "    for p in model.swin.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Optimizer with param groups: main vs Swin\n",
        "    main_params = [p for n, p in model.named_parameters() if not n.startswith(\"swin.\")]\n",
        "    swin_params = [p for n, p in model.named_parameters() if n.startswith(\"swin.\")]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [\n",
        "            {\"params\": main_params, \"lr\": LR_MAIN},\n",
        "            {\"params\": swin_params, \"lr\": LR_SWIN},\n",
        "        ],\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    best_val_cer = float(\"inf\")\n",
        "    best_val_wer = None\n",
        "    best_epoch   = -1\n",
        "    best_state   = None\n",
        "    patience_left = PATIENCE\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        # Unfreeze Swin after FREEZE_EPOCHS\n",
        "        if epoch == FREEZE_EPOCHS + 1:\n",
        "            print(\">> Unfreezing Swin backbone for fine-tuning.\")\n",
        "            for p in model.swin.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, targets, target_lengths, meta in train_loader:\n",
        "            images  = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            log_probs, T_len = model(images)\n",
        "            input_lengths = torch.full(\n",
        "                (images.size(0),), T_len, dtype=torch.long, device=device\n",
        "            )\n",
        "\n",
        "            loss = F.ctc_loss(\n",
        "                log_probs, targets,\n",
        "                input_lengths, target_lengths,\n",
        "                blank=BLANK_IDX, zero_infinity=True\n",
        "            )\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_ds)\n",
        "        val_loss, val_cer, val_wer = evaluate(model, val_loader, text_encoder)\n",
        "        print(f\"Epoch {epoch:02d}: \"\n",
        "              f\"train_loss={train_loss:.4f}  \"\n",
        "              f\"val_loss={val_loss:.4f}  \"\n",
        "              f\"val_CER={val_cer:.4f}  \"\n",
        "              f\"val_WER={val_wer:.4f}\")\n",
        "\n",
        "        # Early stopping on CER\n",
        "        if val_cer < best_val_cer:\n",
        "            best_val_cer = val_cer\n",
        "            best_val_wer = val_wer\n",
        "            best_epoch   = epoch\n",
        "            best_state   = model.state_dict()\n",
        "            patience_left = PATIENCE\n",
        "\n",
        "            ckpt_path = CKPT_DIR / f\"{exp_name}_best.pt\"\n",
        "            torch.save({\n",
        "                \"model_state\": best_state,\n",
        "                \"epoch\": best_epoch,\n",
        "                \"val_cer\": best_val_cer,\n",
        "                \"val_wer\": best_val_wer,\n",
        "                \"vocab_size\": vocab_size,\n",
        "                \"chars\": chars,\n",
        "            }, ckpt_path)\n",
        "            print(f\"  >> New best model saved to: {ckpt_path}\")\n",
        "        else:\n",
        "            patience_left -= 1\n",
        "            if patience_left <= 0:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # Load best model and evaluate on test set\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    test_loss, test_cer, test_wer = evaluate(model, test_loader, text_encoder)\n",
        "    print(f\"[{exp_name}] BEST epoch={best_epoch}  \"\n",
        "          f\"val_CER={best_val_cer:.4f}  val_WER={best_val_wer:.4f}  \"\n",
        "          f\"TEST_CER={test_cer:.4f}  TEST_WER={test_wer:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"exp\": exp_name,\n",
        "        \"train_size\": len(train_ds),\n",
        "        \"val_size\": len(val_ds),\n",
        "        \"test_size\": len(test_ds),\n",
        "        \"best_epoch\": best_epoch,\n",
        "        \"best_val_cer\": best_val_cer,\n",
        "        \"best_val_wer\": best_val_wer,\n",
        "        \"test_cer\": test_cer,\n",
        "        \"test_wer\": test_wer,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "1j4Xm2pkbnaC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for exp_name, base in DATA_ROOTS.items():\n",
        "    res = train_experiment(exp_name, base)\n",
        "    results.append(res)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "s-vTzXgObpol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "abdd8d61-61bc-48cc-87cf-828b72b8d1c1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "########## D0_preprocessed on Preprocessed_HICMA ##########\n",
            "  -> Building dataloaders...\n",
            "  -> Dataloaders ready. Building model...\n",
            "  -> Model ready. Starting training...\n",
            "Epoch 01: train_loss=0.1135  val_loss=0.0822  val_CER=0.9148  val_WER=1.0000\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "Epoch 02: train_loss=0.0938  val_loss=0.0803  val_CER=0.9226  val_WER=0.9988\n",
            "Epoch 03: train_loss=0.0910  val_loss=0.0853  val_CER=0.9188  val_WER=1.0000\n",
            "Epoch 04: train_loss=0.0798  val_loss=0.0774  val_CER=0.8959  val_WER=0.9993\n",
            "  >> New best model saved to: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "Epoch 05: train_loss=0.0846  val_loss=0.0831  val_CER=0.9117  val_WER=0.9995\n",
            ">> Unfreezing Swin backbone for fine-tuning.\n",
            "Epoch 06: train_loss=0.1016  val_loss=0.1045  val_CER=0.9472  val_WER=0.9979\n",
            "Epoch 07: train_loss=0.0884  val_loss=0.0768  val_CER=0.9655  val_WER=0.9990\n",
            "Epoch 08: train_loss=0.0776  val_loss=0.0783  val_CER=0.9681  val_WER=0.9993\n",
            "Epoch 09: train_loss=0.0830  val_loss=0.0730  val_CER=0.9155  val_WER=0.9979\n",
            "Epoch 10: train_loss=0.0736  val_loss=0.0745  val_CER=0.9254  val_WER=0.9955\n",
            "Epoch 11: train_loss=0.0723  val_loss=0.0889  val_CER=0.9741  val_WER=1.0000\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1250270388.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATA_ROOTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1263767346.py\u001b[0m in \u001b[0;36mtrain_experiment\u001b[0;34m(exp_name, base_dir)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_wer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     print(f\"[{exp_name}] BEST epoch={best_epoch}  \"\n\u001b[1;32m    128\u001b[0m           \u001b[0;34mf\"val_CER={best_val_cer:.4f}  val_WER={best_val_wer:.4f}  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4157530655.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, text_encoder)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mimages\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3907009858.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# [1, H, W]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3557\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3559\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3561\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mID\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3545\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3546\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3547\u001b[0;31m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3548\u001b[0m                     \u001b[0m_decompression_bomb_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3549\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodermaxblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAXBLOCK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;31m# filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/_util.py\u001b[0m in \u001b[0;36mis_path\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTypeGuard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStrOrBytesPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "exp_name = \"D2_synth\"\n",
        "base_dir = DATA_ROOTS[\"D2_synth\"]\n",
        "\n",
        "res = train_experiment(exp_name, base_dir)\n",
        "results.append(res)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "4XFK3xXFm7lS",
        "outputId": "c7e3586a-6106-4eb4-a474-fb3d9116f746"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "########## D2_synth on HICMA_Plus_Synthetic ##########\n",
            "  -> Building dataloaders...\n",
            "  -> Dataloaders ready. Building model...\n",
            "  -> Model ready. Starting training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Input width (217) should be divisible by patch size (4).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3056535648.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_ROOTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"D2_synth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1263767346.py\u001b[0m in \u001b[0;36mtrain_experiment\u001b[0;34m(exp_name, base_dir)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             input_lengths = torch.full(\n\u001b[1;32m     77\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-293296372.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# [B, 3, H, W/8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# [B, Hs, Ws, Cs]  (channels-last from timm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# average over height -> [B, Ws, Cs]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/_features.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/_features.py\u001b[0m in \u001b[0;36m_collect\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfirst_or_last_module\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/layers/patch_embed.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;34mf\"Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 )\n\u001b[0;32m--> 128\u001b[0;31m                 _assert(\n\u001b[0m\u001b[1;32m    129\u001b[0m                     \u001b[0mW\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0;34mf\"Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   2183\u001b[0m             \u001b[0m_assert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         )\n\u001b[0;32m-> 2185\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Input width (217) should be divisible by patch size (4)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build loaders just for D0_preprocessed\n",
        "base_dir = DATA_ROOTS[\"D0_preprocessed\"]\n",
        "\n",
        "train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = \\\n",
        "    make_dataloaders_for_experiment(base_dir, text_encoder)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjvUYPFrl19u",
        "outputId": "42767407-5e95-4e0c-816e-f603beb83b74"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4020, 502, 503)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = CKPT_DIR / \"D0_preprocessed_best.pt\"\n",
        "print(\"Loading checkpoint from:\", ckpt_path)\n",
        "\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "print(ckpt.keys())  # should show: dict_keys(['model_state', 'epoch', 'val_cer', ...])\n",
        "\n",
        "model = CNNSwinCTC(vocab_size=vocab_size).to(device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4AjNdZll4PQ",
        "outputId": "4c282638-7b09-443a-dd2c-e5d1d95ca7da"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint from: /content/drive/MyDrive/EECE693_Project/SwinCTC_Checkpoints/D0_preprocessed_best.pt\n",
            "dict_keys(['model_state', 'epoch', 'val_cer', 'val_wer', 'vocab_size', 'chars'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNSwinCTC(\n",
              "  (cnn_stem): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): GELU(approximate='none')\n",
              "    (6): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 2), padding=(1, 1))\n",
              "    (7): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): GELU(approximate='none')\n",
              "  )\n",
              "  (swin): FeatureListNet(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
              "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (layers_0): SwinTransformerStage(\n",
              "      (downsample): Identity()\n",
              "      (blocks): Sequential(\n",
              "        (0): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.009)\n",
              "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.009)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layers_1): SwinTransformerStage(\n",
              "      (downsample): PatchMerging(\n",
              "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.018)\n",
              "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.018)\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.027)\n",
              "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.027)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layers_2): SwinTransformerStage(\n",
              "      (downsample): PatchMerging(\n",
              "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.036)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.036)\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.045)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.045)\n",
              "        )\n",
              "        (2): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.055)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.055)\n",
              "        )\n",
              "        (3): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.064)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.064)\n",
              "        )\n",
              "        (4): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.073)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.073)\n",
              "        )\n",
              "        (5): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.082)\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.082)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layers_3): SwinTransformerStage(\n",
              "      (downsample): PatchMerging(\n",
              "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.091)\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.091)\n",
              "        )\n",
              "        (1): SwinTransformerBlock(\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): WindowAttention(\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (drop_path1): DropPath(drop_prob=0.100)\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path2): DropPath(drop_prob=0.100)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (proj): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=256, out_features=71, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_cer, val_wer = evaluate(model, val_loader, text_encoder)\n",
        "test_loss, test_cer, test_wer = evaluate(model, test_loader, text_encoder)\n",
        "\n",
        "print(f\"VAL  -> loss={val_loss:.4f}, CER={val_cer:.4f}, WER={val_wer:.4f}\")\n",
        "print(f\"TEST -> loss={test_loss:.4f}, CER={test_cer:.4f}, WER={test_wer:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJuJJoXIl6IP",
        "outputId": "77fbad72-1ec7-41ce-e341-5d055ac40782"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL  -> loss=0.0774, CER=0.8959, WER=0.9993\n",
            "TEST -> loss=0.0414, CER=0.9000, WER=0.9996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_predictions(model, loader, text_encoder, num_batches=2):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for b_idx, (images, targets, target_lengths, meta) in enumerate(loader):\n",
        "            images = images.to(device)\n",
        "            log_probs, T_len = model(images)     # [T, B, C]\n",
        "            preds = greedy_decode(log_probs.cpu(), text_encoder)\n",
        "\n",
        "            for gt, pred, img_name in zip(meta[\"texts\"], preds, meta[\"img_names\"]):\n",
        "                print(f\"IMG  : {img_name}\")\n",
        "                print(f\"GT   : {gt}\")\n",
        "                print(f\"PRED : {pred}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            if b_idx + 1 >= num_batches:\n",
        "                break\n",
        "\n",
        "# Show a few from validation\n",
        "show_sample_predictions(model, val_loader, text_encoder, num_batches=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1WA7sb6l9HF",
        "outputId": "2c172b34-2ece-4c23-dc72-8600cb61592f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG  : 177-3.png\n",
            "GT   : Ù…Ù† Ø¹Ø¨Ø§Ø¯Ùƒ Ùˆ Ø§Ø´Ø±Ù Ø§Ù„Ù…Ù†Ø§Ø¯ÙŠÙ† Ù„Ø·Ø±Ù‚ Ø±Ø´Ø§Ø¯Ùƒ\n",
            "PRED : Ø¹Ø§Ù„Ù„Ù‡\n",
            "--------------------------------------------------\n",
            "IMG  : Al-Anam-691-300x57_segment_no_0.png\n",
            "GT   : Ù‚Ù„ Ø§Ù„Ù„Ù‡ Ø«Ù… Ø°Ø±Ù‡Ù… ÙÙŠ Ø®ÙˆØ¶Ù‡Ù… ÙŠÙ„Ø¹Ø¨ÙˆÙ†\n",
            "PRED : Ø¹Ù„Ù‚Ù„Ù„Ù„Ù‡\n",
            "--------------------------------------------------\n",
            "IMG  : 175-4.png\n",
            "GT   : Ø§ÙØ¶Ù„ Ù‚Ø§Ø¦Ù… Ø¨Ø­Ù‚Ùƒ Ø§Ù„Ù…Ø¨ØºÙˆØ« Ø¨ØªÙŠØ³ÙŠØ±Ùƒ Ùˆ Ø±ÙÙ‚Ùƒ\n",
            "PRED : Ø¹Ù…Ø¹Ù…Ø³Ù„Ù„Ù‡\n",
            "--------------------------------------------------\n",
            "IMG  : 566-3.png\n",
            "GT   : ÙˆÙ„ÙƒÙ† Ø£ÙƒØ«Ø±Ù‡Ù… Ù„Ø§ ÙŠØ¹Ù„Ù…ÙˆÙ† ÙˆÙ…Ø§ Ù…Ù† Ø¯Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø£Ø±Ø¶ ÙˆÙ„Ø§\n",
            "PRED : ÙŠÙ„Ù…Ù„Ù…Ù„Ù…Ù„\n",
            "--------------------------------------------------\n",
            "IMG  : Fatiha-2-Round-White_segment_no_0 - Copy (5).png\n",
            "GT   : Ø§Ù‡Ø¯Ù†Ø§ Ø§Ù„ØµØ±Ø§Ø· Ø§Ù„Ù…Ø³ØªÙ‚ÙŠÙ… ØµØ±Ø§Ø· Ø§Ù„Ø°ÙŠÙ†\n",
            "PRED : Ø¹Ø§Ù„Ù„Ù‡\n",
            "--------------------------------------------------\n",
            "IMG  : 105-3.png\n",
            "GT   : Ù…Ù†ÙŠØ¹ Ùˆ Ø­Ø±Ø² Ø­ØµÙŠÙ† Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø®Ù„Ù‚Ùƒ Ø­ØªÙ‰ ØªØ¨Ù„ØºÙ†ÙŠ\n",
            "PRED : Ø¹ÙŠÙ„Ù„Ù‡\n",
            "--------------------------------------------------\n",
            "IMG  : 50-16b_segment_no_0.png\n",
            "GT   : Ùˆ Ù†Ø­Ù† Ø£Ù‚Ø±Ø¨ Ø¥Ù„ÙŠÙ‡ Ù…Ù† Ø­Ø¨Ù„ Ø§Ù„ÙˆØ±ÙŠØ¯\n",
            "PRED : Ù„Ù‚ÙŠÙ„Ù„Ù„Ù‡\n",
            "--------------------------------------------------\n",
            "IMG  : 253-1.png\n",
            "GT   : Ø¹Ù„Ù‰ Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ… ÙˆØ¨Ø§Ø±Ùƒ Ø¹Ù„Ù‰ Ù…Ø­Ù…Ø¯ ÙˆØ¹Ù„Ù‰ Ø¢Ù„ Ù…Ø­Ù…Ø¯\n",
            "PRED : Ø¹Ù‡Ù„Ù„Ù‡\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}