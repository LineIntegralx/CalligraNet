# -*- coding: utf-8 -*-
"""SWIN_CTC_AraGPT2_HICMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pydWPgE7SQlqTSLfdoXXNy8BNcItSvdT

# Arabic Calligraphy OCR with SWIN + CTC + AraGPT2

**Course Project 693: Advanced Deep Learning**

---

## Project Overview

In this notebook, we're building an OCR system specifically designed for Arabic calligraphy. This is challenging because:
- Arabic script is cursive with connected characters
- Calligraphy has artistic variations and styles
- Right-to-left reading direction
- Context-dependent character shapes

### Our Approach: Hybrid Architecture

We combine three powerful components:
1. **SWIN (Swin Transformer)**: Captures visual features from calligraphy images
2. **CTC (Connectionist Temporal Classification)**: Handles variable-length sequences without explicit alignment
3. **AraGPT2**: Provides language model refinement for more accurate predictions

This architecture leverages both visual understanding and linguistic knowledge to achieve robust OCR performance.

## 1. Environment Setup

First, we need to install all the required packages. We're using PyTorch for the deep learning framework, Transformers for pre-trained models, and some specialized libraries for Arabic text processing.
"""

# Install required packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers datasets pillow opencv-python-headless
!pip install timm  # PyTorch Image Models for Swin Transformer
!pip install arabicstopwords arabic-reshaper python-bidi  # Arabic text processing
!pip install tensorboard matplotlib scikit-learn

print("✓ All packages installed successfully!")

# Import all necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

import timm  # For Swin Transformer
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config

import numpy as np
from PIL import Image
import cv2
from pathlib import Path
import json
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

"""## 2. Dataset Preparation

We need to create a custom dataset class that:
- Loads calligraphy images
- Loads corresponding Arabic text labels
- Preprocesses images (resize, normalize)
- Tokenizes text for both CTC and GPT2
"""

# First, let's mount Google Drive to access our dataset
from google.colab import drive
drive.mount('/content/drive')

TRAIN_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/train/images'
TRAIN_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/train_labels.csv'
TEST_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/test/images'
TEST_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/test_labels.csv'
VALID_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/val/images'
VALID_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/val_labels.csv'

class ArabicCharacterTokenizer:
    """
    Custom tokenizer for Arabic text at character level.
    This is used for the CTC layer which needs character-level tokens.
    """
    def __init__(self):
        # Special tokens for CTC
        self.blank_token = '<BLANK>'  # CTC blank token
        self.pad_token = '<PAD>'      # Padding token
        self.unk_token = '<UNK>'      # Unknown character
        self.sos_token = '<SOS>'      # Start of sequence
        self.eos_token = '<EOS>'      # End of sequence

        # We'll build vocabulary from the dataset
        self.char2idx = {
            self.blank_token: 0,
            self.pad_token: 1,
            self.unk_token: 2,
            self.sos_token: 3,
            self.eos_token: 4,
        }
        self.idx2char = {v: k for k, v in self.char2idx.items()}

    def build_vocab(self, texts):
        """Build vocabulary from list of Arabic texts"""
        unique_chars = set()
        for text in texts:
            unique_chars.update(text)

        # Add characters to vocabulary
        for char in sorted(unique_chars):
            if char not in self.char2idx:
                idx = len(self.char2idx)
                self.char2idx[char] = idx
                self.idx2char[idx] = char

        print(f"Vocabulary built: {len(self.char2idx)} unique characters")

    def encode(self, text):
        """Convert text to list of indices"""
        return [self.char2idx.get(char, self.char2idx[self.unk_token]) for char in text]

    def decode(self, indices):
        """Convert list of indices back to text"""
        return ''.join([self.idx2char.get(idx, self.unk_token) for idx in indices])

    def __len__(self):
        return len(self.char2idx)

# Initialize tokenizer
char_tokenizer = ArabicCharacterTokenizer()

class ArabicCalligraphyDataset(Dataset):
    """
    Custom Dataset for Arabic Calligraphy OCR with CSV labels.

    This dataset handles:
    - Loading images from disk
    - Loading corresponding text labels from CSV
    - Image preprocessing (resize, normalize)
    - Text tokenization for both CTC and GPT2
    """
    def __init__(self, img_dir, csv_path, char_tokenizer, gpt2_tokenizer,
                 img_height=64, img_width=256, max_text_len=128):
        self.img_dir = Path(img_dir)
        self.char_tokenizer = char_tokenizer
        self.gpt2_tokenizer = gpt2_tokenizer
        self.img_height = img_height
        self.img_width = img_width
        self.max_text_len = max_text_len

        # Load CSV with labels
        df = pd.read_csv(csv_path)

        # Create a mapping from img_name to label
        self.labels_dict = dict(zip(df['img_name'], df['label']))

        # Get all image files
        self.image_files = sorted(list(self.img_dir.glob('*.png')) +
                                  list(self.img_dir.glob('*.jpg')))

        # Filter to only images that have labels in the CSV
        self.image_files = [img for img in self.image_files
                           if img.name in self.labels_dict]

        print(f"Found {len(self.image_files)} images with labels in {img_dir}")

    def __len__(self):
        return len(self.image_files)

    def preprocess_image(self, image_path):
      """
      Load and preprocess the image with padding to preserve aspect ratio
      """
      # Load image in grayscale
      img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)

      # Get original dimensions
      h, w = img.shape

      # Calculate scaling to fit in target size while preserving aspect ratio
      scale = min(self.img_height / h, self.img_width / w)
      new_h, new_w = int(h * scale), int(w * scale)

      # Resize
      img = cv2.resize(img, (new_w, new_h))

      # Create a square canvas with padding (white background)
      canvas = np.ones((self.img_height, self.img_width), dtype=np.float32) * 255

      # Center the image on canvas
      y_offset = (self.img_height - new_h) // 2
      x_offset = (self.img_width - new_w) // 2
      canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = img

      # Normalize to [0, 1] and add channel dimension
      canvas = canvas.astype(np.float32) / 255.0
      canvas = np.expand_dims(canvas, axis=0)

      return torch.FloatTensor(canvas)

    def load_label(self, image_path):
        """
        Load the text label for an image from the CSV mapping.
        """
        if image_path.name in self.labels_dict:
            return self.labels_dict[image_path.name]

        print(f"Warning: Label not found for {image_path.name}")
        return ""

    def __getitem__(self, idx):
        # Load image
        img_path = self.image_files[idx]
        image = self.preprocess_image(img_path)

        # Load label text from CSV mapping
        text = self.load_label(img_path)

        # Tokenize for CTC (character-level)
        char_tokens = self.char_tokenizer.encode(text)
        char_tokens = torch.LongTensor(char_tokens)

        # Tokenize for GPT2 (subword-level)
        gpt2_tokens = self.gpt2_tokenizer.encode(text,
                                                  max_length=self.max_text_len,
                                                  truncation=True,
                                                  padding='max_length',
                                                  return_tensors='pt')
        gpt2_tokens = gpt2_tokens.squeeze(0)

        return {
            'image': image,
            'char_tokens': char_tokens,
            'gpt2_tokens': gpt2_tokens,
            'text': text,
            'text_length': len(char_tokens)
        }

"""## 3. Build Vocabulary and Initialize Tokenizers

Before we can create datasets, we need to:
1. Scan through all training labels to build character vocabulary
2. Initialize AraGPT2 tokenizer for the decoder
"""

import pandas as pd

# Load all training labels to build vocabulary
def load_all_labels(csv_path):
    """
    Load all text labels from CSV file.
    CSV has columns: img_name, class, label, source
    """
    df = pd.read_csv(csv_path)
    texts = df['label'].tolist()
    return texts

# Load all training labels from CSV to build vocabulary
train_texts = load_all_labels(TRAIN_CSV)
print(f"Loaded {len(train_texts)} training labels")

# Build character vocabulary
char_tokenizer.build_vocab(train_texts)

# Show some example characters from vocabulary
print("\nSample characters from vocabulary:")
sample_chars = list(char_tokenizer.char2idx.keys())[:20]
print(sample_chars)

# View all the vocabulary
print("All characters in vocabulary:")
print(list(char_tokenizer.char2idx.keys()))

# Initialize AraGPT2 tokenizer
# We're using the pre-trained AraGPT2 which understands Arabic text
print("Loading AraGPT2 tokenizer...")
gpt2_tokenizer = GPT2Tokenizer.from_pretrained('aubmindlab/aragpt2-base')

# Set padding token (GPT2 doesn't have one by default)
gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token

print(f"✓ AraGPT2 tokenizer loaded")
print(f"  Vocabulary size: {len(gpt2_tokenizer)}")

"""## 4. Create DataLoaders

Now we can create our datasets and dataloaders. We need custom collate function to handle variable-length sequences properly.
"""

def collate_fn(batch):
    """
    Custom collate function to handle variable-length sequences.

    CTC requires:
    - Images: batched normally
    - Targets: concatenated (not padded!)
    - Target lengths: for each sequence
    """
    images = torch.stack([item['image'] for item in batch])

    # For CTC: concatenate targets and track lengths
    char_tokens = [item['char_tokens'] for item in batch]
    char_lengths = torch.LongTensor([len(tokens) for tokens in char_tokens])
    char_tokens_concat = torch.cat(char_tokens)  # CTC expects concatenated targets

    # For GPT2: already padded in dataset
    gpt2_tokens = torch.stack([item['gpt2_tokens'] for item in batch])

    texts = [item['text'] for item in batch]

    return {
        'images': images,
        'char_tokens': char_tokens_concat,
        'char_lengths': char_lengths,
        'gpt2_tokens': gpt2_tokens,
        'texts': texts
    }

# Create datasets
train_dataset = ArabicCalligraphyDataset(
    TRAIN_IMG_DIR, TRAIN_CSV,
    char_tokenizer, gpt2_tokenizer,
    img_height=224,
    img_width=224
)

valid_dataset = ArabicCalligraphyDataset(
    VALID_IMG_DIR, VALID_CSV,
    char_tokenizer, gpt2_tokenizer,
    img_height=224,
    img_width=224
)

test_dataset = ArabicCalligraphyDataset(
    TEST_IMG_DIR, TEST_CSV,
    char_tokenizer, gpt2_tokenizer,
    img_height=224,
    img_width=224
)

# Create dataloaders
BATCH_SIZE = 16

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

print(f"\nDataset Statistics:")
print(f"  Training samples: {len(train_dataset)}")
print(f"  Validation samples: {len(valid_dataset)}")
print(f"  Test samples: {len(test_dataset)}")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Training batches per epoch: {len(train_loader)}")

"""## 5. Visualize Some Samples

Let's take a look at some samples from our dataset to make sure everything is loading correctly.
"""

# Visualize a few training samples
def visualize_samples(dataset, num_samples=4):
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    axes = axes.flatten()

    for i in range(min(num_samples, len(dataset))):
        sample = dataset[i]
        img = sample['image'].squeeze().numpy()
        text = sample['text']

        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f'Label: {text}', fontsize=10)
        axes[i].axis('off')

    plt.tight_layout()
    plt.show()

print("Sample images from training set:")
visualize_samples(train_dataset)

"""## 6. Model Architecture: SWIN + CTC + AraGPT2

Now for the main part - building our hybrid model!

### Architecture Components:

1. **SWIN Encoder**: Extracts visual features from calligraphy images
2. **CTC Layer**: Handles alignment between visual features and character sequences
3. **AraGPT2 Decoder**: Refines predictions using language model knowledge
"""

class SwinEncoder(nn.Module):
    """
    Swin Transformer encoder for extracting visual features.

    We use a pre-trained Swin Transformer and adapt it for OCR:
    - Remove the classification head
    - Extract feature maps instead of single vector
    - Reshape to sequence format for CTC
    """
    def __init__(self, pretrained=True):
        super().__init__()

        # Load pre-trained Swin Transformer
        # We use swin_tiny for efficiency, but you can try swin_small or swin_base
        self.swin = timm.create_model(
            'swin_tiny_patch4_window7_224',
            pretrained=pretrained,
            num_classes=0,  # Remove classification head
            global_pool=''  # Keep spatial dimensions
        )

        # Get feature dimension from the model
        self.feature_dim = self.swin.num_features

        print(f"Swin Encoder initialized with feature dim: {self.feature_dim}")

    def forward(self, x):
        """
        Args:
            x: Input images [batch, 1, height, width]

        Returns:
            features: [batch, sequence_length, feature_dim]
        """
        # Swin expects 3 channels, so we repeat grayscale
        if x.size(1) == 1:
            x = x.repeat(1, 3, 1, 1)  # [batch, 3, height, width]

        # Extract features from Swin
        features = self.swin.forward_features(x)  # [batch, h, w, feature_dim]

        # Swin already outputs in [batch, h, w, channels] format
        if features.dim() == 4:
            b, h, w, c = features.shape
            features = features.reshape(b, h * w, c)  # [batch, 49, 768]

        return features

class CTCHead(nn.Module):
    """
    CTC (Connectionist Temporal Classification) head.

    Takes visual features and predicts character probabilities at each time step.
    The CTC loss handles the alignment between predictions and ground truth.
    """
    def __init__(self, input_dim, num_classes):
        super().__init__()

        # Bidirectional LSTM to capture sequence context
        # This helps because characters depend on their neighbors
        self.lstm = nn.LSTM(
            input_dim,
            256,  # hidden size
            num_layers=2,
            bidirectional=True,
            batch_first=True,
            dropout=0.3
        )

        # Project LSTM outputs to character probabilities
        # *2 because bidirectional
        self.fc = nn.Linear(256 * 2, num_classes)

        print(f"CTC Head initialized: {input_dim} -> LSTM -> {num_classes} classes")

    def forward(self, features):
        """
        Args:
            features: [batch, seq_len, feature_dim]

        Returns:
            log_probs: [batch, seq_len, num_classes] in log space
        """
        # LSTM for sequence modeling
        lstm_out, _ = self.lstm(features)  # [batch, seq_len, 512]

        # Project to character space
        logits = self.fc(lstm_out)  # [batch, seq_len, num_classes]

        # CTC expects log probabilities
        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

        return log_probs

class AraGPT2Decoder(nn.Module):
    """
    AraGPT2-based decoder for language model refinement.
    """
    def __init__(self, char_vocab_size, gpt2_model_name='aubmindlab/aragpt2-base'):
        super().__init__()

        # Load pre-trained AraGPT2
        print(f"Loading AraGPT2 model: {gpt2_model_name}")
        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model_name)

        # Projection layer to map CTC outputs to GPT2 input space
        self.ctc_to_gpt2 = nn.Linear(char_vocab_size, self.gpt2.config.n_embd)

        print(f"✓ AraGPT2 Decoder initialized")

    def forward(self, ctc_probs, gpt2_input_ids=None):
        """
        Args:
            ctc_probs: [batch, seq_len, char_vocab_size] from CTC
            gpt2_input_ids: [batch, seq_len] ground truth tokens (for training)

        Returns:
            gpt2_logits: [batch, seq_len, gpt2_vocab_size]
        """
        if gpt2_input_ids is not None:
            # Training mode: use ground truth tokens only
            outputs = self.gpt2(
                input_ids=gpt2_input_ids,
                return_dict=True
            )
        else:
            # Inference mode: use CTC embeddings only
            ctc_embeddings = self.ctc_to_gpt2(ctc_probs)
            outputs = self.gpt2(
                inputs_embeds=ctc_embeddings,
                return_dict=True
            )

        return outputs.logits

class SWIN_CTC_AraGPT2(nn.Module):
    """
    Complete model combining:
    - SWIN encoder for visual features
    - CTC for alignment-free character prediction
    - AraGPT2 for language model refinement

    This is the full architecture for our 693 project!
    """
    def __init__(self, char_vocab_size, gpt2_tokenizer):
        super().__init__()

        print("\n" + "="*60)
        print("Building SWIN + CTC + AraGPT2 Model")
        print("="*60)

        # Component 1: Visual Encoder
        self.encoder = SwinEncoder(pretrained=True)

        # Component 2: CTC Head
        self.ctc_head = CTCHead(
            input_dim=self.encoder.feature_dim,
            num_classes=char_vocab_size
        )

        # Component 3: GPT2 Decoder
        self.decoder = AraGPT2Decoder(char_vocab_size)

        # Loss functions
        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)
        self.lm_loss = nn.CrossEntropyLoss(ignore_index=gpt2_tokenizer.pad_token_id)

        print("\n✓ Model architecture complete!")
        print("="*60 + "\n")

    def forward(self, images, char_targets=None, char_lengths=None,
                gpt2_targets=None, training=True):
        """
        Forward pass through the entire architecture.

        Args:
            images: [batch, 1, height, width]
            char_targets: concatenated character targets for CTC
            char_lengths: length of each target sequence
            gpt2_targets: [batch, seq_len] for language modeling
            training: whether in training mode

        Returns:
            Dictionary with losses and predictions
        """
        # Step 1: Extract visual features
        visual_features = self.encoder(images)  # [batch, seq_len, feature_dim]

        # Step 2: CTC predictions
        ctc_log_probs = self.ctc_head(visual_features)  # [batch, seq_len, char_vocab]

        # Calculate CTC loss if targets provided
        ctc_loss = None
        if char_targets is not None and training:
            # CTC expects: [seq_len, batch, num_classes]
            ctc_log_probs_t = ctc_log_probs.permute(1, 0, 2)
            input_lengths = torch.full(
                size=(ctc_log_probs.size(0),),
                fill_value=ctc_log_probs.size(1),
                dtype=torch.long
            )

            ctc_loss = self.ctc_loss(
                ctc_log_probs_t,
                char_targets,
                input_lengths,
                char_lengths
            )

        # Step 3: Language model refinement
        ctc_probs = torch.exp(ctc_log_probs)  # Convert from log space
        gpt2_logits = self.decoder(ctc_probs, gpt2_targets if training else None)

        # Calculate language model loss
        lm_loss = None
        if gpt2_targets is not None and training:
            # Shift targets for next-token prediction
            shift_logits = gpt2_logits[..., :-1, :].contiguous()
            shift_labels = gpt2_targets[..., 1:].contiguous()

            lm_loss = self.lm_loss(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )

        # Combine losses (you can adjust weights)
        total_loss = None
        if ctc_loss is not None and lm_loss is not None:
            total_loss = ctc_loss + 0.5 * lm_loss  # Weight LM loss lower

        return {
            'total_loss': total_loss,
            'ctc_loss': ctc_loss,
            'lm_loss': lm_loss,
            'ctc_log_probs': ctc_log_probs,
            'gpt2_logits': gpt2_logits
        }

    def decode_ctc(self, ctc_log_probs, char_tokenizer):
        """
        Decode CTC predictions to text using greedy decoding.
        Removes blanks and repeated characters.
        """
        # Get most likely character at each timestep
        _, preds = torch.max(ctc_log_probs, dim=-1)  # [batch, seq_len]

        decoded_texts = []
        for pred in preds:
            # Remove blanks (index 0) and consecutive duplicates
            pred = pred.cpu().numpy()
            chars = []
            prev_char = None

            for char_idx in pred:
                if char_idx != 0 and char_idx != prev_char:  # Not blank and not repeat
                    chars.append(int(char_idx))
                prev_char = char_idx

            # Decode to text
            text = char_tokenizer.decode(chars)
            decoded_texts.append(text)

        return decoded_texts

"""## 7. Initialize Model and Training Setup

Now let's create an instance of our model and set up the training configuration.
"""

# Create model
model = SWIN_CTC_AraGPT2(
    char_vocab_size=len(char_tokenizer),
    gpt2_tokenizer=gpt2_tokenizer
).to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nModel Statistics:")
print(f"  Total parameters: {total_params:,}")
print(f"  Trainable parameters: {trainable_params:,}")
print(f"  Model size: ~{total_params * 4 / 1024**2:.1f} MB (fp32)")

# Training configuration
EPOCHS = 5
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 1e-5

# Optimizer: AdamW works well for transformers
optimizer = optim.AdamW(
    model.parameters(),
    lr=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY
)

# Learning rate scheduler: reduce on plateau
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=3
)

print("Training configuration:")
print(f"  Epochs: {EPOCHS}")
print(f"  Learning rate: {LEARNING_RATE}")
print(f"  Weight decay: {WEIGHT_DECAY}")
print(f"  Optimizer: AdamW")
print(f"  Scheduler: ReduceLROnPlateau")

"""## 8. Training Loop

Here's where we actually train the model! We'll track both CTC loss and language model loss.
"""

class EarlyStopping:
    """Early stopping to stop training when validation loss doesn't improve."""
    def __init__(self, patience=5, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# Initialize early stopping
early_stopping = EarlyStopping(patience=3, min_delta=0.001)

def train_epoch(model, dataloader, optimizer, device):
    """
    Train for one epoch.
    """
    model.train()
    total_loss = 0
    total_ctc_loss = 0
    total_lm_loss = 0

    progress_bar = tqdm(dataloader, desc="Training")

    for batch_idx, batch in enumerate(progress_bar):
        # Move data to device
        images = batch['images'].to(device)
        char_tokens = batch['char_tokens'].to(device)
        char_lengths = batch['char_lengths'].to(device)
        gpt2_tokens = batch['gpt2_tokens'].to(device)

        # Forward pass
        optimizer.zero_grad()
        outputs = model(
            images,
            char_targets=char_tokens,
            char_lengths=char_lengths,
            gpt2_targets=gpt2_tokens,
            training=True
        )

        # Backward pass
        loss = outputs['total_loss']
        loss.backward()

        # Gradient clipping to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

        optimizer.step()

        # Track losses
        total_loss += loss.item()
        if outputs['ctc_loss'] is not None:
            total_ctc_loss += outputs['ctc_loss'].item()
        if outputs['lm_loss'] is not None:
            total_lm_loss += outputs['lm_loss'].item()

        # Update progress bar
        progress_bar.set_postfix({
            'loss': loss.item(),
            'ctc': outputs['ctc_loss'].item() if outputs['ctc_loss'] else 0,
            'lm': outputs['lm_loss'].item() if outputs['lm_loss'] else 0
        })

    return {
        'loss': total_loss / len(dataloader),
        'ctc_loss': total_ctc_loss / len(dataloader),
        'lm_loss': total_lm_loss / len(dataloader)
    }

def validate(model, dataloader, device, char_tokenizer):
    """
    Validate the model and compute metrics.
    """
    model.eval()
    total_loss = 0
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validation"):
            images = batch['images'].to(device)
            char_tokens = batch['char_tokens'].to(device)
            char_lengths = batch['char_lengths'].to(device)
            gpt2_tokens = batch['gpt2_tokens'].to(device)

            # Forward pass - set training=True to compute losses
            outputs = model(
                images,
                char_targets=char_tokens,
                char_lengths=char_lengths,
                gpt2_targets=gpt2_tokens,
                training=True  # Changed from False to True to compute losses
            )

            # Check if loss exists before adding
            if outputs['total_loss'] is not None:
                total_loss += outputs['total_loss'].item()

            # Decode predictions
            predictions = model.decode_ctc(outputs['ctc_log_probs'], char_tokenizer)
            all_predictions.extend(predictions)
            all_targets.extend(batch['texts'])

    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0

    # Calculate CER (Character Error Rate)
    total_chars = 0
    total_char_errors = 0

    # Calculate WER (Word Error Rate)
    total_words = 0
    total_word_errors = 0

    # Calculate Accuracy (exact match)
    exact_matches = 0

    for pred, target in zip(all_predictions, all_targets):
        # CER calculation
        char_errors = sum(c1 != c2 for c1, c2 in zip(pred, target))
        char_errors += abs(len(pred) - len(target))
        total_char_errors += char_errors
        total_chars += len(target)

        # WER calculation
        pred_words = pred.split()
        target_words = target.split()
        word_errors = sum(w1 != w2 for w1, w2 in zip(pred_words, target_words))
        word_errors += abs(len(pred_words) - len(target_words))
        total_word_errors += word_errors
        total_words += len(target_words)

        # Accuracy calculation (exact match)
        if pred == target:
            exact_matches += 1

    cer = total_char_errors / total_chars if total_chars > 0 else 0
    wer = total_word_errors / total_words if total_words > 0 else 0
    accuracy = exact_matches / len(all_predictions) if len(all_predictions) > 0 else 0

    return {
        'loss': avg_loss,
        'cer': cer,
        'wer': wer,
        'accuracy': accuracy,
        'predictions': all_predictions[:5],
        'targets': all_targets[:5]
    }

import os
checkpoint_dir = '/content/drive/MyDrive/calligraphy_checkpoints/SWIN_CTC_AraGPT2_HICMA_Plus_Syntheti'
os.makedirs(checkpoint_dir, exist_ok=True)
print(f"Checkpoints will be saved to: {checkpoint_dir}")



# Validate
val_metrics = validate(model, valid_loader, device, char_tokenizer)
val_losses.append(val_metrics['loss'])
val_cers.append(val_metrics['cer'])
val_wers.append(val_metrics['wer'])
val_accuracies.append(val_metrics['accuracy'])

print(f"Validation - Loss: {val_metrics['loss']:.4f}")
print(f"             CER: {val_metrics['cer']:.4f} ({(1-val_metrics['cer'])*100:.2f}% char accuracy)")
print(f"             WER: {val_metrics['wer']:.4f} ({(1-val_metrics['wer'])*100:.2f}% word accuracy)")
print(f"             Accuracy (exact match): {val_metrics['accuracy']*100:.2f}%")

# Save complete checkpoint
torch.save({
    'epoch': 0,  # Epoch 1 (0-indexed)
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_loss': train_metrics['loss'],
    'ctc_loss': train_metrics['ctc_loss'],
    'lm_loss': train_metrics['lm_loss'],
    'train_losses': [train_metrics['loss']],
}, f'{checkpoint_dir}/epoch_1_complete.pt')

print("✓ Epoch 1 model and metrics saved!")
print(f"  Location: {checkpoint_dir}/epoch_1_complete.pt")
print(f"\nEpoch 1 Results:")
print(f"  Training Loss: {train_metrics['loss']:.4f}")
print(f"  CTC Loss: {train_metrics['ctc_loss']:.4f}")
print(f"  LM Loss: {train_metrics['lm_loss']:.4f}")

print("\nDetailed sample predictions:")
for i, (pred, target) in enumerate(zip(val_metrics['predictions'], val_metrics['targets'])):
    print(f"\n{i+1}.")
    print(f"  Pred: '{pred}' (len={len(pred)})")
    print(f"  True: '{target}' (len={len(target)})")

"""## 9. Visualize Training Progress

Let's plot the training curves to see how well our model learned.
"""

# Plot training curves
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss curves
axes[0].plot(train_losses, label='Training Loss', marker='o')
axes[0].plot(val_losses, label='Validation Loss', marker='s')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].set_title('Training and Validation Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# CER curve
axes[1].plot(val_cers, label='Validation CER', marker='o', color='red')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Character Error Rate')
axes[1].set_title('Validation Character Error Rate')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/content/drive/MyDrive/SWIN_CTC_AraGPT2_HICMA_Plus_Syntheti/training_curves.png', dpi=300, bbox_inches='tight')
plt.show()

print("Training curves saved to Google Drive!")

"""## 10. Test Set Evaluation

Now let's evaluate our trained model on the test set to see final performance.
"""

# Load best model
print("Loading best model...")
checkpoint = torch.load('/content/drive/MyDrive/SWIN_CTC_AraGPT2_HICMA_Plus_Syntheti/best_model.pt')
model.load_state_dict(checkpoint['model_state_dict'])
print(f"✓ Loaded model from epoch {checkpoint['epoch']+1}")

# Evaluate on test set
print("\nEvaluating on test set...")
test_metrics = validate(model, test_loader, device, char_tokenizer)

print("\n" + "="*60)
print("Test Set Results")
print("="*60)
print(f"Test Loss: {test_metrics['loss']:.4f}")
print(f"Test CER: {test_metrics['cer']:.4f}")
print(f"Character Accuracy: {(1 - test_metrics['cer']) * 100:.2f}%")

print("\nSample Test Predictions:")
for i, (pred, target) in enumerate(zip(test_metrics['predictions'], test_metrics['targets'])):
    print(f"\n{i+1}.")
    print(f"  Prediction: {pred}")
    print(f"  Ground Truth: {target}")
    print(f"  Match: {'✓' if pred == target else '✗'}")

"""## 11. Inference on New Images

Let's create a function to run inference on new calligraphy images.
"""

def predict_on_image(image_path, model, char_tokenizer, device):
    """
    Run OCR on a single image.
    """
    model.eval()

    # Load and preprocess image
    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, (256, 64))
    img = img.astype(np.float32) / 255.0
    img = np.expand_dims(img, axis=0)
    img_tensor = torch.FloatTensor(img).unsqueeze(0).to(device)  # [1, 1, H, W]

    # Run inference
    with torch.no_grad():
        outputs = model(img_tensor, training=False)
        predictions = model.decode_ctc(outputs['ctc_log_probs'], char_tokenizer)

    return predictions[0]

# Example: predict on a test image
test_image_path = list(Path(TEST_IMG_DIR).glob('*.png'))[0]
prediction = predict_on_image(test_image_path, model, char_tokenizer, device)

# Visualize
img = Image.open(test_image_path)
plt.figure(figsize=(10, 3))
plt.imshow(img, cmap='gray')
plt.title(f'Predicted: {prediction}', fontsize=12)
plt.axis('off')
plt.tight_layout()
plt.show()

"""## 12. Save Final Results

Let's save our model and create a summary of results for the project report.
"""

# Save final model
torch.save({
    'model_state_dict': model.state_dict(),
    'char_tokenizer': char_tokenizer,
    'config': {
        'char_vocab_size': len(char_tokenizer),
        'img_height': 64,
        'img_width': 256,
        'epochs': EPOCHS,
        'batch_size': BATCH_SIZE,
        'learning_rate': LEARNING_RATE
    },
    'test_metrics': test_metrics
}, '/content/drive/MyDrive/SWIN_CTC_AraGPT2_HICMA_Plus_Syntheti/final_model_693.pt')

# Create results summary
results_summary = {
    'Architecture': 'SWIN (Swin Transformer) + CTC + AraGPT2',
    'Dataset': {
        'Training samples': len(train_dataset),
        'Validation samples': len(valid_dataset),
        'Test samples': len(test_dataset)
    },
    'Training': {
        'Epochs': EPOCHS,
        'Batch size': BATCH_SIZE,
        'Learning rate': LEARNING_RATE,
        'Optimizer': 'AdamW',
        'Best validation loss': float(best_val_loss),
        'Best validation CER': float(best_cer)
    },
    'Test Results': {
        'Test loss': float(test_metrics['loss']),
        'Test CER': float(test_metrics['cer']),
        'Character accuracy': float((1 - test_metrics['cer']) * 100)
    },
    'Model Size': {
        'Total parameters': int(total_params),
        'Trainable parameters': int(trainable_params)
    }
}

# Save as JSON
with open('/content/drive/MyDrive/SWIN_CTC_AraGPT2_HICMA_Plus_Syntheti/results_summary_693.json', 'w', encoding='utf-8') as f:
    json.dump(results_summary, f, indent=2, ensure_ascii=False)

print("\n" + "="*60)
print("Project 693 Complete!")
print("="*60)
print("\nSaved files:")
print("  • best_model.pt - Best model checkpoint")
print("  • final_model_693.pt - Final model with config")
print("  • results_summary_693.json - Results summary")
print("  • training_curves.png - Training visualizations")
print("\n" + json.dumps(results_summary, indent=2, ensure_ascii=False))