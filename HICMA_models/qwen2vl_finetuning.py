# -*- coding: utf-8 -*-
"""Qwen2VL_FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/102dzmCHWKMDcKfZT0XgBgkKGTn06EMoj

# Arabic Calligraphy OCR: Qwen2-VL Fine-tuning

**Approach:** Fine-tune Qwen2-VL-2B-Instruct with LoRA for Arabic calligraphy recognition

---

## Overview

This notebook implements fine-tuning of Qwen2-VL for Arabic OCR following QARI-OCR's methodology:

- Pre-trained vision-language model (Qwen2-VL-2B-Instruct)
- LoRA for parameter-efficient fine-tuning
- Single epoch training (proven sufficient)
- Conversational format for natural task framing

## Expected Performance

- Word Error Rate: 5-15%
- Character Error Rate: 2-8%
- Training time: 45-60 minutes on A100 GPU

## Training Configuration (QARI-OCR parameters)

- Epochs: 1
- Learning rate: 2e-4 with linear warmup and decay
- Batch size: 2 per device
- Gradient accumulation: 4 steps (effective batch size = 8)
- Optimizer: AdamW with weight decay 0.01
- LoRA rank: 16

## 1. Installation
"""

# Install required packages
!pip install -q transformers>=4.37.0
!pip install -q qwen-vl-utils
!pip install -q accelerate
!pip install -q peft
!pip install -q bitsandbytes
!pip install -q pillow pandas
!pip install -q torch torchvision

print("Installation complete")

"""## 2. Imports and Device Setup"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from pathlib import Path
from PIL import Image
from tqdm.auto import tqdm
import json
import os
import gc

from transformers import (
    Qwen2VLForConditionalGeneration,
    AutoProcessor,
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from qwen_vl_utils import process_vision_info

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

"""## 3. Dataset Paths Configuration

**To run on different dataset:** Simply change the paths below and re-run all cells
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Dataset paths - MODIFY THESE for different datasets
TRAIN_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/train/images'
TRAIN_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/train_labels.csv'

VALID_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/val/images'
VALID_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/val_labels.csv'

TEST_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/test/images'
TEST_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/test_labels.csv'

# Checkpoint directory
CHECKPOINT_DIR = '/content/drive/MyDrive/qwen2vl_checkpoints/hicma'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

print("Paths configured:")
print(f"  Train: {TRAIN_IMG_DIR}")
print(f"  Valid: {VALID_IMG_DIR}")
print(f"  Test: {TEST_IMG_DIR}")
print(f"  Checkpoints: {CHECKPOINT_DIR}")

# Verify paths exist
print("\nVerifying paths...")
for name, path in [('Train', TRAIN_IMG_DIR), ('Valid', VALID_IMG_DIR), ('Test', TEST_IMG_DIR)]:
    exists = Path(path).exists()
    print(f"  {name}: {'OK' if exists else 'NOT FOUND'}")

"""## 4. Load Qwen2-VL Model with LoRA"""

!pip install -U hf-transfer

!mkdir -p /content/drive/MyDrive/hf_cache

import os
os.environ["HF_HOME"] = "/content/drive/MyDrive/hf_cache"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

print("Loading Qwen2-VL-2B-Instruct...")
print("This takes 3-5 minutes on first run\n")

# Load model with 8-bit quantization
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    load_in_8bit=True
)

# Prepare for k-bit training
model = prepare_model_for_kbit_training(model)

# Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

# Print model statistics
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
total = sum(p.numel() for p in model.parameters())

print("Model Statistics:")
print(f"  Total parameters: {total:,}")
print(f"  Trainable parameters: {trainable:,}")
print(f"  Trainable percentage: {100 * trainable / total:.2f}%")

# Load processor
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
print("\nModel and processor ready")

"""## 5. Dataset Implementation"""

class ArabicOCRDataset(Dataset):
    """
    Dataset formatted as Qwen2-VL conversations.
    User provides image, assistant responds with transcription.
    """
    def __init__(self, img_dir, csv_path, processor, split='train'):
        self.img_dir = Path(img_dir)
        self.processor = processor
        self.split = split

        # Load labels from CSV
        df = pd.read_csv(csv_path)
        self.labels_dict = dict(zip(df['img_name'], df['label']))

        # Get image files
        self.image_files = sorted(
            list(self.img_dir.glob('*.png')) +
            list(self.img_dir.glob('*.jpg'))
        )
        self.image_files = [img for img in self.image_files
                           if img.name in self.labels_dict]

        print(f"{split.capitalize()}: {len(self.image_files)} images")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        text = self.labels_dict[img_path.name]

        # Format as conversation
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": str(img_path)},
                    {"type": "text", "text": "Please transcribe the Arabic text in this image."}
                ]
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": text}]
            }
        ]

        return {
            'messages': messages,
            'text': text,
            'img_path': str(img_path)
        }

# Create datasets
print("Creating datasets...\n")
train_dataset = ArabicOCRDataset(TRAIN_IMG_DIR, TRAIN_CSV, processor, 'train')
valid_dataset = ArabicOCRDataset(VALID_IMG_DIR, VALID_CSV, processor, 'valid')
test_dataset = ArabicOCRDataset(TEST_IMG_DIR, TEST_CSV, processor, 'test')

print(f"\nTotal: {len(train_dataset) + len(valid_dataset) + len(test_dataset)} samples")

from torch.utils.data import Subset
import random

def make_subset(dataset, n):
    indices = list(range(len(dataset)))
    random.shuffle(indices)
    return Subset(dataset, indices[:n])

"""## 6. Collate Function and DataLoaders"""

def collate_fn(batch):
    """Process batch into model inputs."""
    messages_batch = [item['messages'] for item in batch]
    texts = [item['text'] for item in batch]

    # Apply chat template
    formatted_texts = [
        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)
        for msg in messages_batch
    ]

    # Process images
    image_inputs, video_inputs = process_vision_info(messages_batch)

    # Tokenize
    inputs = processor(
        text=formatted_texts,
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    )

    inputs['labels'] = inputs['input_ids'].clone()

    return {'inputs': inputs, 'texts': texts}

# Configuration
BATCH_SIZE = 2
GRADIENT_ACCUMULATION = 4

# Create loaders
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

print(f"DataLoaders:")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Gradient accumulation: {GRADIENT_ACCUMULATION}")
print(f"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}")
print(f"  Training batches: {len(train_loader)}")

"""## 7. Training Functions"""

def train_epoch(model, dataloader, optimizer, scheduler, device, grad_accum=4):
    """Train for one epoch with gradient accumulation."""
    model.train()
    total_loss = 0
    optimizer.zero_grad()

    progress_bar = tqdm(dataloader, desc="Training")

    for batch_idx, batch in enumerate(progress_bar):
        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                 for k, v in batch['inputs'].items()}

        outputs = model(**inputs)
        loss = outputs.loss / grad_accum
        loss.backward()

        if (batch_idx + 1) % grad_accum == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

        total_loss += loss.item() * grad_accum
        progress_bar.set_postfix({'loss': f"{loss.item() * grad_accum:.4f}"})

    return total_loss / len(dataloader)

def generate_text(model, processor, image_path):
    """Generate text from image."""
    model.eval()

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image_path},
                {"type": "text", "text": "Please transcribe the Arabic text in this image."}
            ]
        }
    ]

    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    )
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)

    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )

    return output_text[0].strip()

def evaluate(model, processor, dataset, device):
    """Evaluate model on dataset."""
    model.eval()

    all_preds = []
    all_targets = []

    for item in tqdm(dataset, desc="Evaluating"):
        pred = generate_text(model, processor, item['img_path'])
        all_preds.append(pred)
        all_targets.append(item['text'])

    # Calculate metrics
    total_chars = 0
    total_char_errors = 0
    total_words = 0
    total_word_errors = 0
    exact_matches = 0

    for pred, target in zip(all_preds, all_targets):
        # CER
        char_errors = sum(c1 != c2 for c1, c2 in zip(pred, target))
        char_errors += abs(len(pred) - len(target))
        total_char_errors += char_errors
        total_chars += len(target)

        # WER
        pred_words = pred.split()
        target_words = target.split()
        word_errors = sum(w1 != w2 for w1, w2 in zip(pred_words, target_words))
        word_errors += abs(len(pred_words) - len(target_words))
        total_word_errors += word_errors
        total_words += len(target_words)

        if pred == target:
            exact_matches += 1

    cer = total_char_errors / total_chars if total_chars > 0 else 0
    wer = total_word_errors / total_words if total_words > 0 else 0
    accuracy = exact_matches / len(all_preds) if len(all_preds) > 0 else 0

    return {
        'cer': cer,
        'wer': wer,
        'accuracy': accuracy,
        'predictions': all_preds[:5],
        'targets': all_targets[:5]
    }

print("Functions defined")

"""## 8. Training Setup"""

# Training configuration
EPOCHS = 1
LEARNING_RATE = 2e-4
WEIGHT_DECAY = 0.01
WARMUP_RATIO = 0.03

# Calculate steps
num_training_steps = (len(train_loader) // GRADIENT_ACCUMULATION) * EPOCHS
num_warmup_steps = int(num_training_steps * WARMUP_RATIO)

# Optimizer
optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY
)

# Scheduler
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

print("Training Configuration:")
print(f"  Epochs: {EPOCHS}")
print(f"  Learning rate: {LEARNING_RATE}")
print(f"  Weight decay: {WEIGHT_DECAY}")
print(f"  Total steps: {num_training_steps}")
print(f"  Warmup steps: {num_warmup_steps}")
print(f"\nEstimated time: 45-60 minutes on A100")

"""## 9. Training"""

import gc

print("\n" + "="*60)
print("Training Qwen2-VL")
print("="*60 + "\n")

best_wer = float('inf')
train_losses = []
val_wers = []
val_cers = []

# Create validation subset for faster evaluation during training
from torch.utils.data import Subset

val_subset = Subset(valid_dataset, range(min(200, len(valid_dataset))))  # 200 samples
val_subset_loader = DataLoader(
    val_subset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

for epoch in range(EPOCHS):
    print(f"\n{'='*60}")
    print(f"EPOCH {epoch+1}/{EPOCHS}")
    print('='*60)

    # Clear memory
    torch.cuda.empty_cache()
    gc.collect()

    # TRAIN
    train_loss = train_epoch(
        model, train_loader, optimizer, scheduler, device, GRADIENT_ACCUMULATION
    )
    train_losses.append(train_loss)

    print(f"\nTrain Loss: {train_loss:.4f}")

    # Clear memory
    torch.cuda.empty_cache()
    gc.collect()

    # EVALUATE ON VALIDATION SUBSET
    print("\nEvaluating on validation subset (200 samples)...")

    model.eval()
    all_preds = []
    all_targets = []

    for batch in tqdm(val_subset_loader, desc="Validating"):
        pixel_values = batch['inputs']['pixel_values'].to(device)
        texts = batch['texts']

        # Generate predictions
        with torch.no_grad():
            generated_ids = model.generate(
                pixel_values,
                max_new_tokens=128,
                do_sample=False  # Greedy for speed during training
            )

        # Decode
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(batch['inputs']['input_ids'], generated_ids)
        ]
        predictions = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )

        all_preds.extend([p.strip() for p in predictions])
        all_targets.extend(texts)

    # Calculate metrics
    total_chars = 0
    total_char_errors = 0
    total_words = 0
    total_word_errors = 0
    exact_matches = 0

    for pred, target in zip(all_preds, all_targets):
        # CER
        char_errors = sum(c1 != c2 for c1, c2 in zip(pred, target))
        char_errors += abs(len(pred) - len(target))
        total_char_errors += char_errors
        total_chars += len(target)

        # WER
        pred_words = pred.split()
        target_words = target.split()
        word_errors = sum(w1 != w2 for w1, w2 in zip(pred_words, target_words))
        word_errors += abs(len(pred_words) - len(target_words))
        total_word_errors += word_errors
        total_words += len(target_words)

        if pred == target:
            exact_matches += 1

    cer = total_char_errors / total_chars if total_chars > 0 else 0
    wer = total_word_errors / total_words if total_words > 0 else 0
    accuracy = exact_matches / len(all_preds) if len(all_preds) > 0 else 0

    val_wers.append(wer)
    val_cers.append(cer)

    # Clear memory
    torch.cuda.empty_cache()
    gc.collect()

    # PRINT RESULTS
    print("\n" + "="*60)
    print(f"EPOCH {epoch+1} RESULTS")
    print("="*60)
    print(f"Training Loss:                   {train_loss:.4f}")
    print(f"\nValidation Metrics (200 samples):")
    print(f"  Character Error Rate (CER):    {cer:.4f}")
    print(f"  Word Error Rate (WER):          {wer:.4f}")
    print(f"  Character Accuracy:             {(1-cer)*100:.2f}%")
    print(f"  Word Accuracy:                  {(1-wer)*100:.2f}%")
    print(f"  Exact Match:                    {accuracy*100:.2f}%")
    print("="*60)

    # Sample predictions
    print("\nSample Validation Predictions:")
    for i in range(min(5, len(all_preds))):
        match = 'MATCH' if all_preds[i] == all_targets[i] else 'DIFF'
        print(f"  {i+1}. [{match}]")
        print(f"     Pred: '{all_preds[i]}'")
        print(f"     True: '{all_targets[i]}'")

    # Save best model
    if wer < best_wer:
        best_wer = wer
        print(f"\n*** NEW BEST MODEL ***")
        print(f"Best Validation WER: {best_wer:.4f} ({(1-best_wer)*100:.2f}% accuracy)")
        print("Saving...")
        model.save_pretrained(f"{CHECKPOINT_DIR}/best_model")
        processor.save_pretrained(f"{CHECKPOINT_DIR}/best_model")

    # Save checkpoint
    print(f"\nSaving epoch {epoch+1}...")
    model.save_pretrained(f"{CHECKPOINT_DIR}/epoch_{epoch+1}")
    processor.save_pretrained(f"{CHECKPOINT_DIR}/epoch_{epoch+1}")

    print(f"\nGPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB")

# FINAL SUMMARY
print("\n" + "="*60)
print("TRAINING COMPLETE - FULL SUMMARY")
print("="*60)
print("\nEpoch-by-Epoch Performance:")
print("-"*60)
print(f"{'Epoch':<8} {'Loss':<12} {'Val WER':<12} {'Val CER':<12}")
print("-"*60)
for i in range(len(train_losses)):
    print(f"{i+1:<8} {train_losses[i]:<12.4f} {val_wers[i]:<12.4f} {val_cers[i]:<12.4f}")
print("-"*60)
print(f"\nBest Validation WER: {best_wer:.4f} ({(1-best_wer)*100:.2f}% word accuracy)")
print("="*60)

"""## 10. Validation Evaluation"""

print("\nEvaluating on validation set...")
valid_metrics = evaluate(model, processor, valid_dataset, device)

print("\n" + "="*60)
print("Validation Results")
print("="*60)
print(f"  CER: {valid_metrics['cer']:.4f} ({(1-valid_metrics['cer'])*100:.2f}% accuracy)")
print(f"  WER: {valid_metrics['wer']:.4f} ({(1-valid_metrics['wer'])*100:.2f}% accuracy)")
print(f"  Exact Match: {valid_metrics['accuracy']*100:.2f}%")

print("\nSample Predictions:")
for i, (pred, target) in enumerate(zip(valid_metrics['predictions'], valid_metrics['targets'])):
    match = 'MATCH' if pred == target else 'DIFF'
    print(f"\n{i+1}. [{match}]")
    print(f"  Pred: '{pred}'")
    print(f"  True: '{target}'")

"""## 11. Test Set Evaluation"""

print("\nEvaluating on test set...")
print("This takes 10-15 minutes\n")

test_metrics = evaluate(model, processor, test_dataset, device)

print("\n" + "="*60)
print("FINAL TEST SET RESULTS")
print("="*60)
print(f"  CER: {test_metrics['cer']:.4f} ({(1-test_metrics['cer'])*100:.2f}% accuracy)")
print(f"  WER: {test_metrics['wer']:.4f} ({(1-test_metrics['wer'])*100:.2f}% accuracy)")
print(f"  Exact Match: {test_metrics['accuracy']*100:.2f}%")

print("\nSample Test Predictions:")
for i, (pred, target) in enumerate(zip(test_metrics['predictions'], test_metrics['targets'])):
    match = 'MATCH' if pred == target else 'DIFF'
    print(f"\n{i+1}. [{match}]")
    print(f"  Pred: '{pred}'")
    print(f"  True: '{target}'")

"""## 12. Save Results"""

# Save results summary
results = {
    'model': 'Qwen2-VL-2B-Instruct',
    'lora_rank': 16,
    'training': {
        'epochs': EPOCHS,
        'learning_rate': LEARNING_RATE,
        'batch_size': BATCH_SIZE * GRADIENT_ACCUMULATION,
        'final_loss': train_loss
    },
    'validation': {
        'cer': float(valid_metrics['cer']),
        'wer': float(valid_metrics['wer']),
        'word_accuracy': float((1-valid_metrics['wer'])*100),
        'exact_match': float(valid_metrics['accuracy']*100)
    },
    'test': {
        'cer': float(test_metrics['cer']),
        'wer': float(test_metrics['wer']),
        'word_accuracy': float((1-test_metrics['wer'])*100),
        'exact_match': float(test_metrics['accuracy']*100)
    }
}

# Save to file
results_path = f"{CHECKPOINT_DIR}/results.json"
with open(results_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print("\nResults Summary:")
print(json.dumps(results, indent=2, ensure_ascii=False))
print(f"\nSaved to: {results_path}")
print(f"Model saved to: {CHECKPOINT_DIR}/epoch_1")