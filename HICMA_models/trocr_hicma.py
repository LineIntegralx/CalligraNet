# -*- coding: utf-8 -*-
"""TrOCR_HICMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1paACspGM1qtxa5di4Y2utCw3jW4yYG50

# Arabic Calligraphy OCR: TrOCR Fine-tuning

**Approach:** Fine-tune Microsoft's TrOCR with LoRA for Arabic calligraphy recognition

---

## Overview

TrOCR (Transformer-based OCR) is Microsoft's specialized model for optical character recognition:

- **Vision Encoder:** Vision Transformer (ViT) processes images
- **Text Decoder:** RoBERTa generates text sequences
- **Training Strategy:** LoRA for parameter-efficient fine-tuning
- **Task:** Seq2seq where image → text transcription

## Why TrOCR for This Project?

1. **Purpose-built for OCR** - Not a general vision-language model
2. **Encoder-decoder architecture** - Natural fit for image-to-text tasks
3. **Proven on Arabic text** - Has Arabic-specific variants
4. **Efficient training** - LoRA reduces trainable parameters
5. **Different from QARI** - Uses ViT + RoBERTa vs Qwen's architecture


## Training Configuration

- Epochs: 5
- Learning rate: 5e-5 with warmup
- Batch size: 4 per device
- Gradient accumulation: 2 (effective batch size = 8)
- Optimizer: AdamW with weight decay
- LoRA rank: 16

## 1. Installation
"""

# Install required packages
!pip install -q transformers>=4.30.0
!pip install -q peft
!pip install -q accelerate
!pip install -q pillow pandas
!pip install -q torch torchvision
!pip install -q jiwer  # For WER calculation

print("Installation complete")

"""## 2. Imports and Device Setup"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from pathlib import Path
from PIL import Image
from tqdm.auto import tqdm
import json
import os

from transformers import (
    VisionEncoderDecoderModel,
    TrOCRProcessor,
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, TaskType

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

"""## 3. Dataset Paths Configuration

**To run on different dataset:** Change these paths and re-run all cells
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Dataset paths - MODIFY THESE for different datasets
TRAIN_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/train/images'
TRAIN_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/train_labels.csv'

VALID_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/val/images'
VALID_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/val_labels.csv'

TEST_IMG_DIR = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/test/images'
TEST_CSV = '/content/drive/MyDrive/EECE693_Project/Preprocessed_HICMA/test_labels.csv'

# Checkpoint directory
CHECKPOINT_DIR = '/content/drive/MyDrive/trocr_checkpoints/hicma'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

print("Paths configured:")
print(f"  Train: {TRAIN_IMG_DIR}")
print(f"  Valid: {VALID_IMG_DIR}")
print(f"  Test: {TEST_IMG_DIR}")
print(f"  Checkpoints: {CHECKPOINT_DIR}")

# Verify paths exist
print("\nVerifying paths...")
for name, path in [('Train', TRAIN_IMG_DIR), ('Valid', VALID_IMG_DIR), ('Test', TEST_IMG_DIR)]:
    exists = Path(path).exists()
    print(f"  {name}: {'OK' if exists else 'NOT FOUND'}")

"""## 4. Load TrOCR Model and Processor

We use the base TrOCR model and apply LoRA adapters for efficient fine-tuning.
"""

print("Loading TrOCR model...")

# Load model
model = VisionEncoderDecoderModel.from_pretrained(
    "microsoft/trocr-base-handwritten"
)

processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")

# Configure model
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.eos_token_id = processor.tokenizer.sep_token_id
model.config.max_length = 128
model.config.early_stopping = True
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 4

# UNFREEZE ENCODER - let it adapt to Arabic!
print("Encoder will be trained (fine-tuned for Arabic)")
print("Decoder will have LoRA adapters")

# Apply LoRA to decoder only
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Will target decoder's attention
    lora_dropout=0.05,
    bias="none"
)

model = get_peft_model(model, lora_config)

# Move to device
model = model.to(device)

# Print stats
trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
total = sum(p.numel() for p in model.parameters())

print(f"\nModel Statistics:")
print(f"  Total: {total:,}")
print(f"  Trainable: {trainable:,}")
print(f"  Percentage: {100*trainable/total:.2f}%")
print(f"  Encoder: TRAINABLE (adapting to Arabic)")
print(f"  Decoder: LoRA adapters")
print("\nReady for training")

"""## 5. Dataset Implementation

Dataset handles variable-size images by resizing with aspect ratio preservation.
"""

class ArabicOCRDataset(Dataset):
    """
    Dataset for TrOCR training.

    Processes images and text labels for encoder-decoder training.
    Images are resized to fixed dimensions while preserving aspect ratio.
    """
    def __init__(self, img_dir, csv_path, processor, max_target_length=128, split='train'):
        self.img_dir = Path(img_dir)
        self.processor = processor
        self.max_target_length = max_target_length
        self.split = split

        # Load labels from CSV
        df = pd.read_csv(csv_path)
        self.labels_dict = dict(zip(df['img_name'], df['label']))

        # Get image files
        self.image_files = sorted(
            list(self.img_dir.glob('*.png')) +
            list(self.img_dir.glob('*.jpg'))
        )
        self.image_files = [img for img in self.image_files
                           if img.name in self.labels_dict]

        print(f"{split.capitalize()}: {len(self.image_files)} images")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]

        # Load image
        image = Image.open(img_path).convert('RGB')

        # Get text label
        text = self.labels_dict[img_path.name]

        # Process image with processor
        # The processor handles resizing and normalization
        pixel_values = self.processor(
            images=image,
            return_tensors="pt"
        ).pixel_values.squeeze()

        # Tokenize text
        labels = self.processor.tokenizer(
            text,
            padding="max_length",
            max_length=self.max_target_length,
            truncation=True,
            return_tensors="pt"
        ).input_ids.squeeze()

        # Replace padding token id with -100 for loss calculation
        labels[labels == self.processor.tokenizer.pad_token_id] = -100

        return {
            'pixel_values': pixel_values,
            'labels': labels,
            'text': text
        }

# Create datasets
print("Creating datasets...\n")
train_dataset = ArabicOCRDataset(TRAIN_IMG_DIR, TRAIN_CSV, processor, split='train')
valid_dataset = ArabicOCRDataset(VALID_IMG_DIR, VALID_CSV, processor, split='valid')
test_dataset = ArabicOCRDataset(TEST_IMG_DIR, TEST_CSV, processor, split='test')

print(f"\nTotal: {len(train_dataset) + len(valid_dataset) + len(test_dataset)} samples")

"""## 6. DataLoaders"""

def collate_fn(batch):
    """Collate function to stack batch items."""
    pixel_values = torch.stack([item['pixel_values'] for item in batch])
    labels = torch.stack([item['labels'] for item in batch])
    texts = [item['text'] for item in batch]

    return {
        'pixel_values': pixel_values,
        'labels': labels,
        'texts': texts
    }

# Configuration
BATCH_SIZE = 4
GRADIENT_ACCUMULATION = 2

# Create loaders
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=2,
    pin_memory=True
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

print(f"DataLoaders:")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Gradient accumulation: {GRADIENT_ACCUMULATION}")
print(f"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}")
print(f"  Training batches: {len(train_loader)}")

"""## 7. Training and Evaluation Functions"""

def train_epoch(model, dataloader, optimizer, scheduler, device, grad_accum=2):
    """Train for one epoch with gradient accumulation."""
    model.train()
    total_loss = 0
    optimizer.zero_grad()

    progress_bar = tqdm(dataloader, desc="Training")

    for batch_idx, batch in enumerate(progress_bar):
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        # Forward pass
        # For VisionEncoderDecoderModel, just pass pixel_values and labels
        outputs = model(
            pixel_values=pixel_values,
            labels=labels
        )
        loss = outputs.loss / grad_accum

        # Backward pass
        loss.backward()

        # Update weights every grad_accum steps
        if (batch_idx + 1) % grad_accum == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

        total_loss += loss.item() * grad_accum
        progress_bar.set_postfix({'loss': f"{loss.item() * grad_accum:.4f}"})

    return total_loss / len(dataloader)

def generate_text(model, processor, pixel_values):
    """Generate text from image pixel values."""
    model.eval()

    with torch.no_grad():
        generated_ids = model.generate(pixel_values)

    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
    return generated_text

def evaluate(model, processor, dataloader, device):
    """Evaluate model on dataset."""
    model.eval()

    all_preds = []
    all_targets = []

    for batch in tqdm(dataloader, desc="Evaluating"):
        pixel_values = batch['pixel_values'].to(device)
        texts = batch['texts']

        # Generate predictions
        predictions = generate_text(model, processor, pixel_values)

        all_preds.extend(predictions)
        all_targets.extend(texts)

    # Calculate metrics
    total_chars = 0
    total_char_errors = 0
    total_words = 0
    total_word_errors = 0
    exact_matches = 0

    for pred, target in zip(all_preds, all_targets):
        # Character Error Rate
        char_errors = sum(c1 != c2 for c1, c2 in zip(pred, target))
        char_errors += abs(len(pred) - len(target))
        total_char_errors += char_errors
        total_chars += len(target)

        # Word Error Rate
        pred_words = pred.split()
        target_words = target.split()
        word_errors = sum(w1 != w2 for w1, w2 in zip(pred_words, target_words))
        word_errors += abs(len(pred_words) - len(target_words))
        total_word_errors += word_errors
        total_words += len(target_words)

        # Exact match
        if pred == target:
            exact_matches += 1

    cer = total_char_errors / total_chars if total_chars > 0 else 0
    wer = total_word_errors / total_words if total_words > 0 else 0
    accuracy = exact_matches / len(all_preds) if len(all_preds) > 0 else 0

    return {
        'cer': cer,
        'wer': wer,
        'accuracy': accuracy,
        'predictions': all_preds[:5],
        'targets': all_targets[:5]
    }

print("Functions defined")

"""## 8. Training Setup"""

EPOCHS = 5
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 0.01
WARMUP_RATIO = 0.05

# Use COSINE scheduler
num_training_steps = (len(train_loader) // GRADIENT_ACCUMULATION) * EPOCHS
num_warmup_steps = int(num_training_steps * WARMUP_RATIO)

optimizer = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY
)

# COSINE scheduler
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

print("OPTIMIZED Training Configuration:")
print(f"  Epochs: {EPOCHS}")
print(f"  Learning rate: {LEARNING_RATE} (2x higher)")
print(f"  Scheduler: COSINE (maintains higher LR longer)")
print(f"  Weight decay: {WEIGHT_DECAY}")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Gradient accumulation: {GRADIENT_ACCUMULATION}")
print(f"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}")
print(f"  Total steps: {num_training_steps}")
print(f"  Warmup steps: {num_warmup_steps}")
```

"""## 9. Training Loop"""

import gc

print("\n" + "="*60)
print("Training TrOCR")
print("="*60 + "\n")

best_wer = float('inf')
train_losses = []
val_wers = []
val_cers = []

# Create validation subset for faster evaluation
from torch.utils.data import Subset

val_subset = Subset(valid_dataset, range(min(200, len(valid_dataset))))  # 200 samples
val_subset_loader = DataLoader(
    val_subset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    collate_fn=collate_fn,
    num_workers=2
)

for epoch in range(EPOCHS):
    print(f"\n{'='*60}")
    print(f"EPOCH {epoch+1}/{EPOCHS}")
    print('='*60)

    # Clear memory
    torch.cuda.empty_cache()
    gc.collect()

    # TRAIN
    train_loss = train_epoch(
        model, train_loader, optimizer, scheduler, device, GRADIENT_ACCUMULATION
    )
    train_losses.append(train_loss)

    print(f"\nTrain Loss: {train_loss:.4f}")

    # Clear memory
    torch.cuda.empty_cache()
    gc.collect()

    # EVALUATE ON VALIDATION SUBSET
    print("\nEvaluating on validation subset (200 samples)...")
    val_metrics = evaluate(model, processor, val_subset_loader, device)
    val_wers.append(val_metrics['wer'])
    val_cers.append(val_metrics['cer'])

    # Clear memory
    torch.cuda.empty_cache()
    gc.collect()

    # PRINT RESULTS
    print("\n" + "="*60)
    print(f"EPOCH {epoch+1} RESULTS")
    print("="*60)
    print(f"Training Loss:                   {train_loss:.4f}")
    print(f"\nValidation Metrics (200 samples):")
    print(f"  Character Error Rate (CER):    {val_metrics['cer']:.4f}")
    print(f"  Word Error Rate (WER):          {val_metrics['wer']:.4f}")
    print(f"  Character Accuracy:             {(1-val_metrics['cer'])*100:.2f}%")
    print(f"  Word Accuracy:                  {(1-val_metrics['wer'])*100:.2f}%")
    print(f"  Exact Match:                    {val_metrics['accuracy']*100:.2f}%")
    print("="*60)

    # Sample predictions
    print("\nSample Validation Predictions:")
    for i, (pred, target) in enumerate(zip(val_metrics['predictions'], val_metrics['targets'])):
        match = 'MATCH' if pred == target else 'DIFF'
        print(f"  {i+1}. [{match}]")
        print(f"     Pred: '{pred}'")
        print(f"     True: '{target}'")

    # Save best model
    if val_metrics['wer'] < best_wer:
        best_wer = val_metrics['wer']
        print(f"\n*** NEW BEST MODEL ***")
        print(f"Best Validation WER: {best_wer:.4f} ({(1-best_wer)*100:.2f}% accuracy)")
        print("Saving...")
        model.save_pretrained(f"{CHECKPOINT_DIR}/best_model")
        processor.save_pretrained(f"{CHECKPOINT_DIR}/best_model")

    # Save checkpoint
    print(f"\nSaving epoch {epoch+1}...")
    model.save_pretrained(f"{CHECKPOINT_DIR}/epoch_{epoch+1}")
    processor.save_pretrained(f"{CHECKPOINT_DIR}/epoch_{epoch+1}")

    print(f"\nGPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB")

# FINAL SUMMARY
print("\n" + "="*60)
print("TRAINING COMPLETE - FULL SUMMARY")
print("="*60)
print("\nEpoch-by-Epoch Performance:")
print("-"*60)
print(f"{'Epoch':<8} {'Loss':<12} {'Val WER':<12} {'Val CER':<12}")
print("-"*60)
for i in range(len(train_losses)):
    print(f"{i+1:<8} {train_losses[i]:<12.4f} {val_wers[i]:<12.4f} {val_cers[i]:<12.4f}")
print("-"*60)
print(f"\nBest Validation WER: {best_wer:.4f} ({(1-best_wer)*100:.2f}% word accuracy)")
print("="*60)

"""## 10. Test Set Evaluation"""

print("\n" + "="*60)
print("FINAL TEST SET EVALUATION")
print("="*60)

# Load best model
best_model = VisionEncoderDecoderModel.from_pretrained(f"{CHECKPOINT_DIR}/best_model")
best_model = best_model.to(device)

# Evaluate on FULL test set
print("\nEvaluating on full test set...")
test_metrics = evaluate(best_model, processor, test_loader, device)

print("\nFINAL TEST RESULTS:")
print(f"  CER: {test_metrics['cer']:.4f} ({(1-test_metrics['cer'])*100:.2f}% accuracy)")
print(f"  WER: {test_metrics['wer']:.4f} ({(1-test_metrics['wer'])*100:.2f}% accuracy)")
print(f"  Exact Match: {test_metrics['accuracy']*100:.2f}%")

"""## 11. Save Results Summary"""

# Compile results
results = {
    'model': 'TrOCR (microsoft/trocr-base-handwritten)',
    'architecture': {
        'encoder': 'Vision Transformer (ViT)',
        'decoder': 'RoBERTa',
        'lora_rank': 16,
        'frozen_encoder': True
    },
    'training': {
        'epochs': EPOCHS,
        'learning_rate': LEARNING_RATE,
        'batch_size': BATCH_SIZE * GRADIENT_ACCUMULATION,
        'optimizer': 'AdamW',
        'train_losses': train_losses,
        'val_wers': val_wers
    },
    'best_validation': {
        'wer': float(best_wer),
        'word_accuracy': float((1-best_wer)*100)
    },
    'test_results': {
        'cer': float(test_metrics['cer']),
        'wer': float(test_metrics['wer']),
        'word_accuracy': float((1-test_metrics['wer'])*100),
        'char_accuracy': float((1-test_metrics['cer'])*100),
        'exact_match': float(test_metrics['accuracy']*100)
    }
}

# Save to file
results_path = f"{CHECKPOINT_DIR}/results.json"
with open(results_path, 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print("\nResults Summary:")
print(json.dumps(results, indent=2, ensure_ascii=False))
print(f"\nSaved to: {results_path}")
print(f"Best model saved to: {CHECKPOINT_DIR}/best_model")

"""## Instructions for Running on Second Dataset

To train on a different dataset:

1. **Restart Runtime**: Runtime → Restart runtime (clears GPU memory)

2. **Update Paths in Section 3**:
   ```python
   TRAIN_IMG_DIR = '/path/to/dataset2/train/images'
   TRAIN_CSV = '/path/to/dataset2/train_labels.csv'
   # ... update all paths
   CHECKPOINT_DIR = '/content/drive/MyDrive/trocr_checkpoints/dataset2'
   ```

3. **Run All Cells** from top to bottom

4. **Compare Results** by checking both checkpoint directories

## Summary

This notebook implements TrOCR fine-tuning for Arabic calligraphy OCR:

**Architecture:**
- Vision Transformer (ViT) encoder for image understanding
- RoBERTa decoder for text generation
- LoRA adapters on decoder for efficient training
- Frozen encoder to preserve pre-trained visual features

**Why TrOCR vs QARI:**
- Purpose-built OCR architecture vs general vision-language model
- ViT + RoBERTa vs Qwen's unified architecture
- Specialized for document text vs multimodal understanding

**Training:**
- 3 epochs with learning rate warmup
- Effective batch size 8 via gradient accumulation
- AdamW optimizer with weight decay
- Saves best model based on validation WER

**Expected Performance:**
- WER: 8-15%
- CER: 3-8%
- Training time: 1.5-2 hours per dataset on A100

Results and checkpoints are saved to Google Drive for easy comparison and deployment.
"""