{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZtLmdsZDS3o",
        "outputId": "390dbeaf-e433-4050-f856-0260b3d51dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "OCR Model Testing User Interface\n",
        "================================\n",
        "A Gradio-based UI for testing different OCR architectures trained on Arabic text.\n",
        "\n",
        "Supported Architectures:\n",
        "1. CNN-BiLSTM-CTC (Baseline)\n",
        "2. Conformer-CTC\n",
        "3. TrOCR-SMALL\n",
        "4. ViT + OCR\n",
        "\n",
        "Usage:\n",
        "    python user_interface.py\n",
        "\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "# ====================== Device Setup ======================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# ====================== Default Charset (Arabic characters + common symbols) ======================\n",
        "# This is a default charset - should match the one used during training\n",
        "DEFAULT_CHARSET = ['<BLANK>'] + list(\"ءآأؤإئابةتثجحخدذرزسشصضطظعغـفقكلمنهوىيًٌٍَُِّْ 0123456789.,؟!؛:()-\")\n",
        "\n",
        "# For seq2seq models (TrOCR, ViT+OCR)\n",
        "PAD_TOKEN = '<PAD>'\n",
        "BOS_TOKEN = '<BOS>'\n",
        "SOS_TOKEN = '<SOS>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "\n",
        "# ====================== Image Processing Constants ======================\n",
        "H_TARGET = 64\n",
        "W_MAX = 512\n",
        "W_TARGET = 384  # For TrOCR and ViT+OCR\n",
        "MEAN, STD = 0.5, 0.5\n",
        "\n",
        "# ====================== Image Preprocessing Functions ======================\n",
        "def preprocess_image_ctc(img_array, h=H_TARGET, w_max=W_MAX):\n",
        "    \"\"\"Preprocess image for CTC-based models (Baseline, Conformer).\"\"\"\n",
        "    if img_array is None:\n",
        "        return None\n",
        "\n",
        "    # Convert to grayscale if needed\n",
        "    if len(img_array.shape) == 3:\n",
        "        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        gray = img_array\n",
        "\n",
        "    h0, w0 = gray.shape[:2]\n",
        "    scale = h / float(h0)\n",
        "    new_w = int(np.ceil(w0 * scale))\n",
        "    new_w = min(new_w, w_max)\n",
        "    new_w = max(new_w, 16)\n",
        "\n",
        "    interp = cv2.INTER_AREA if scale < 1.0 else cv2.INTER_CUBIC\n",
        "    gray = cv2.resize(gray, (new_w, h), interpolation=interp)\n",
        "\n",
        "    # Pad to multiple of 4\n",
        "    pad_w = (4 - (new_w % 4)) % 4\n",
        "    if pad_w > 0:\n",
        "        gray = cv2.copyMakeBorder(gray, 0, 0, 0, pad_w, cv2.BORDER_CONSTANT, value=255)\n",
        "\n",
        "    # Normalize\n",
        "    img = gray.astype(np.float32) / 255.0\n",
        "    img = (img - MEAN) / STD\n",
        "    img_t = torch.from_numpy(img)[None, None, ...]  # (1, 1, H, W)\n",
        "\n",
        "    return img_t\n",
        "\n",
        "def preprocess_image_trocr(img_array, h=H_TARGET, w=W_TARGET):\n",
        "    \"\"\"Preprocess image for TrOCR (3-channel, fixed size).\"\"\"\n",
        "    if img_array is None:\n",
        "        return None\n",
        "\n",
        "    # Convert to grayscale if needed\n",
        "    if len(img_array.shape) == 3:\n",
        "        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        gray = img_array\n",
        "\n",
        "    h0, w0 = gray.shape[:2]\n",
        "    scale = h / h0\n",
        "    new_w = int(w0 * scale)\n",
        "\n",
        "    if new_w > w:\n",
        "        scale = w / w0\n",
        "        new_h = int(h0 * scale)\n",
        "        new_w = w\n",
        "        gray = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_AREA if scale < 1 else cv2.INTER_CUBIC)\n",
        "        pad_top = (h - new_h) // 2\n",
        "        pad_bottom = h - new_h - pad_top\n",
        "        gray = cv2.copyMakeBorder(gray, pad_top, pad_bottom, 0, 0, cv2.BORDER_CONSTANT, value=255)\n",
        "    else:\n",
        "        gray = cv2.resize(gray, (new_w, h), interpolation=cv2.INTER_AREA if scale < 1 else cv2.INTER_CUBIC)\n",
        "        pad_right = w - new_w\n",
        "        gray = cv2.copyMakeBorder(gray, 0, 0, 0, pad_right, cv2.BORDER_CONSTANT, value=255)\n",
        "\n",
        "    # Convert to 3-channel and normalize\n",
        "    img = np.stack([gray, gray, gray], axis=-1)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = (img - 0.5) / 0.5\n",
        "    img_t = torch.from_numpy(img).permute(2, 0, 1)[None, ...]  # (1, 3, H, W)\n",
        "\n",
        "    return img_t\n",
        "\n",
        "def preprocess_image_vit(img_array, h=H_TARGET, w=W_TARGET):\n",
        "    \"\"\"Preprocess image for ViT+OCR (1-channel, fixed size).\"\"\"\n",
        "    if img_array is None:\n",
        "        return None\n",
        "\n",
        "    # Convert to grayscale if needed\n",
        "    if len(img_array.shape) == 3:\n",
        "        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        gray = img_array\n",
        "\n",
        "    h0, w0 = gray.shape[:2]\n",
        "    scale = h / h0\n",
        "    new_w = int(w0 * scale)\n",
        "    new_w = min(new_w, w)\n",
        "\n",
        "    gray = cv2.resize(gray, (new_w, h), interpolation=cv2.INTER_AREA if scale < 1 else cv2.INTER_CUBIC)\n",
        "\n",
        "    if new_w < w:\n",
        "        pad_w = w - new_w\n",
        "        gray = cv2.copyMakeBorder(gray, 0, 0, 0, pad_w, cv2.BORDER_CONSTANT, value=255)\n",
        "\n",
        "    # Normalize\n",
        "    img = gray.astype(np.float32) / 255.0\n",
        "    img = (img - MEAN) / STD\n",
        "    img_t = torch.from_numpy(img)[None, None, ...]  # (1, 1, H, W)\n",
        "\n",
        "    return img_t\n",
        "\n",
        "\n",
        "# ====================== Model Definitions ======================\n",
        "\n",
        "# ----- 1. CNN-BiLSTM-CTC (Baseline) -----\n",
        "from torchvision import models\n",
        "\n",
        "class ResNet34OCRBackbone(nn.Module):\n",
        "    \"\"\"ResNet34 backbone adapted for OCR (preserves width dimension).\"\"\"\n",
        "    def __init__(self, in_ch=1):\n",
        "        super().__init__()\n",
        "        try:\n",
        "            m = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "        except:\n",
        "            m = models.resnet34(pretrained=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            w = m.conv1.weight.data\n",
        "            new_w = w.mean(dim=1, keepdim=True)\n",
        "        m.conv1 = nn.Conv2d(in_ch, 64, kernel_size=7, stride=(2, 1), padding=3, bias=False)\n",
        "        with torch.no_grad():\n",
        "            m.conv1.weight.copy_(new_w)\n",
        "\n",
        "        m.maxpool = nn.MaxPool2d(kernel_size=3, stride=(2, 1), padding=1)\n",
        "\n",
        "        def set_layer_height_only(layer):\n",
        "            b0 = layer[0]\n",
        "            b0.conv1.stride = (2, 1)\n",
        "            if b0.downsample is not None and isinstance(b0.downsample, nn.Sequential):\n",
        "                ds0 = b0.downsample[0]\n",
        "                if isinstance(ds0, nn.Conv2d):\n",
        "                    ds0.stride = (2, 1)\n",
        "\n",
        "        set_layer_height_only(m.layer2)\n",
        "        set_layer_height_only(m.layer3)\n",
        "        set_layer_height_only(m.layer4)\n",
        "\n",
        "        self.stem = nn.Sequential(m.conv1, m.bn1, m.relu, m.maxpool)\n",
        "        self.layer1 = m.layer1\n",
        "        self.layer2 = m.layer2\n",
        "        self.layer3 = m.layer3\n",
        "        self.layer4 = m.layer4\n",
        "        self.out = nn.AdaptiveAvgPool2d((1, None))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        fmap = x\n",
        "        seq = self.out(fmap).squeeze(2).permute(2, 0, 1)\n",
        "        return seq, fmap\n",
        "\n",
        "class TemporalGLUBlock(nn.Module):\n",
        "    def __init__(self, c=512, k=5, dilation=1, p=0.1):\n",
        "        super().__init__()\n",
        "        pad = (k - 1) // 2 * dilation\n",
        "        self.conv = nn.Conv1d(c, 2 * c, kernel_size=k, padding=pad, dilation=dilation)\n",
        "        self.glu = nn.GLU(dim=1)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.res = nn.Conv1d(c, c, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 2, 0)\n",
        "        y = self.glu(self.conv(x))\n",
        "        y = self.drop(y) + self.res(x)\n",
        "        return y.permute(2, 0, 1)\n",
        "\n",
        "class BiLSTMStack(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=256, num_layers=2, dropout=0.25):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            inp = in_dim if i == 0 else hidden * 2\n",
        "            layers.append(nn.LSTM(inp, hidden, bidirectional=True, batch_first=False))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(0, len(self.layers), 2):\n",
        "            x, _ = self.layers[i](x)\n",
        "            x = self.layers[i + 1](x)\n",
        "        return x\n",
        "\n",
        "class OCR_MultiTask_V3(nn.Module):\n",
        "    \"\"\"Baseline CNN-BiLSTM-CTC Model\"\"\"\n",
        "    def __init__(self, num_chars, num_styles, blank_idx=0, aux_weight=0.3):\n",
        "        super().__init__()\n",
        "        self.backbone = ResNet34OCRBackbone(in_ch=1)\n",
        "        self.tcn1 = TemporalGLUBlock(512, k=5, dilation=1, p=0.1)\n",
        "        self.tcn2 = TemporalGLUBlock(512, k=5, dilation=2, p=0.1)\n",
        "        self.lstm = BiLSTMStack(in_dim=512, hidden=256, num_layers=2, dropout=0.25)\n",
        "        self.ctc_head_main = nn.Linear(512, num_chars)\n",
        "        self.ctc_head_aux = nn.Linear(512, num_chars)\n",
        "        self.style_head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, max(256, num_styles * 4)), nn.ReLU(True),\n",
        "            nn.Linear(max(256, num_styles * 4), num_styles)\n",
        "        )\n",
        "        self.blank_idx = blank_idx\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_feats, fmap = self.backbone(x)\n",
        "        tcn_out = self.tcn2(self.tcn1(seq_feats))\n",
        "        lstm_out = self.lstm(tcn_out)\n",
        "        ctc_logits_main = self.ctc_head_main(lstm_out)\n",
        "        ctc_logits_aux = self.ctc_head_aux(tcn_out)\n",
        "        style_logits = self.style_head(fmap.detach())\n",
        "        return ctc_logits_main, ctc_logits_aux, style_logits\n",
        "\n",
        "\n",
        "# ----- 2. Conformer-CTC -----\n",
        "class ConvSubsampling(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=256):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=(2, 1), padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=(2, 1), padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=(2, 1), padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, out_channels, kernel_size=3, stride=(2, 1), padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.squeeze(2)\n",
        "        x = x.permute(2, 0, 1)\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ConformerConvModule(nn.Module):\n",
        "    def __init__(self, d_model, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.pointwise_conv1 = nn.Conv1d(d_model, 2 * d_model, kernel_size=1)\n",
        "        self.glu = nn.GLU(dim=1)\n",
        "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size=kernel_size,\n",
        "                                         padding=(kernel_size - 1) // 2, groups=d_model)\n",
        "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
        "        self.swish = nn.SiLU()\n",
        "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = x.permute(1, 2, 0)\n",
        "        x = self.pointwise_conv1(x)\n",
        "        x = self.glu(x)\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.swish(x)\n",
        "        x = self.pointwise_conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.permute(2, 0, 1)\n",
        "        return x + residual\n",
        "\n",
        "class ConformerFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.swish = nn.SiLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.swish(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout2(x)\n",
        "        return 0.5 * x + residual\n",
        "\n",
        "class ConformerMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x, _ = self.mha(x, x, x, key_padding_mask=mask)\n",
        "        x = self.dropout(x)\n",
        "        return x + residual\n",
        "\n",
        "class ConformerBlock(nn.Module):\n",
        "    def __init__(self, d_model=256, d_ff=1024, num_heads=4, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ff1 = ConformerFeedForward(d_model, d_ff, dropout)\n",
        "        self.mha = ConformerMultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.conv = ConformerConvModule(d_model, kernel_size, dropout)\n",
        "        self.ff2 = ConformerFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.ff1(x)\n",
        "        x = self.mha(x, mask)\n",
        "        x = self.conv(x)\n",
        "        x = self.ff2(x)\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "class ConformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model=256, d_ff=1024, num_heads=4, num_layers=6, kernel_size=31, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            ConformerBlock(d_model, d_ff, num_heads, kernel_size, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class ConformerCTC(nn.Module):\n",
        "    \"\"\"Conformer-CTC Model for Arabic OCR\"\"\"\n",
        "    def __init__(self, num_chars, num_styles, d_model=256, d_ff=1024, num_heads=4,\n",
        "                 num_layers=8, kernel_size=31, dropout=0.1, blank_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.blank_idx = blank_idx\n",
        "        self.conv_subsample = ConvSubsampling(in_channels=1, out_channels=d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=2048, dropout=dropout)\n",
        "        self.conformer = ConformerEncoder(d_model=d_model, d_ff=d_ff, num_heads=num_heads,\n",
        "                                          num_layers=num_layers, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.ctc_head = nn.Linear(d_model, num_chars)\n",
        "        self.style_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(d_model, num_styles)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.conv_subsample(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.conformer(x, mask)\n",
        "        ctc_logits = self.ctc_head(x)\n",
        "        pooled = x.mean(dim=0)\n",
        "        style_logits = self.style_head(pooled)\n",
        "        return ctc_logits, style_logits\n",
        "\n",
        "\n",
        "# ----- 3. TrOCR-SMALL -----\n",
        "try:\n",
        "    import timm\n",
        "    TIMM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TIMM_AVAILABLE = False\n",
        "    print(\"Warning: timm not available. TrOCR-SMALL will not work.\")\n",
        "\n",
        "class TrOCRDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=384, nhead=6, num_layers=6,\n",
        "                 dim_feedforward=1536, dropout=0.1, max_seq_len=128, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.pos_drop = nn.Dropout(dropout)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                    dim_feedforward=dim_feedforward,\n",
        "                                                    dropout=dropout, batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz, device):\n",
        "        mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1).bool()\n",
        "        return mask\n",
        "\n",
        "    def forward(self, encoder_output, tgt, tgt_key_padding_mask=None):\n",
        "        tgt_emb = self.token_embedding(tgt)\n",
        "        tgt_emb = tgt_emb + self.pe[:, :tgt.size(1)]\n",
        "        tgt_emb = self.pos_drop(tgt_emb)\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1), tgt.device)\n",
        "        output = self.transformer_decoder(tgt_emb, encoder_output, tgt_mask=tgt_mask,\n",
        "                                          tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        logits = self.output_proj(output)\n",
        "        return logits\n",
        "\n",
        "class TrOCRSmall(nn.Module):\n",
        "    \"\"\"TrOCR-Small: Vision Transformer encoder + Transformer decoder\"\"\"\n",
        "    def __init__(self, vocab_size, num_styles, d_model=384, nhead=6,\n",
        "                 num_decoder_layers=6, dim_feedforward=1536, dropout=0.1,\n",
        "                 max_seq_len=128, img_size=(64, 384), pad_idx=0, bos_idx=1, eos_idx=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.bos_idx = bos_idx\n",
        "        self.eos_idx = eos_idx\n",
        "\n",
        "        if TIMM_AVAILABLE:\n",
        "            self.encoder = timm.create_model('deit_small_patch16_224', pretrained=False,\n",
        "                                              img_size=img_size, in_chans=3, num_classes=0)\n",
        "            encoder_dim = self.encoder.embed_dim\n",
        "        else:\n",
        "            raise ImportError(\"timm is required for TrOCR-SMALL\")\n",
        "\n",
        "        if encoder_dim != d_model:\n",
        "            self.encoder_proj = nn.Linear(encoder_dim, d_model)\n",
        "        else:\n",
        "            self.encoder_proj = nn.Identity()\n",
        "\n",
        "        self.decoder = TrOCRDecoder(vocab_size=vocab_size, d_model=d_model, nhead=nhead,\n",
        "                                     num_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n",
        "                                     dropout=dropout, max_seq_len=max_seq_len, pad_idx=pad_idx)\n",
        "\n",
        "        self.style_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(d_model // 2, num_styles)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        features = self.encoder.forward_features(x)\n",
        "        features = self.encoder_proj(features)\n",
        "        return features\n",
        "\n",
        "    def forward(self, x, tgt, tgt_key_padding_mask=None):\n",
        "        encoder_output = self.encode(x)\n",
        "        logits = self.decoder(encoder_output, tgt, tgt_key_padding_mask)\n",
        "        cls_token = encoder_output[:, 0]\n",
        "        style_logits = self.style_head(cls_token)\n",
        "        return logits, style_logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, x, max_len=None, temperature=1.0):\n",
        "        if max_len is None:\n",
        "            max_len = self.max_seq_len\n",
        "        batch_size = x.size(0)\n",
        "        device = x.device\n",
        "        encoder_output = self.encode(x)\n",
        "        generated = torch.full((batch_size, 1), self.bos_idx, dtype=torch.long, device=device)\n",
        "\n",
        "        for _ in range(max_len - 1):\n",
        "            logits = self.decoder(encoder_output, generated)\n",
        "            next_token_logits = logits[:, -1, :] / temperature\n",
        "            next_token = next_token_logits.argmax(dim=-1, keepdim=True)\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "            if (next_token == self.eos_idx).all():\n",
        "                break\n",
        "        return generated\n",
        "\n",
        "\n",
        "# ----- 4. ViT + OCR -----\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_h=64, img_w=384, patch_size=16, in_chans=1, embed_dim=384):\n",
        "        super().__init__()\n",
        "        self.n_patches_h = img_h // patch_size\n",
        "        self.n_patches_w = img_w // patch_size\n",
        "        self.n_patches = self.n_patches_h * self.n_patches_w\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, img_h=64, img_w=384, patch_size=16, in_chans=1,\n",
        "                 embed_dim=384, depth=6, num_heads=6, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_h, img_w, patch_size, in_chans, embed_dim)\n",
        "        n_patches = self.patch_embed.n_patches\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                                    dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "                                                    dropout=dropout, activation='gelu',\n",
        "                                                    batch_first=True, norm_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class TransformerDecoderVit(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=384, depth=4, num_heads=6,\n",
        "                 mlp_ratio=4.0, dropout=0.1, max_seq_len=64, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.token_embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
        "        self.embed_drop = nn.Dropout(dropout)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                                    dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "                                                    dropout=dropout, activation='gelu',\n",
        "                                                    batch_first=True, norm_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=depth)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.output_proj = nn.Linear(embed_dim, vocab_size)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None):\n",
        "        B, T = tgt.shape\n",
        "        x = self.token_embed(tgt)\n",
        "        x = x + self.pos_embed[:, :T, :]\n",
        "        x = self.embed_drop(x)\n",
        "        if tgt_mask is None:\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(T, device=tgt.device)\n",
        "        x = self.decoder(x, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        x = self.norm(x)\n",
        "        logits = self.output_proj(x)\n",
        "        return logits\n",
        "\n",
        "class VisionTransformerOCR(nn.Module):\n",
        "    \"\"\"ViT + Transformer Decoder for OCR\"\"\"\n",
        "    def __init__(self, vocab_size, num_styles, img_h=64, img_w=384, patch_size=16,\n",
        "                 embed_dim=384, enc_depth=6, dec_depth=4, num_heads=6,\n",
        "                 mlp_ratio=4.0, dropout=0.1, max_seq_len=64,\n",
        "                 pad_idx=0, sos_idx=1, eos_idx=2):\n",
        "        super().__init__()\n",
        "        self.encoder = ViTEncoder(img_h=img_h, img_w=img_w, patch_size=patch_size, in_chans=1,\n",
        "                                   embed_dim=embed_dim, depth=enc_depth, num_heads=num_heads,\n",
        "                                   mlp_ratio=mlp_ratio, dropout=dropout)\n",
        "        self.decoder = TransformerDecoderVit(vocab_size=vocab_size, embed_dim=embed_dim, depth=dec_depth,\n",
        "                                              num_heads=num_heads, mlp_ratio=mlp_ratio, dropout=dropout,\n",
        "                                              max_seq_len=max_seq_len, pad_idx=pad_idx)\n",
        "        self.style_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim // 2, num_styles)\n",
        "        )\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.sos_idx = sos_idx\n",
        "        self.eos_idx = eos_idx\n",
        "\n",
        "    def forward(self, imgs, tgt_ids=None, tgt_padding_mask=None):\n",
        "        memory = self.encoder(imgs)\n",
        "        style_logits = self.style_head(memory.mean(dim=1))\n",
        "        if tgt_ids is not None:\n",
        "            logits = self.decoder(tgt_ids, memory, tgt_key_padding_mask=tgt_padding_mask)\n",
        "            return logits, style_logits\n",
        "        else:\n",
        "            return memory, style_logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, imgs, max_len=None, temperature=1.0):\n",
        "        if max_len is None:\n",
        "            max_len = self.max_seq_len\n",
        "        B = imgs.size(0)\n",
        "        device = imgs.device\n",
        "        memory = self.encoder(imgs)\n",
        "        generated = torch.full((B, 1), self.sos_idx, dtype=torch.long, device=device)\n",
        "\n",
        "        for _ in range(max_len - 1):\n",
        "            logits = self.decoder(generated, memory)\n",
        "            next_logits = logits[:, -1, :] / temperature\n",
        "            next_token = next_logits.argmax(dim=-1, keepdim=True)\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "            if (next_token == self.eos_idx).all():\n",
        "                break\n",
        "        return generated\n",
        "\n",
        "\n",
        "# ====================== Decode Functions ======================\n",
        "def decode_ctc(indices, idx2char, blank_idx=0, rtl_reverse=True):\n",
        "    \"\"\"CTC decoding for Baseline and Conformer models.\"\"\"\n",
        "    res, prev = [], None\n",
        "    for idx in indices:\n",
        "        if idx != blank_idx and idx != prev:\n",
        "            if idx in idx2char:\n",
        "                res.append(idx2char[idx])\n",
        "        prev = idx\n",
        "    s = \"\".join(res)\n",
        "    return s[::-1] if rtl_reverse else s\n",
        "\n",
        "def decode_seq2seq(indices, idx2char, pad_idx=0, bos_idx=1, eos_idx=2):\n",
        "    \"\"\"Seq2seq decoding for TrOCR and ViT+OCR models.\"\"\"\n",
        "    res = []\n",
        "    for idx in indices:\n",
        "        if idx == eos_idx:\n",
        "            break\n",
        "        if idx not in [pad_idx, bos_idx]:\n",
        "            if idx in idx2char:\n",
        "                res.append(idx2char[idx])\n",
        "    return \"\".join(res)\n",
        "\n",
        "\n",
        "# ====================== Model Loading and Inference ======================\n",
        "class ModelWrapper:\n",
        "    \"\"\"Wrapper class to handle model loading and inference for all architectures.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.model_type = None\n",
        "        self.charset = None\n",
        "        self.char2idx = None\n",
        "        self.idx2char = None\n",
        "        self.num_styles = 1\n",
        "        self.config = {}\n",
        "\n",
        "    def load_model(self, weights_path, model_type):\n",
        "        \"\"\"Load model from checkpoint.\"\"\"\n",
        "        if not Path(weights_path).exists():\n",
        "            return f\"Error: Weights file not found at {weights_path}\"\n",
        "\n",
        "        try:\n",
        "            checkpoint = torch.load(weights_path, map_location=DEVICE, weights_only=False)\n",
        "\n",
        "            # Try to extract charset and config from checkpoint\n",
        "            if isinstance(checkpoint, dict):\n",
        "                if 'charset' in checkpoint:\n",
        "                    self.charset = checkpoint['charset']\n",
        "                elif 'char2idx' in checkpoint:\n",
        "                    self.char2idx = checkpoint['char2idx']\n",
        "                    self.charset = [''] * len(self.char2idx)\n",
        "                    for char, idx in self.char2idx.items():\n",
        "                        self.charset[idx] = char\n",
        "                else:\n",
        "                    self.charset = DEFAULT_CHARSET\n",
        "\n",
        "                if 'config' in checkpoint:\n",
        "                    self.config = checkpoint['config']\n",
        "                if 'num_styles' in checkpoint:\n",
        "                    self.num_styles = checkpoint['num_styles']\n",
        "                elif 'style_names' in checkpoint:\n",
        "                    self.num_styles = len(checkpoint['style_names'])\n",
        "                else:\n",
        "                    self.num_styles = 5  # Default\n",
        "\n",
        "                state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint.get('state_dict', checkpoint)))\n",
        "            else:\n",
        "                self.charset = DEFAULT_CHARSET\n",
        "                state_dict = checkpoint\n",
        "\n",
        "            # Build char2idx and idx2char\n",
        "            self.char2idx = {c: i for i, c in enumerate(self.charset)}\n",
        "            self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
        "\n",
        "            num_chars = len(self.charset)\n",
        "            self.model_type = model_type\n",
        "\n",
        "            # Create model based on type\n",
        "            if model_type == \"CNN-BiLSTM-CTC (Baseline)\":\n",
        "                self.model = OCR_MultiTask_V3(num_chars=num_chars, num_styles=self.num_styles, blank_idx=0)\n",
        "            elif model_type == \"Conformer-CTC\":\n",
        "                self.model = ConformerCTC(num_chars=num_chars, num_styles=self.num_styles,\n",
        "                                          d_model=256, d_ff=1024, num_heads=4, num_layers=8,\n",
        "                                          kernel_size=31, dropout=0.1, blank_idx=0)\n",
        "            elif model_type == \"TrOCR-SMALL\":\n",
        "                if not TIMM_AVAILABLE:\n",
        "                    return \"Error: TrOCR-SMALL requires the 'timm' library. Install with: pip install timm\"\n",
        "                self.model = TrOCRSmall(vocab_size=num_chars, num_styles=self.num_styles,\n",
        "                                        d_model=384, nhead=6, num_decoder_layers=6,\n",
        "                                        dim_feedforward=1536, dropout=0.1, max_seq_len=128,\n",
        "                                        img_size=(64, 384), pad_idx=0, bos_idx=1, eos_idx=2)\n",
        "            elif model_type == \"ViT + OCR\":\n",
        "                self.model = VisionTransformerOCR(vocab_size=num_chars, num_styles=self.num_styles,\n",
        "                                                  img_h=64, img_w=384, patch_size=16,\n",
        "                                                  embed_dim=384, enc_depth=6, dec_depth=4,\n",
        "                                                  num_heads=6, mlp_ratio=4.0, dropout=0.1,\n",
        "                                                  max_seq_len=64, pad_idx=0, sos_idx=1, eos_idx=2)\n",
        "            else:\n",
        "                return f\"Error: Unknown model type '{model_type}'\"\n",
        "\n",
        "            # Load state dict (handle potential key mismatches)\n",
        "            try:\n",
        "                self.model.load_state_dict(state_dict, strict=True)\n",
        "            except RuntimeError as e:\n",
        "                # Try loading with strict=False to see what loaded\n",
        "                self.model.load_state_dict(state_dict, strict=False)\n",
        "                print(f\"Warning: Some weights may not have loaded correctly: {e}\")\n",
        "\n",
        "            self.model.to(DEVICE)\n",
        "            self.model.eval()\n",
        "\n",
        "            return f\"✅ Model loaded successfully!\\nType: {model_type}\\nCharset size: {num_chars}\\nDevice: {DEVICE}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error loading model: {str(e)}\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, image):\n",
        "        \"\"\"Run inference on an image.\"\"\"\n",
        "        if self.model is None:\n",
        "            return \"Error: No model loaded. Please load a model first.\"\n",
        "\n",
        "        if image is None:\n",
        "            return \"Error: No image provided.\"\n",
        "\n",
        "        try:\n",
        "            # Preprocess based on model type\n",
        "            if self.model_type in [\"CNN-BiLSTM-CTC (Baseline)\", \"Conformer-CTC\"]:\n",
        "                img_t = preprocess_image_ctc(image)\n",
        "            elif self.model_type == \"TrOCR-SMALL\":\n",
        "                img_t = preprocess_image_trocr(image)\n",
        "            elif self.model_type == \"ViT + OCR\":\n",
        "                img_t = preprocess_image_vit(image)\n",
        "            else:\n",
        "                return \"Error: Unknown model type\"\n",
        "\n",
        "            img_t = img_t.to(DEVICE)\n",
        "\n",
        "            # Run inference\n",
        "            if self.model_type == \"CNN-BiLSTM-CTC (Baseline)\":\n",
        "                ctc_logits, _, _ = self.model(img_t)\n",
        "                probs = F.softmax(ctc_logits, dim=-1)\n",
        "                pred_indices = probs.argmax(dim=-1).squeeze().cpu().numpy()\n",
        "                text = decode_ctc(pred_indices, self.idx2char, blank_idx=0, rtl_reverse=True)\n",
        "\n",
        "            elif self.model_type == \"Conformer-CTC\":\n",
        "                ctc_logits, _ = self.model(img_t)\n",
        "                probs = F.softmax(ctc_logits, dim=-1)\n",
        "                pred_indices = probs.argmax(dim=-1).squeeze().cpu().numpy()\n",
        "                text = decode_ctc(pred_indices, self.idx2char, blank_idx=0, rtl_reverse=True)\n",
        "\n",
        "            elif self.model_type == \"TrOCR-SMALL\":\n",
        "                generated = self.model.generate(img_t, max_len=128, temperature=1.0)\n",
        "                pred_indices = generated[0].cpu().numpy()\n",
        "                text = decode_seq2seq(pred_indices, self.idx2char, pad_idx=0, bos_idx=1, eos_idx=2)\n",
        "\n",
        "            elif self.model_type == \"ViT + OCR\":\n",
        "                generated = self.model.generate(img_t, max_len=64, temperature=1.0)\n",
        "                pred_indices = generated[0].cpu().numpy()\n",
        "                text = decode_seq2seq(pred_indices, self.idx2char, pad_idx=0, bos_idx=1, eos_idx=2)\n",
        "\n",
        "            return text if text else \"(empty prediction)\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error during inference: {str(e)}\"\n",
        "\n",
        "\n",
        "# ====================== Gradio Interface ======================\n",
        "model_wrapper = ModelWrapper()\n",
        "\n",
        "def load_model_handler(weights_file, model_type):\n",
        "    \"\"\"Handler for loading model.\"\"\"\n",
        "    if weights_file is None:\n",
        "        return \"Please upload a weights file (.pt)\"\n",
        "    return model_wrapper.load_model(weights_file.name, model_type)\n",
        "\n",
        "def predict_handler(image):\n",
        "    \"\"\"Handler for prediction.\"\"\"\n",
        "    return model_wrapper.predict(image)\n",
        "\n",
        "# Build the UI\n",
        "with gr.Blocks(title=\"Arabic OCR Model Tester\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🔤 Arabic OCR Model Tester\n",
        "\n",
        "    Test your trained OCR models on Arabic text images. Upload your model weights and an image to see the prediction.\n",
        "\n",
        "    **Supported Architectures:**\n",
        "    - CNN-BiLSTM-CTC (Baseline)\n",
        "    - Conformer-CTC\n",
        "    - TrOCR-SMALL\n",
        "    - ViT + OCR\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 1️⃣ Load Model\")\n",
        "            model_type = gr.Dropdown(\n",
        "                choices=[\n",
        "                    \"CNN-BiLSTM-CTC (Baseline)\",\n",
        "                    \"Conformer-CTC\",\n",
        "                    \"TrOCR-SMALL\",\n",
        "                    \"ViT + OCR\"\n",
        "                ],\n",
        "                value=\"CNN-BiLSTM-CTC (Baseline)\",\n",
        "                label=\"Model Architecture\"\n",
        "            )\n",
        "            weights_file = gr.File(label=\"Upload Model Weights (.pt file)\", file_types=[\".pt\", \".pth\"])\n",
        "            load_btn = gr.Button(\"Load Model\", variant=\"primary\")\n",
        "            load_status = gr.Textbox(label=\"Load Status\", lines=4, interactive=False)\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 2️⃣ Test Prediction\")\n",
        "            input_image = gr.Image(label=\"Upload Image\", type=\"numpy\")\n",
        "            predict_btn = gr.Button(\"Predict\", variant=\"primary\")\n",
        "            prediction_output = gr.Textbox(label=\"Prediction\", lines=3, interactive=False, rtl=True)\n",
        "\n",
        "    # Connect handlers\n",
        "    load_btn.click(\n",
        "        fn=load_model_handler,\n",
        "        inputs=[weights_file, model_type],\n",
        "        outputs=load_status\n",
        "    )\n",
        "\n",
        "    predict_btn.click(\n",
        "        fn=predict_handler,\n",
        "        inputs=input_image,\n",
        "        outputs=prediction_output\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    ### ℹ️ Instructions\n",
        "\n",
        "    1. **Select the model architecture** that matches your trained model\n",
        "    2. **Upload the weights file** (.pt file containing model state_dict)\n",
        "    3. **Upload an image** containing Arabic text\n",
        "    4. **Click Predict** to see the OCR result\n",
        "\n",
        "    **Note:** The model expects the weights file to be in the same format as saved during training.\n",
        "    For best results, use images with clear Arabic text on a light background.\n",
        "    \"\"\")\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "_FCrE64TDrSP",
        "outputId": "b13ccc6f-53ae-4933-9f37-da13bf77af88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3934437305.py:835: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"Arabic OCR Model Tester\", theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}